@software{graphlab,
  author = {{Carnegie Mellon University}},
  title = {Graphlab Collaborative Filtering},
  url = {http://www.select.cs.cmu.edu/code/graphlab/pmf.html},
  version = {2.2},
  date = {2013-07-01},
}
@software{pyspark,
  author = {{Apache Spark}},
  title = {Spark Python API},
  url = {http://www.select.cs.cmu.edu/code/graphlab/pmf.html},
  version = {1.6.2},
  date = {2016-06-25},
}
@software{surprise,
  author = {{Nicolas Hug}},
  title = {Surprise},
  url = {https://github.com/NicolasHug/Surprise/releases},
  version = {1.0.0},
  date = {2016-11-22},
}
@software{fancyimpute,
  author = {{Alex Rubinsteyn, Sergey Feldman}},
  title = {fancyimpute},
  url = {https://pypi.python.org/pypi/fancyimpute/},
  version = {0.0.19},
  date = {2016-07-12},
}

@article{Dietterich,
  abstract = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.},
  archivePrefix = {arXiv},
  arxivId = {arXiv:1011.1669v3},
  author = {Dietterich, Thomas G.},
  doi = {10.1007/3-540-45014-9},
  eprint = {arXiv:1011.1669v3},
  file = {:home/kiki/Downloads/mcs-ensembles.pdf:pdf},
  isbn = {978-3-540-67704-8},
  issn = {0010-4485},
  journal = {Multiple Classifier Systems},
  pages = {1--15},
  pmid = {25246403},
  title = {{Ensemble Methods in Machine Learning}},
  url = {http://link.springer.com/chapter/10.1007/3-540-45014-9{\_}1},
  volume = {1857},
  year = {2000}
}

@inproceedings{Salak2008,
  author = {Salakhutdinov, Ruslan and Mnih, A},
  booktitle = {Neural Information Processing Systems 21},
  doi = {10.1145/1390156.1390267},
  file = {:home/kiki/Downloads/3208-probabilistic-matrix-factorization.pdf:pdf},
  isbn = {9781605582054},
  issn = {1049-5258},
  keywords = {Verified},
  mendeley-tags = {Verified},
  number = {2005},
  pages = {880--887},
  title = {{Probabilistic Matrix Factorization}},
  volume = {25},
  year = {2008}
}
@inbook{Zhou2008,
  address = {Berlin, Heidelberg},
  author = {Zhou, Yunhong and Wilkinson, Dennis and Schreiber, Robert and Pan, Rong},
  booktitle = {Algorithmic Aspects in Information and Management: 4th International Conference, AAIM 2008, Shanghai, China, June 23-25, 2008. Proceedings},
  doi = {10.1007/978-3-540-68880-8_32},
  editor = {Fleischer, Rudolf and Xu, Jinhui},
  isbn = {978-3-540-68880-8},
  pages = {337--348},
  publisher = {Springer Berlin Heidelberg},
  title = {{Large-Scale Parallel Collaborative Filtering for the Netflix Prize}},
  url = {http://dx.doi.org/10.1007/978-3-540-68880-8{\_}32},
  year = {2008}
}
@article{Koren2009,
  abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.},
  archivePrefix = {arXiv},
  arxivId = {ISSN 0018-9162},
  author = {Koren, Y. and Bell, R. and Volinsky, C.},
  doi = {10.1109/MC.2009.263},
  eprint = {ISSN 0018-9162},
  file = {:home/kiki/PhD/MachineLearning/2Project/github/literature/3{\_}RecommenderSystemsNetflix.pdf:pdf},
  isbn = {0018-9162},
  issn = {0018-9162},
  journal = {Computer},
  keywords = {Computational intelligence,Matrix factorization,Netflix Prize},
  number = {8},
  pages = {42--49},
  pmid = {17255001},
  title = {{Matrix Factorization Techniques for Recommender Systems}},
  volume = {42},
  year = {2009}
}

@article{Aberger2009,
abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.},
archivePrefix = {arXiv},
arxivId = {ISSN 0018-9162},
author = {Aberger, Christopher R and Koren, Y. and Bell, R. and Volinsky, C.},
doi = {10.1109/MC.2009.263},
eprint = {ISSN 0018-9162},
file = {:home/kiki/PhD/MachineLearning/2Project/github/literature/2{\_}ChristopherAbergerRecommender.pdf:pdf},
isbn = {0018-9162},
issn = {0018-9162},
journal = {Computer},
keywords = {Computational intelligence,Matrix factorization,Netflix Prize},
number = {8},
pages = {42--49},
pmid = {17255001},
title = {{Recommender : An Analysis of Collaborative Filtering Techniques}},
volume = {42},
year = {2009}
}
@article{Andreas2009,
author = {Andreas, T and Jahrer, Michael and Bell, Robert M and Park, Florham},
file = {:home/kiki/PhD/MachineLearning/2Project/github/literature/GrandPrize2009{\_}BPC{\_}BigChaos.pdf:pdf},
pages = {1--52},
title = {{The BigChaos Solution to the Netflix Grand Prize}},
year = {2009}
}
@article{Gorrell1990,
abstract = {An algorithm based on the Generalized Hebbian Algorithm is described that allows the singular value decomposition of a dataset to be learned based on single observation pairs presented seri- ally. The algorithm has minimal mem- ory requirements, and is therefore in- teresting in the natural language do- main, where very large datasets are of- ten used, and datasets quickly become intractable. The technique is demon- strated on the task of learning word and letter bigram pairs from text.},
author = {Gorrell, Genevieve},
file = {:home/kiki/PhD/MachineLearning/2Project/github/literature/E06-1013.pdf:pdf},
isbn = {1-932432-59-0},
journal = {Supercomputer},
pages = {97--104},
title = {{Generalized Hebbian Algorithm for Incremental Singular Value Decomposition in Natural Language Processing}},
year = {1990}
}
@article{Hu2008,
author = {Hu, Yifan and Koren, Yehuda and Volinsky, Chris},
doi = {10.1109/ICDM.2008.22},
file = {:home/kiki/PhD/MachineLearning/2Project/github/literature/1{\_}cf.pdf:pdf},
isbn = {978-0-7695-3502-9},
issn = {15504786},
journal = {IEEE International Conference on Data Mining},
keywords = {academic research,academic software,academics,bibliography,digital library,library management,library software,reference software,research paper,research tool,researcher},
pages = {263--272},
title = {{Collaborative Filtering for Implicit Feedback Datasets Yifan}},
year = {2008}
}

@article{Koren2008,
author = {Koren, Yehuda and Ave, Park and Park, Florham and Management, H Database and Applications, Database},
doi = {10.1145/1401890.1401944},
file = {:home/kiki/PhD/MachineLearning/2Project/github/literature/4{\_}NeighborhoodModles.pdf:pdf},
isbn = {1605581933},
journal = {Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining},
keywords = {collaborative filtering,recommender systems},
pages = {426--434},
title = {{Factorization meets the neighborhood: a multifaceted collaborative filtering model}},
year = {2008}
}
@article{Piotte2009,
author = {Piotte, Martin},
file = {:home/kiki/PhD/MachineLearning/2Project/github/literature/GrandPrize2009{\_}BPC{\_}PragmaticTheory.pdf:pdf},
number = {August},
pages = {1--92},
title = {{The Pragmatic Theory solution to the Netflix Grand Prize}},
year = {2009}
}
