{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell only once! \n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf() \\\n",
    "  .setAppName(\"MovieLensALS\") \\\n",
    "  .set(\"spark.executor.memory\", \"2g\")\n",
    "spark_context = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count: 157\n"
     ]
    }
   ],
   "source": [
    "# For testing only.\n",
    "lines = spark_context.textFile(\"../README.md\")  \n",
    "words = lines.flatMap(lambda line: line.split())  \n",
    "count = words.count()  \n",
    "print(\"Word Count: \" + str(count)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try recommender system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# This is actually not necessary.\n",
    "def preprocess_data(data):\n",
    "    \"\"\"preprocessing the text data, conversion to numerical array format.\"\"\"\n",
    "    def deal_line(line):\n",
    "        pos = line[0]\n",
    "        rating = line[1]\n",
    "        row, col = pos.split(\"_\")\n",
    "        row = row.replace(\"r\", \"\")\n",
    "        col = col.replace(\"c\", \"\")\n",
    "        return int(row), int(col), float(rating)\n",
    "    \n",
    "    def statistics(data):\n",
    "        min_row = np.min(data[:,0])\n",
    "        max_row = np.max(data[:,0])\n",
    "        min_col = np.min(data[:,1]) \n",
    "        max_col = np.max(data[:,1])\n",
    "        return min_row, max_row, min_col, max_col\n",
    "\n",
    "    # parse each line\n",
    "    data_matrix = np.apply_along_axis(deal_line,axis=1,arr=data)\n",
    "\n",
    "    min_row, max_row, min_col, max_col = statistics(data_matrix)\n",
    "    ratings = sp.lil_matrix((int(max_row), int(max_col)))\n",
    "    for row, col, rating in data_matrix:\n",
    "        ratings[row - 1, col - 1] = rating\n",
    "    return ratings, data_matrix\n",
    "\n",
    "#data = pd.read_csv(\"../data/data_train.csv\")\n",
    "#__, data_new = preprocess_data(data.as_matrix().reshape((-1,2)))\n",
    "#train_pd = pd.DataFrame({'user':data_new[:,0],'item':data_new[:,1],'rating':data_new[:,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n",
      "number of items: 10000, number of users: 1000\n",
      "train raw shape: (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data\n",
    "import scipy.sparse as sp\n",
    "print(\"load\")\n",
    "train_raw = load_data(\"../data/data_train.csv\")\n",
    "print(\"train raw shape:\",train_raw.shape)\n",
    "#df_panda = pd.DataFrame([(0, 0, 4.0), (0, 1, 2.0), (1, 1, 3.0), (1, 2, 4.0), (2, 1, 1.0), (2, 2, 5.0)],[\"user\", \"item\", \"rating\"])\n",
    "#df_panda = pd.DataFrame([(0, 0, 1, 1, 2, 2),(0,1,1,2,1,2),(4.0,2.0,3.0,4.0,1.0,5.0)],[\"user\", \"item\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find\n",
      "panda dataframe\n",
      "sql dataframe\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "print(\"find\")\n",
    "rows, cols, ratings = sp.find(train_raw) \n",
    "print(\"panda dataframe\")\n",
    "train_pd = pd.DataFrame({'item':rows+1,'user':cols+1,'rating':ratings})\n",
    "print(\"sql dataframe\")\n",
    "context = SQLContext(spark_context)\n",
    "train = context.createDataFrame(train_pd)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+\n",
      "|item|rating|user|\n",
      "+----+------+----+\n",
      "|  44|   4.0|   1|\n",
      "|  61|   3.0|   1|\n",
      "|  67|   4.0|   1|\n",
      "|  72|   3.0|   1|\n",
      "|  86|   5.0|   1|\n",
      "|  90|   4.0|   1|\n",
      "| 108|   3.0|   1|\n",
      "| 114|   3.0|   1|\n",
      "| 120|   2.0|   1|\n",
      "| 135|   5.0|   1|\n",
      "| 152|   4.0|   1|\n",
      "| 165|   3.0|   1|\n",
      "| 182|   3.0|   1|\n",
      "| 310|   3.0|   1|\n",
      "| 318|   1.0|   1|\n",
      "| 333|   3.0|   1|\n",
      "| 355|   2.0|   1|\n",
      "| 390|   4.0|   1|\n",
      "| 401|   4.0|   1|\n",
      "| 410|   2.0|   1|\n",
      "+----+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solved for rank 30\n"
     ]
    }
   ],
   "source": [
    "# Apply alternating least squares\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# TODO: split in test and training data for cross validation.\n",
    "\n",
    "als = ALS(rank=30, maxIter=5,userCol=\"user\",itemCol=\"item\")\n",
    "model = als.fit(train)\n",
    "print('solved for rank',model.rank)\n",
    "#print(model.userFactors.orderBy(\"id\").collect())\n",
    "#print(model.itemFactors.orderBy(\"id\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "+----+----+\n",
      "|item|user|\n",
      "+----+----+\n",
      "|  37|   1|\n",
      "|  73|   1|\n",
      "| 156|   1|\n",
      "| 160|   1|\n",
      "| 248|   1|\n",
      "| 256|   1|\n",
      "| 284|   1|\n",
      "| 400|   1|\n",
      "| 416|   1|\n",
      "| 456|   1|\n",
      "| 474|   1|\n",
      "| 495|   1|\n",
      "| 515|   1|\n",
      "| 518|   1|\n",
      "| 521|   1|\n",
      "| 559|   1|\n",
      "| 596|   1|\n",
      "| 614|   1|\n",
      "| 621|   1|\n",
      "| 661|   1|\n",
      "+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data\n",
    "ratings = load_data('../data/sampleSubmission.csv')\n",
    "rows, cols, __ = sp.find(ratings)\n",
    "test_pd = pd.DataFrame({'item':rows+1,'user':cols+1})\n",
    "test = context.createDataFrame(test_pd)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/manana/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 577, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/manana/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 690, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/manana/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/manana/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c180131aee0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions_df\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/manana/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be either a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/manana/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/manana/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/home/manana/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry)\u001b[0m\n\u001b[1;32m    622\u001b[0m          \u001b[0mthe\u001b[0m \u001b[0mPy4J\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \"\"\"\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/manana/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/manana/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    584\u001b[0m             self.address, self.port, self.auto_close, self.gateway_property)\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/manana/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    695\u001b[0m                 \u001b[0;34m\"server\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server"
     ]
    }
   ],
   "source": [
    "predictions_df =  model.transform(test)\n",
    "predictions = sorted(predictions_df.collect(), key=lambda r: r[0])\n",
    "print(predictions[0])\n",
    "print(predictions[1])\n",
    "predictions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         item  user  prediction\n",
      "0         148   148    4.077627\n",
      "1         148   808    3.707975\n",
      "2         148   458    4.072464\n",
      "3         148   898    3.625180\n",
      "4         148   673    3.349541\n",
      "5         148   876    3.950046\n",
      "6         148   683    3.669017\n",
      "7         148   211    3.341513\n",
      "8         148   756    3.812518\n",
      "9         148   847    3.721761\n",
      "10        148   874    3.291717\n",
      "11        148   300    3.503820\n",
      "12        148   406    3.430869\n",
      "13        148   412    3.887383\n",
      "14        148   667    3.750972\n",
      "15        148    44    3.257751\n",
      "16        148   606    3.942766\n",
      "17        148   329    3.414915\n",
      "18        148   663    4.013635\n",
      "19        148   604    3.959615\n",
      "20        148    93    3.416903\n",
      "21        148   246    3.407135\n",
      "22        148   353    3.444619\n",
      "23        148   694    3.627520\n",
      "24        148   748    3.379458\n",
      "25        148   169    3.622392\n",
      "26        148   431    3.103256\n",
      "27        148   980    3.587841\n",
      "28        148   402    3.366875\n",
      "29        148   306    3.896518\n",
      "...       ...   ...         ...\n",
      "1176922  9901   608    4.835957\n",
      "1176923  9901   592    3.627489\n",
      "1176924  9901   978    4.468507\n",
      "1176925  9901   272    2.927768\n",
      "1176926  9901   478    4.332197\n",
      "1176927  9901   426    4.157505\n",
      "1176928  9901   611    4.558384\n",
      "1176929  9901   105    3.284490\n",
      "1176930  9901   913    3.354526\n",
      "1176931  9901   106    4.180699\n",
      "1176932  9901   805    4.091286\n",
      "1176933  9901   657    4.480555\n",
      "1176934  9901   123    3.063930\n",
      "1176935  9901   971    3.975278\n",
      "1176936  9901   135    2.500598\n",
      "1176937  9901   499    4.154021\n",
      "1176938  9901   778    3.775257\n",
      "1176939  9901   131    4.567386\n",
      "1176940  9901   573    4.130765\n",
      "1176941  9901   571    3.455315\n",
      "1176942  9901   184    4.053015\n",
      "1176943  9901   915    4.099843\n",
      "1176944  9901   628    4.342100\n",
      "1176945  9901   864    4.082340\n",
      "1176946  9901   705    4.412601\n",
      "1176947  9901   475    3.957394\n",
      "1176948  9901   138    3.206564\n",
      "1176949  9901   584    3.763443\n",
      "1176950  9901   617    4.473536\n",
      "1176951  9901   954    2.320033\n",
      "\n",
      "[1176952 rows x 3 columns]\n",
      "                 Id  Prediction\n",
      "0         r148_c148         4.0\n",
      "1         r148_c808         4.0\n",
      "2         r148_c458         4.0\n",
      "3         r148_c898         4.0\n",
      "4         r148_c673         3.0\n",
      "5         r148_c876         4.0\n",
      "6         r148_c683         4.0\n",
      "7         r148_c211         3.0\n",
      "8         r148_c756         4.0\n",
      "9         r148_c847         4.0\n",
      "10        r148_c874         3.0\n",
      "11        r148_c300         4.0\n",
      "12        r148_c406         3.0\n",
      "13        r148_c412         4.0\n",
      "14        r148_c667         4.0\n",
      "15         r148_c44         3.0\n",
      "16        r148_c606         4.0\n",
      "17        r148_c329         3.0\n",
      "18        r148_c663         4.0\n",
      "19        r148_c604         4.0\n",
      "20         r148_c93         3.0\n",
      "21        r148_c246         3.0\n",
      "22        r148_c353         3.0\n",
      "23        r148_c694         4.0\n",
      "24        r148_c748         3.0\n",
      "25        r148_c169         4.0\n",
      "26        r148_c431         3.0\n",
      "27        r148_c980         4.0\n",
      "28        r148_c402         3.0\n",
      "29        r148_c306         4.0\n",
      "...             ...         ...\n",
      "1176922  r9901_c608         5.0\n",
      "1176923  r9901_c592         4.0\n",
      "1176924  r9901_c978         4.0\n",
      "1176925  r9901_c272         3.0\n",
      "1176926  r9901_c478         4.0\n",
      "1176927  r9901_c426         4.0\n",
      "1176928  r9901_c611         5.0\n",
      "1176929  r9901_c105         3.0\n",
      "1176930  r9901_c913         3.0\n",
      "1176931  r9901_c106         4.0\n",
      "1176932  r9901_c805         4.0\n",
      "1176933  r9901_c657         4.0\n",
      "1176934  r9901_c123         3.0\n",
      "1176935  r9901_c971         4.0\n",
      "1176936  r9901_c135         3.0\n",
      "1176937  r9901_c499         4.0\n",
      "1176938  r9901_c778         4.0\n",
      "1176939  r9901_c131         5.0\n",
      "1176940  r9901_c573         4.0\n",
      "1176941  r9901_c571         3.0\n",
      "1176942  r9901_c184         4.0\n",
      "1176943  r9901_c915         4.0\n",
      "1176944  r9901_c628         4.0\n",
      "1176945  r9901_c864         4.0\n",
      "1176946  r9901_c705         4.0\n",
      "1176947  r9901_c475         4.0\n",
      "1176948  r9901_c138         3.0\n",
      "1176949  r9901_c584         4.0\n",
      "1176950  r9901_c617         4.0\n",
      "1176951  r9901_c954         2.0\n",
      "\n",
      "[1176952 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#test.write.format('com.databricks.spark.csv').save(output_path)\n",
    "#df = pd.DataFrame({'Year': ['2014', '2015'], 'quarter': ['q1', 'q2']})\n",
    "predictions_df.sort('item',inplace=True)\n",
    "predictions_pd = predictions_df.toPandas()\n",
    "print(predictions_pd)\n",
    "#predictions_pd[\"Id\"] = predictions_pd[['item', 'user']].apply(lambda x: ''.join(x.map(str)), axis=1)\n",
    "predictions_pd[\"Id\"] = \"r\" + predictions_pd[\"item\"].map(str) + \"_c\" +predictions_pd[\"user\"].map(str)\n",
    "predictions_pd[\"Prediction\"] = predictions_pd[\"prediction\"].round()\n",
    "predictions_pd_new = predictions_pd.drop([\"item\",\"user\",\"prediction\"],1)\n",
    "print(predictions_pd_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_pd_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-8ab96861bac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../results/submission_pyspark.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions_pd_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_pd_new' is not defined"
     ]
    }
   ],
   "source": [
    "output_path = '../results/submission_pyspark.csv'\n",
    "predictions_pd_new.to_csv(output_path,columns=[\"Id\",\"Prediction\"],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"alpha: alpha for implicit preference (default: 1.0)\\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. (default: 10)\\nfinalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\\nimplicitPrefs: whether to use implicit preference (default: False)\\nintermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\\nitemCol: column name for item ids. Ids must be within the integer value range. (default: item)\\nmaxIter: max number of iterations (>= 0). (default: 10, current: 5)\\nnonnegative: whether to use nonnegative constraint for least squares (default: False)\\nnumItemBlocks: number of item blocks (default: 10)\\nnumUserBlocks: number of user blocks (default: 10)\\npredictionCol: prediction column name. (default: prediction)\\nrank: rank of the factorization (default: 10, current: 30)\\nratingCol: column name for ratings (default: rating)\\nregParam: regularization parameter (>= 0). (default: 0.1)\\nseed: random seed. (default: 231637589800051193)\\nuserCol: column name for user ids. Ids must be within the integer value range. (default: user)\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions[0])\n",
    "print(model.userFactors.orderBy(\"id\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+\n",
      "|item|user|prediction|\n",
      "+----+----+----------+\n",
      "| 148| 148|  3.215077|\n",
      "| 148| 463|  3.377499|\n",
      "| 148| 496| 3.6420448|\n",
      "| 148|3175|  3.296978|\n",
      "| 148|3794| 2.8517187|\n",
      "| 148|3918|  3.342958|\n",
      "| 148|4519| 3.4460375|\n",
      "| 148|5300|  3.169028|\n",
      "| 148|5803| 3.0466917|\n",
      "| 148|6654| 3.2227583|\n",
      "| 148|6658| 3.1316009|\n",
      "| 148|7240|  3.047616|\n",
      "| 148|7833| 3.6500893|\n",
      "| 148|7880| 3.5918689|\n",
      "| 148|7993| 3.6432185|\n",
      "| 148|8592| 3.5379062|\n",
      "| 148|9376| 3.4238617|\n",
      "| 148|9465| 3.4224896|\n",
      "| 148| 623| 3.2936347|\n",
      "| 148| 737| 3.4555721|\n",
      "+----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#train = context.createDataFrame([(0, 0, 4.0), (0, 1, 2.0), (1, 1, 3.0), (1, 2, 4.0), (2, 1, 1.0), (2, 2, 5.0)],\n",
    "#                                 [\"user\", \"item\", \"rating\"])\n",
    "#test = context.createDataFrame([(0, 0), (0, 1), (1, 1), (1, 2), (2, 1), (2, 2)], [\"user\", \"item\"])\n",
    "\n",
    "als = ALS()\n",
    "model = als.fit(train)\n",
    "param_map = ParamGridBuilder() \\\n",
    "                    .addGrid(als.rank, [20, 22]) \\\n",
    "                    .addGrid(als.maxIter, [10, 15]) \\\n",
    "                    .addGrid(als.regParam, [1.0, 10.0]) \\\n",
    "                    .build()\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\")\n",
    "cvExplicit = CrossValidator(estimator=als, estimatorParamMaps=param_map, evaluator=evaluator)\n",
    "cvModelExplicit = cvExplicit.fit(train)\n",
    "predsExplicit = cvModelExplicit.bestModel.transform(test)\n",
    "predsExplicit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: \n",
      " rank: 21\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ALSModel' object has no attribute 'maxIter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-79c762723152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best model: \\n rank:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcvModelExplicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxIter:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcvModelExplicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxIter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'regParam: \\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcvModelExplicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apache-spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mgetParam\u001b[0;34m(self, paramName)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mGets\u001b[0m \u001b[0ma\u001b[0m \u001b[0mparam\u001b[0m \u001b[0mby\u001b[0m \u001b[0mits\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ALSModel' object has no attribute 'maxIter'"
     ]
    }
   ],
   "source": [
    "print('best model: \\n rank:',cvModelExplicit.bestModel.rank)\n",
    "#print('maxIter:',cvModelExplicit.bestModel.getParam('maxIter'))\n",
    "print('regParam: \\n',cvModelExplicit.bestModel.regParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pySpark (Spark 1 .6.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
