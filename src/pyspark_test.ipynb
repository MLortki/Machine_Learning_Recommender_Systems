{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-b698fc44b8a5>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d1ec42dd8104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MovieLensALS\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.memory\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/apache-spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/apache-spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    257\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 259\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-b698fc44b8a5>:3 "
     ]
    }
   ],
   "source": [
    "# Run this cell only once! \n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf() \\\n",
    "  .setAppName(\"MovieLensALS\") \\\n",
    "  .set(\"spark.executor.memory\", \"2g\")\n",
    "spark_context = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count: 14\n"
     ]
    }
   ],
   "source": [
    "# For testing only.\n",
    "\n",
    "lines = spark_context.textFile(\"../README.md\")  \n",
    "words = lines.flatMap(lambda line: line.split())  \n",
    "count = words.count()  \n",
    "print(\"Word Count: \" + str(count)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try recommender system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# This is actually not necessary.\n",
    "def preprocess_data(data):\n",
    "    \"\"\"preprocessing the text data, conversion to numerical array format.\"\"\"\n",
    "    def deal_line(line):\n",
    "        pos = line[0]\n",
    "        rating = line[1]\n",
    "        row, col = pos.split(\"_\")\n",
    "        row = row.replace(\"r\", \"\")\n",
    "        col = col.replace(\"c\", \"\")\n",
    "        return int(row), int(col), float(rating)\n",
    "    \n",
    "    def statistics(data):\n",
    "        min_row = np.min(data[:,0])\n",
    "        max_row = np.max(data[:,0])\n",
    "        min_col = np.min(data[:,1]) \n",
    "        max_col = np.max(data[:,1])\n",
    "        return min_row, max_row, min_col, max_col\n",
    "\n",
    "    # parse each line\n",
    "    data_matrix = np.apply_along_axis(deal_line,axis=1,arr=data)\n",
    "\n",
    "    min_row, max_row, min_col, max_col = statistics(data_matrix)\n",
    "    ratings = sp.lil_matrix((int(max_row), int(max_col)))\n",
    "    for row, col, rating in data_matrix:\n",
    "        ratings[row - 1, col - 1] = rating\n",
    "    return ratings, data_matrix\n",
    "\n",
    "#data = pd.read_csv(\"../data/data_train.csv\")\n",
    "#__, data_new = preprocess_data(data.as_matrix().reshape((-1,2)))\n",
    "#train_pd = pd.DataFrame({'user':data_new[:,0],'item':data_new[:,1],'rating':data_new[:,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "find\n",
      "sql_context\n",
      "panda dataframe\n",
      "sql dataframe\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from helpers import load_data\n",
    "import scipy.sparse as sp\n",
    "print(\"load\")\n",
    "train_raw = load_data(\"../data/data_train.csv\")\n",
    "print(\"find\")\n",
    "rows, cols, ratings = sp.find(train_raw) \n",
    "print(\"sql_context\")\n",
    "context = SQLContext(spark_context)\n",
    "#df_panda = pd.DataFrame([(0, 0, 4.0), (0, 1, 2.0), (1, 1, 3.0), (1, 2, 4.0), (2, 1, 1.0), (2, 2, 5.0)],[\"user\", \"item\", \"rating\"])\n",
    "#df_panda = pd.DataFrame([(0, 0, 1, 1, 2, 2),(0,1,1,2,1,2),(4.0,2.0,3.0,4.0,1.0,5.0)],[\"user\", \"item\", \"rating\"])\n",
    "print(\"panda dataframe\")\n",
    "train_pd = pd.DataFrame({'user':rows+1,'item':cols+1,'rating':ratings})\n",
    "print(\"sql dataframe\")\n",
    "train = context.createDataFrame(train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+\n",
      "|item|rating|user|\n",
      "+----+------+----+\n",
      "|   1|   4.0|  44|\n",
      "|   1|   3.0|  61|\n",
      "|   1|   4.0|  67|\n",
      "|   1|   3.0|  72|\n",
      "|   1|   5.0|  86|\n",
      "|   1|   4.0|  90|\n",
      "|   1|   3.0| 108|\n",
      "|   1|   3.0| 114|\n",
      "|   1|   2.0| 120|\n",
      "|   1|   5.0| 135|\n",
      "|   1|   4.0| 152|\n",
      "|   1|   3.0| 165|\n",
      "|   1|   3.0| 182|\n",
      "|   1|   3.0| 310|\n",
      "|   1|   1.0| 318|\n",
      "|   1|   3.0| 333|\n",
      "|   1|   2.0| 355|\n",
      "|   1|   4.0| 390|\n",
      "|   1|   4.0| 401|\n",
      "|   1|   2.0| 410|\n",
      "+----+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solved for rank 30\n"
     ]
    }
   ],
   "source": [
    "# Apply alternating least squares\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# TODO: split in test and training data for cross validation.\n",
    "\n",
    "als = ALS(rank=30, maxIter=5)\n",
    "model = als.fit(train)\n",
    "print('solved for rank',model.rank)\n",
    "#print(model.userFactors.orderBy(\"id\").collect())\n",
    "#print(model.itemFactors.orderBy(\"id\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "+----+----+\n",
      "|item|user|\n",
      "+----+----+\n",
      "|   1|  37|\n",
      "|   1|  73|\n",
      "|   1| 156|\n",
      "|   1| 160|\n",
      "|   1| 248|\n",
      "|   1| 256|\n",
      "|   1| 284|\n",
      "|   1| 400|\n",
      "|   1| 416|\n",
      "|   1| 456|\n",
      "|   1| 474|\n",
      "|   1| 495|\n",
      "|   1| 515|\n",
      "|   1| 518|\n",
      "|   1| 521|\n",
      "|   1| 559|\n",
      "|   1| 596|\n",
      "|   1| 614|\n",
      "|   1| 621|\n",
      "|   1| 661|\n",
      "+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data\n",
    "ratings = load_data('../data/sampleSubmission.csv')\n",
    "rows, cols, __ = sp.find(ratings)\n",
    "rows += 1\n",
    "cols += 1\n",
    "test_pd = pd.DataFrame({'user':rows,'item':cols})\n",
    "test = context.createDataFrame(test_pd)\n",
    "#test = context.createDataFrame([(1, 2), (1, 1), (2, 1)], [\"user\", \"item\"])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = sorted(model.transform(test).collect(), key=lambda r: r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(item=1, user=7833, prediction=3.4777028560638428)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(item=1, user=3986, prediction=2.6524620056152344)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"alpha: alpha for implicit preference (default: 1.0)\\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. (default: 10)\\nfinalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\\nimplicitPrefs: whether to use implicit preference (default: False)\\nintermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\\nitemCol: column name for item ids. Ids must be within the integer value range. (default: item)\\nmaxIter: max number of iterations (>= 0). (default: 10, current: 5)\\nnonnegative: whether to use nonnegative constraint for least squares (default: False)\\nnumItemBlocks: number of item blocks (default: 10)\\nnumUserBlocks: number of user blocks (default: 10)\\npredictionCol: prediction column name. (default: prediction)\\nrank: rank of the factorization (default: 10, current: 30)\\nratingCol: column name for ratings (default: rating)\\nregParam: regularization parameter (>= 0). (default: 0.1)\\nseed: random seed. (default: 231637589800051193)\\nuserCol: column name for user ids. Ids must be within the integer value range. (default: user)\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions[0])\n",
    "print(model.userFactors.orderBy(\"id\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+\n",
      "|item|user|prediction|\n",
      "+----+----+----------+\n",
      "| 148| 148|  3.215077|\n",
      "| 148| 463|  3.377499|\n",
      "| 148| 496| 3.6420448|\n",
      "| 148|3175|  3.296978|\n",
      "| 148|3794| 2.8517187|\n",
      "| 148|3918|  3.342958|\n",
      "| 148|4519| 3.4460375|\n",
      "| 148|5300|  3.169028|\n",
      "| 148|5803| 3.0466917|\n",
      "| 148|6654| 3.2227583|\n",
      "| 148|6658| 3.1316009|\n",
      "| 148|7240|  3.047616|\n",
      "| 148|7833| 3.6500893|\n",
      "| 148|7880| 3.5918689|\n",
      "| 148|7993| 3.6432185|\n",
      "| 148|8592| 3.5379062|\n",
      "| 148|9376| 3.4238617|\n",
      "| 148|9465| 3.4224896|\n",
      "| 148| 623| 3.2936347|\n",
      "| 148| 737| 3.4555721|\n",
      "+----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#train = context.createDataFrame([(0, 0, 4.0), (0, 1, 2.0), (1, 1, 3.0), (1, 2, 4.0), (2, 1, 1.0), (2, 2, 5.0)],\n",
    "#                                 [\"user\", \"item\", \"rating\"])\n",
    "#test = context.createDataFrame([(0, 0), (0, 1), (1, 1), (1, 2), (2, 1), (2, 2)], [\"user\", \"item\"])\n",
    "\n",
    "\n",
    "als = ALS()\n",
    "model = als.fit(train)\n",
    "param_map = ParamGridBuilder() \\\n",
    "                    .addGrid(als.rank, [20, 21]) \\\n",
    "                    .addGrid(als.maxIter, [10, 15]) \\\n",
    "                    .addGrid(als.regParam, [1.0, 10.0]) \\\n",
    "                    .build()\n",
    "evaluatorR = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\")\n",
    "cvExplicit = CrossValidator(estimator=als, estimatorParamMaps=param_map, evaluator=evaluatorR)\n",
    "cvModelExplicit = cvExplicit.fit(train)\n",
    "predsExplicit = cvModelExplicit.bestModel.transform(test)\n",
    "predsExplicit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: \n",
      " rank: 21\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ALSModel' object has no attribute 'maxIter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-79c762723152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best model: \\n rank:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcvModelExplicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxIter:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcvModelExplicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxIter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'regParam: \\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcvModelExplicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apache-spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mgetParam\u001b[0;34m(self, paramName)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mGets\u001b[0m \u001b[0ma\u001b[0m \u001b[0mparam\u001b[0m \u001b[0mby\u001b[0m \u001b[0mits\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ALSModel' object has no attribute 'maxIter'"
     ]
    }
   ],
   "source": [
    "print('best model: \\n rank:',cvModelExplicit.bestModel.rank)\n",
    "print('maxIter:',cvModelExplicit.bestModel.getParam('maxIter'))\n",
    "print('regParam: \\n',cvModelExplicit.bestModel.regParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
