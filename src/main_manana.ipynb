{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as skm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Note that `ratings` is a sparse matrix that in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "shape of dataset: (100, 101)\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "#location of data\n",
    "path_dataset = \"../data/data_train.csv\"\n",
    "subset = [100, 101]\n",
    "ratings = load_data(path_dataset, subset)\n",
    "#ratings = load_data(path_dataset)\n",
    "print(\"shape of dataset:\",ratings.shape)\n",
    "#print(ratings)\n",
    "#print( ratings[0,9])\n",
    "#print( ratings[0:5,0:5])\n",
    "#ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fedb0db8ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # you should return:\n",
    "    #     user_features: shape = num_features, num_user\n",
    "    #     item_features: shape = num_features, num_item\n",
    "    # ***************************************************\n",
    "    \n",
    "    user_features = 3 * np.random.rand( num_features, train.shape[1])\n",
    "    item_features = 3 * np.random.rand( num_features, train.shape[0])\n",
    "    \n",
    "    item_features[0, :] = train[train != 0].mean(axis = 1)\n",
    "    \n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost by the method of matrix factorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # calculate rmse (we only consider nonzero entries.)\n",
    "    # ***************************************************\n",
    "    X = np.dot ( np.transpose(item_features), user_features )\n",
    "    \n",
    "    rmse = 0\n",
    "    counter = 0\n",
    "    for i, j in nz:\n",
    "        \n",
    "        rmse += (data[i,j] - X[i,j])**2\n",
    "        counter += 1\n",
    "    \n",
    "    rmse = rmse/(counter)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user):   #, nz_user_itemindices\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # update and return user feature.\n",
    "    # ***************************************************\n",
    "    N = train.shape[1]\n",
    "    D = train.shape[0]\n",
    "    K = item_features.shape[0]\n",
    "    \n",
    "    new_user_features = np.zeros((K, N))\n",
    "    \n",
    "    #print(\"UPDATE USER\")\n",
    "    for g , value in nnz_items_per_user:\n",
    "    \n",
    "        #print(\"column index = \", g)\n",
    "        #print(\"elementebis raodenoba = \", value.shape)\n",
    "        #scipy lil sparse matrixisdan ro gadaviyvanot numpy arrayshi\n",
    "        #print(g)\n",
    "        X = train[value, g].toarray()\n",
    "        #print(\"shape of new train data = \", X.shape)\n",
    "        Wt = item_features[: , value]\n",
    "        #print(\"shape of Wt = \", Wt.shape)\n",
    "        #print(type(Wt))\n",
    "        #print(type(X))\n",
    "        new_user_features[:, g] = np.linalg.solve( \n",
    "            np.dot(Wt, np.transpose(Wt))+ lambda_user* value.shape[0]* np.eye(K) ,\n",
    "            np.dot( Wt, X)).flatten()\n",
    "    \n",
    "    \n",
    "    return new_user_features\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item):   #, nz_item_userindices\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    N = train.shape[1]\n",
    "    D = train.shape[0]\n",
    "    K = user_features.shape[0]\n",
    "        \n",
    "    new_item_features = np.zeros((K, D))\n",
    "    #print(\"UPDATE ITEM\")\n",
    "    for g, value  in nnz_users_per_item:\n",
    "        \n",
    "        #print(\"column index = \", g)\n",
    "        #print(\"elementebis raodenoba = \", value.shape)\n",
    "        \n",
    "        X = train[g,value].toarray()\n",
    "        #print(\"shape of new train data = \", X.shape)\n",
    "        Zt = user_features[: , value]\n",
    "        #print(\"shape of Zt = \", Zt.shape)\n",
    "        new_item_features[:, g] = np.linalg.solve( \n",
    "            np.dot(Zt, np.transpose(Zt))+ lambda_item * value.shape[0]* np.eye(K)  ,\n",
    "            np.dot( Zt, np.transpose(X))).flatten()\n",
    "        \n",
    "    \n",
    "    return new_item_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import build_index_groups\n",
    "\n",
    "\n",
    "def ALS (train , test, num_features,lambda_user, \n",
    "         lambda_item, stop_criterion,error_list,rng ):\n",
    "    \n",
    "    # initialize user and movies latent matrices\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    \n",
    "    #indices of nonzero elements\n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    nz_train, nz_row_colindices, nz_col_rowindices = build_index_groups(train)   \n",
    "   \n",
    "  \n",
    "    #print(nz_train)\n",
    "    #print(nz_row_colindices )\n",
    "    print(\"Original data shape: \", train.shape)\n",
    "    #print(nz_train.shape)\n",
    "    #print(nz_row_colindices.shape)\n",
    "    #print(nz_col_rowindices)\n",
    "    #i=0\n",
    "                \n",
    "    #while True:\n",
    "    for i in range(rng):\n",
    "\n",
    "        user_features_new = update_user_feature(\n",
    "            train, item_features, lambda_user, nz_col_rowindices)\n",
    "        if( user_features_new.shape != user_features.shape):\n",
    "            print(\"AAAAA\")\n",
    "        item_features_new = update_item_feature(\n",
    "            train, user_features_new, lambda_item, nz_row_colindices)\n",
    "\n",
    "\n",
    "        '''rmse_tr = compute_error(train, user_features_new, item_features_new , nz_train)\n",
    "        rmse_te = compute_error(test, user_features_new, item_features_new , nz_test)\n",
    "        if i % 5 == 0:\n",
    "            train_errors.append(rmse_tr)\n",
    "            test_errors.append(rmse_te)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(i, rmse_tr))\n",
    "        print(\"iter: {}, RMSE on test set: {}.\".format(i, rmse_te))'''\n",
    "        #i +=1\n",
    "        if np.linalg.norm(item_features_new - item_features ) < stop_criterion and np.linalg.norm(user_features_new - user_features ) < stop_criterion:\n",
    "            break\n",
    "\n",
    "        item_features = item_features_new\n",
    "        user_features = user_features_new\n",
    "    \n",
    "    \n",
    "    rmse_te = compute_error(test, user_features, item_features, nz_test)\n",
    "    rmse_tr = compute_error(train, user_features, item_features, nz_train)\n",
    "    print(\"Final RMSE on test data: {}.\".format(rmse_te))\n",
    "\n",
    "    return rmse_tr, rmse_te\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_train_test(ratings, nz_ratings, i_ind , j_ind):\n",
    "    \n",
    "    #estimating shape\n",
    "    d = ratings.shape[0]\n",
    "    n = ratings.shape[1] \n",
    "    \n",
    "    \n",
    "    train_index = [ nz_ratings[i] for i in i_ind]\n",
    "    test_index = [ nz_ratings[j] for j in j_ind]\n",
    "\n",
    "    print(\"train index size = \", len(train_index))\n",
    "    print(\"test index size = \", len(test_index))\n",
    "\n",
    "    \n",
    "    if len(train_index) + len(test_index) != len(nz_ratings):\n",
    "        print(\"Wrong !!\")\n",
    "\n",
    "    train = np.zeros((d,n )) \n",
    "    test  = np.zeros((d,n))\n",
    "\n",
    "    for ind in train_index:\n",
    "        i = ind[0]\n",
    "        j = ind[1]\n",
    "        train[i,j] = ratings[i,j]\n",
    "        \n",
    "    for ind in test_index:\n",
    "        i = ind[0]\n",
    "        j = ind[1]\n",
    "        test[i,j] = ratings[i,j]\n",
    "        \n",
    "    train = sp.lil_matrix(train)\n",
    "    test = sp.lil_matrix(test)\n",
    "    \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    print(\"nonzero elems in train: \", len(nz_train))\n",
    "    print(\"nonzero elems in test: \", len(nz_test))\n",
    "    \n",
    "    return train, test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation(\n",
    "    ratings, n_of_splits,num_features,lambdas,\n",
    "    stop_criterion, error_list, rng):\n",
    "\n",
    "   \n",
    "    #cross_validation(n_splits)\n",
    "    kf = skm.KFold(n_splits=n_of_splits, shuffle = True)\n",
    "    \n",
    "    #creating matrix of non-zero indices\n",
    "    nz_row, nz_col = ratings.nonzero()\n",
    "    nz_ratings = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    print(\"Length of list of nonzero elems of ratings is : \", len(nz_ratings) )\n",
    "    \n",
    "    t = range(nz_row.shape[0])\n",
    "    \n",
    "    #t = nz_row.shape[0]\n",
    "    #nz_row = nz_row.reshape(t,1)\n",
    "    #nz_col = nz_col.reshape(t,1)\n",
    "    #nz_ratings = np.concatenate((nz_row, nz_col), axis = 1)\n",
    "    \n",
    "    #print(nz_row.shape)\n",
    "    #print(nz_col.shape)\n",
    "    #print(nz_ratings.shape)\n",
    "    \n",
    "    # creating matrix where results of cross validation are stored\n",
    "    test_avg_cost =  np.zeros(len(lambdas))\n",
    "    train_avg_cost = np.zeros(len(lambdas))\n",
    "\n",
    "     \n",
    "    for ind,lambda_ in enumerate(lambdas):\n",
    "            \n",
    "        lambda_user = lambda_[0]\n",
    "        lambda_item = lambda_[1]\n",
    "            \n",
    "        avg_train = 0\n",
    "        avg_test = 0\n",
    "            \n",
    "        print(\"lambda user = \",lambda_user, \"lambda item = \", lambda_item)\n",
    "\n",
    "        for i, j in kf.split(t):\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"train: \",i.shape)\n",
    "            print(\"test: \",j.shape)\n",
    "            print(j)\n",
    "            print(\"Percentage of train and test reps. : \", \n",
    "                  len(i)*100/nz_row.shape[0],\"% \", len(j)*100/nz_row.shape[0],\"%\" )\n",
    "            \n",
    "            train , test = gen_train_test(ratings, nz_ratings, i , j)\n",
    "            rmse_tr, rmse_te = ALS (train , test, num_features,lambda_user, \n",
    "                                    lambda_item, stop_criterion,error_list,rng )\n",
    "                \n",
    "            avg_train += rmse_tr\n",
    "            avg_test += rmse_te\n",
    "                    \n",
    "                \n",
    "                \n",
    "        avg_train /= n_of_splits\n",
    "        avg_test /= n_of_splits\n",
    "        test_avg_cost[ind] = avg_test\n",
    "        train_avg_cost[ind] = avg_train\n",
    "    \n",
    "    return test_avg_cost, train_avg_cost\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different pairs of lambdas :  1\n",
      "Length of list of nonzero elems of ratings is :  1226\n",
      "lambda user =  0.1 lambda item =  0.1\n",
      "train:  (1103,)\n",
      "test:  (123,)\n",
      "[   1   15   20   40   42   51   68   77   98  115  118  120  127  131  141\n",
      "  150  156  164  171  174  183  184  195  198  205  206  209  214  218  226\n",
      "  235  237  245  246  259  289  295  302  304  306  323  341  366  368  382\n",
      "  402  420  425  437  438  472  478  479  480  490  494  498  500  501  503\n",
      "  507  510  530  540  546  560  590  591  596  600  609  624  637  655  660\n",
      "  679  690  698  722  730  731  781  790  793  803  805  817  821  843  846\n",
      "  847  848  858  861  873  914  918  961  963  972  976  996 1002 1010 1033\n",
      " 1035 1054 1066 1074 1076 1096 1097 1108 1112 1118 1128 1150 1167 1172 1181\n",
      " 1185 1186 1204]\n",
      "Percentage of train and test reps. :  89.9673735725938 %  10.0326264274062 %\n",
      "train index size =  1103\n",
      "test index size =  123\n",
      "nonzero elems in train:  1103\n",
      "nonzero elems in test:  123\n",
      "Original data shape:  (100, 101)\n",
      "iter: 0, RMSE on training set: 1.1339771714407432.\n",
      "iter: 0, RMSE on test set: 3.8741162078190334.\n",
      "iter: 1, RMSE on training set: 0.3610466114210788.\n",
      "iter: 1, RMSE on test set: 2.979437157500937.\n",
      "iter: 2, RMSE on training set: 0.22145396687691518.\n",
      "iter: 2, RMSE on test set: 2.7748945888446164.\n",
      "iter: 3, RMSE on training set: 0.164582798628878.\n",
      "iter: 3, RMSE on test set: 2.597933843737234.\n",
      "iter: 4, RMSE on training set: 0.13471578126440978.\n",
      "iter: 4, RMSE on test set: 2.455689917295919.\n",
      "iter: 5, RMSE on training set: 0.11674348059457047.\n",
      "iter: 5, RMSE on test set: 2.34597634000162.\n",
      "iter: 6, RMSE on training set: 0.10499124080941115.\n",
      "iter: 6, RMSE on test set: 2.263175852690019.\n",
      "iter: 7, RMSE on training set: 0.09684356028079136.\n",
      "iter: 7, RMSE on test set: 2.200332928502592.\n",
      "iter: 8, RMSE on training set: 0.09094520781196491.\n",
      "iter: 8, RMSE on test set: 2.151841332194927.\n",
      "iter: 9, RMSE on training set: 0.08653336756159474.\n",
      "iter: 9, RMSE on test set: 2.11377444618964.\n",
      "iter: 10, RMSE on training set: 0.08314702322779385.\n",
      "iter: 10, RMSE on test set: 2.0833830992642253.\n",
      "iter: 11, RMSE on training set: 0.08049197416415955.\n",
      "iter: 11, RMSE on test set: 2.058710765277287.\n",
      "iter: 12, RMSE on training set: 0.07837298019188382.\n",
      "iter: 12, RMSE on test set: 2.0383664846654628.\n",
      "iter: 13, RMSE on training set: 0.07665654863630113.\n",
      "iter: 13, RMSE on test set: 2.021368087408918.\n",
      "iter: 14, RMSE on training set: 0.0752490489685123.\n",
      "iter: 14, RMSE on test set: 2.0070206319175057.\n",
      "iter: 15, RMSE on training set: 0.0740832143320938.\n",
      "iter: 15, RMSE on test set: 1.9948241082097011.\n",
      "iter: 16, RMSE on training set: 0.07310956641023124.\n",
      "iter: 16, RMSE on test set: 1.984407675294694.\n",
      "iter: 17, RMSE on training set: 0.07229087362531891.\n",
      "iter: 17, RMSE on test set: 1.9754855495778747.\n",
      "iter: 18, RMSE on training set: 0.07159852886028874.\n",
      "iter: 18, RMSE on test set: 1.9678289410100396.\n",
      "iter: 19, RMSE on training set: 0.07101015249770642.\n",
      "iter: 19, RMSE on test set: 1.961248990855684.\n",
      "iter: 20, RMSE on training set: 0.0705079769155918.\n",
      "iter: 20, RMSE on test set: 1.955586679929764.\n",
      "iter: 21, RMSE on training set: 0.07007772951863527.\n",
      "iter: 21, RMSE on test set: 1.9507067859419442.\n",
      "iter: 22, RMSE on training set: 0.06970783717136506.\n",
      "iter: 22, RMSE on test set: 1.9464939901183826.\n",
      "iter: 23, RMSE on training set: 0.06938884286342575.\n",
      "iter: 23, RMSE on test set: 1.9428500411792968.\n",
      "iter: 24, RMSE on training set: 0.06911296734978534.\n",
      "iter: 24, RMSE on test set: 1.939691432106329.\n",
      "iter: 25, RMSE on training set: 0.06887377347481141.\n",
      "iter: 25, RMSE on test set: 1.9369473619669464.\n",
      "iter: 26, RMSE on training set: 0.06866590556972704.\n",
      "iter: 26, RMSE on test set: 1.9345579093558425.\n",
      "iter: 27, RMSE on training set: 0.06848488507931196.\n",
      "iter: 27, RMSE on test set: 1.9324724034228835.\n",
      "iter: 28, RMSE on training set: 0.06832694902391429.\n",
      "iter: 28, RMSE on test set: 1.9306479919660593.\n",
      "iter: 29, RMSE on training set: 0.06818892147589516.\n",
      "iter: 29, RMSE on test set: 1.9290484022982408.\n",
      "iter: 30, RMSE on training set: 0.06806811069471762.\n",
      "iter: 30, RMSE on test set: 1.9276428833424994.\n",
      "iter: 31, RMSE on training set: 0.06796222633470446.\n",
      "iter: 31, RMSE on test set: 1.9264053118771922.\n",
      "iter: 32, RMSE on training set: 0.06786931244474195.\n",
      "iter: 32, RMSE on test set: 1.9253134431317915.\n",
      "iter: 33, RMSE on training set: 0.06778769295807571.\n",
      "iter: 33, RMSE on test set: 1.9243482855576692.\n",
      "iter: 34, RMSE on training set: 0.06771592711194688.\n",
      "iter: 34, RMSE on test set: 1.9234935807382878.\n",
      "iter: 35, RMSE on training set: 0.0676527728023533.\n",
      "iter: 35, RMSE on test set: 1.9227353713400932.\n",
      "iter: 36, RMSE on training set: 0.0675971563125907.\n",
      "iter: 36, RMSE on test set: 1.9220616422358463.\n",
      "iter: 37, RMSE on training set: 0.06754814718767968.\n",
      "iter: 37, RMSE on test set: 1.9214620221517302.\n",
      "iter: 38, RMSE on training set: 0.06750493728439486.\n",
      "iter: 38, RMSE on test set: 1.920927535236816.\n",
      "iter: 39, RMSE on training set: 0.0674668232264319.\n",
      "iter: 39, RMSE on test set: 1.920450393758425.\n",
      "iter: 40, RMSE on training set: 0.06743319164992577.\n",
      "iter: 40, RMSE on test set: 1.920023824673468.\n",
      "iter: 41, RMSE on training set: 0.0674035067463323.\n",
      "iter: 41, RMSE on test set: 1.919641924126115.\n",
      "iter: 42, RMSE on training set: 0.06737729970544598.\n",
      "iter: 42, RMSE on test set: 1.9192995350014763.\n",
      "iter: 43, RMSE on training set: 0.06735415973693983.\n",
      "iter: 43, RMSE on test set: 1.9189921435532955.\n",
      "iter: 44, RMSE on training set: 0.06733372640882426.\n",
      "iter: 44, RMSE on test set: 1.9187157918504687.\n",
      "iter: 45, RMSE on training set: 0.06731568308904538.\n",
      "iter: 45, RMSE on test set: 1.9184670033795832.\n",
      "iter: 46, RMSE on training set: 0.06729975131476199.\n",
      "iter: 46, RMSE on test set: 1.9182427196224379.\n",
      "iter: 47, RMSE on training set: 0.06728568594465145.\n",
      "iter: 47, RMSE on test set: 1.918040245818785.\n",
      "iter: 48, RMSE on training set: 0.06727327097451177.\n",
      "iter: 48, RMSE on test set: 1.9178572044422983.\n",
      "iter: 49, RMSE on training set: 0.06726231591663484.\n",
      "iter: 49, RMSE on test set: 1.9176914951758863.\n",
      "iter: 50, RMSE on training set: 0.06725265265990636.\n",
      "iter: 50, RMSE on test set: 1.9175412603823248.\n",
      "iter: 51, RMSE on training set: 0.06724413274107773.\n",
      "iter: 51, RMSE on test set: 1.9174048552372567.\n",
      "iter: 52, RMSE on training set: 0.06723662496873194.\n",
      "iter: 52, RMSE on test set: 1.917280821831026.\n",
      "iter: 53, RMSE on training set: 0.06723001335061508.\n",
      "iter: 53, RMSE on test set: 1.9171678666600505.\n",
      "iter: 54, RMSE on training set: 0.06722419528257324.\n",
      "iter: 54, RMSE on test set: 1.917064841022012.\n",
      "iter: 55, RMSE on training set: 0.06721907996363102.\n",
      "iter: 55, RMSE on test set: 1.9169707239062321.\n",
      "iter: 56, RMSE on training set: 0.06721458700700078.\n",
      "iter: 56, RMSE on test set: 1.9168846070341556.\n",
      "iter: 57, RMSE on training set: 0.06721064522120526.\n",
      "iter: 57, RMSE on test set: 1.9168056817575458.\n",
      "iter: 58, RMSE on training set: 0.06720719153919517.\n",
      "iter: 58, RMSE on test set: 1.9167332275658047.\n",
      "iter: 59, RMSE on training set: 0.0672041700764541.\n",
      "iter: 59, RMSE on test set: 1.9166666019902951.\n",
      "iter: 60, RMSE on training set: 0.06720153130171615.\n",
      "iter: 60, RMSE on test set: 1.9166052317241684.\n",
      "iter: 61, RMSE on training set: 0.06719923130616295.\n",
      "iter: 61, RMSE on test set: 1.916548604801882.\n",
      "iter: 62, RMSE on training set: 0.06719723115885712.\n",
      "iter: 62, RMSE on test set: 1.9164962637042704.\n",
      "iter: 63, RMSE on training set: 0.06719549633780394.\n",
      "iter: 63, RMSE on test set: 1.9164477992733737.\n",
      "iter: 64, RMSE on training set: 0.06719399622741422.\n",
      "iter: 64, RMSE on test set: 1.9164028453368431.\n",
      "iter: 65, RMSE on training set: 0.06719270367433629.\n",
      "iter: 65, RMSE on test set: 1.9163610739549861.\n",
      "iter: 66, RMSE on training set: 0.06719159459465114.\n",
      "iter: 66, RMSE on test set: 1.9163221912149113.\n",
      "iter: 67, RMSE on training set: 0.06719064762630979.\n",
      "iter: 67, RMSE on test set: 1.9162859335059808.\n",
      "iter: 68, RMSE on training set: 0.06718984382145383.\n",
      "iter: 68, RMSE on test set: 1.9162520642190966.\n",
      "iter: 69, RMSE on training set: 0.06718916637392791.\n",
      "iter: 69, RMSE on test set: 1.916220370819674.\n",
      "iter: 70, RMSE on training set: 0.06718860037786438.\n",
      "iter: 70, RMSE on test set: 1.9161906622503244.\n",
      "iter: 71, RMSE on training set: 0.06718813261372038.\n",
      "iter: 71, RMSE on test set: 1.916162766624734.\n",
      "iter: 72, RMSE on training set: 0.06718775135858938.\n",
      "iter: 72, RMSE on test set: 1.9161365291789096.\n",
      "iter: 73, RMSE on training set: 0.06718744621798176.\n",
      "iter: 73, RMSE on test set: 1.9161118104500678.\n",
      "iter: 74, RMSE on training set: 0.06718720797661012.\n",
      "iter: 74, RMSE on test set: 1.9160884846570112.\n",
      "iter: 75, RMSE on training set: 0.06718702846599872.\n",
      "iter: 75, RMSE on test set: 1.9160664382589616.\n",
      "iter: 76, RMSE on training set: 0.06718690044699985.\n",
      "iter: 76, RMSE on test set: 1.9160455686725444.\n",
      "iter: 77, RMSE on training set: 0.06718681750551754.\n",
      "iter: 77, RMSE on test set: 1.9160257831290006.\n",
      "iter: 78, RMSE on training set: 0.06718677395994001.\n",
      "iter: 78, RMSE on test set: 1.916006997655823.\n",
      "iter: 79, RMSE on training set: 0.06718676477895523.\n",
      "iter: 79, RMSE on test set: 1.9159891361688106.\n",
      "iter: 80, RMSE on training set: 0.06718678550857471.\n",
      "iter: 80, RMSE on test set: 1.9159721296622287.\n",
      "iter: 81, RMSE on training set: 0.06718683220732868.\n",
      "iter: 81, RMSE on test set: 1.9159559154860824.\n",
      "iter: 82, RMSE on training set: 0.06718690138871244.\n",
      "iter: 82, RMSE on test set: 1.9159404367008752.\n",
      "iter: 83, RMSE on training set: 0.06718698997006683.\n",
      "iter: 83, RMSE on test set: 1.9159256415012518.\n",
      "iter: 84, RMSE on training set: 0.06718709522717317.\n",
      "iter: 84, RMSE on test set: 1.9159114827009225.\n",
      "iter: 85, RMSE on training set: 0.06718721475392087.\n",
      "iter: 85, RMSE on test set: 1.9158979172721577.\n",
      "iter: 86, RMSE on training set: 0.06718734642648037.\n",
      "iter: 86, RMSE on test set: 1.9158849059338432.\n",
      "iter: 87, RMSE on training set: 0.06718748837147617.\n",
      "iter: 87, RMSE on test set: 1.9158724127828217.\n",
      "iter: 88, RMSE on training set: 0.06718763893771536.\n",
      "iter: 88, RMSE on test set: 1.9158604049637904.\n",
      "iter: 89, RMSE on training set: 0.06718779667107092.\n",
      "iter: 89, RMSE on test set: 1.9158488523735846.\n",
      "iter: 90, RMSE on training set: 0.06718796029217239.\n",
      "iter: 90, RMSE on test set: 1.915837727396125.\n",
      "iter: 91, RMSE on training set: 0.06718812867658747.\n",
      "iter: 91, RMSE on test set: 1.915827004664732.\n",
      "iter: 92, RMSE on training set: 0.06718830083721711.\n",
      "iter: 92, RMSE on test set: 1.9158166608488476.\n",
      "iter: 93, RMSE on training set: 0.06718847590866024.\n",
      "iter: 93, RMSE on test set: 1.9158066744625863.\n",
      "iter: 94, RMSE on training set: 0.06718865313332507.\n",
      "iter: 94, RMSE on test set: 1.9157970256927548.\n",
      "iter: 95, RMSE on training set: 0.06718883184909523.\n",
      "iter: 95, RMSE on test set: 1.91578769624429.\n",
      "iter: 96, RMSE on training set: 0.06718901147837639.\n",
      "iter: 96, RMSE on test set: 1.9157786692012677.\n",
      "iter: 97, RMSE on training set: 0.06718919151836995.\n",
      "iter: 97, RMSE on test set: 1.9157699289018275.\n",
      "iter: 98, RMSE on training set: 0.06718937153243851.\n",
      "iter: 98, RMSE on test set: 1.915761460825578.\n",
      "iter: 99, RMSE on training set: 0.06718955114243769.\n",
      "iter: 99, RMSE on test set: 1.9157532514921283.\n",
      "iter: 100, RMSE on training set: 0.06718973002191256.\n",
      "iter: 100, RMSE on test set: 1.9157452883696293.\n",
      "iter: 101, RMSE on training set: 0.06718990789005723.\n",
      "iter: 101, RMSE on test set: 1.9157375597922712.\n",
      "iter: 102, RMSE on training set: 0.06719008450635602.\n",
      "iter: 102, RMSE on test set: 1.9157300548857912.\n",
      "iter: 103, RMSE on training set: 0.06719025966582721.\n",
      "iter: 103, RMSE on test set: 1.9157227635001846.\n",
      "iter: 104, RMSE on training set: 0.0671904331948065.\n",
      "iter: 104, RMSE on test set: 1.9157156761488776.\n",
      "iter: 105, RMSE on training set: 0.0671906049472057.\n",
      "iter: 105, RMSE on test set: 1.9157087839536895.\n",
      "iter: 106, RMSE on training set: 0.06719077480119545.\n",
      "iter: 106, RMSE on test set: 1.915702078595009.\n",
      "iter: 107, RMSE on training set: 0.06719094265626645.\n",
      "iter: 107, RMSE on test set: 1.9156955522666377.\n",
      "iter: 108, RMSE on training set: 0.06719110843062265.\n",
      "iter: 108, RMSE on test set: 1.9156891976348547.\n",
      "iter: 109, RMSE on training set: 0.0671912720588744.\n",
      "iter: 109, RMSE on test set: 1.915683007801255.\n",
      "iter: 110, RMSE on training set: 0.06719143348999411.\n",
      "iter: 110, RMSE on test set: 1.9156769762689971.\n",
      "iter: 111, RMSE on training set: 0.06719159268550616.\n",
      "iter: 111, RMSE on test set: 1.915671096912109.\n",
      "iter: 112, RMSE on training set: 0.06719174961788574.\n",
      "iter: 112, RMSE on test set: 1.9156653639475738.\n",
      "iter: 113, RMSE on training set: 0.06719190426914316.\n",
      "iter: 113, RMSE on test set: 1.9156597719098878.\n",
      "iter: 114, RMSE on training set: 0.06719205662957002.\n",
      "iter: 114, RMSE on test set: 1.915654315627892.\n",
      "iter: 115, RMSE on training set: 0.06719220669663413.\n",
      "iter: 115, RMSE on test set: 1.9156489902035911.\n",
      "iter: 116, RMSE on training set: 0.06719235447400107.\n",
      "iter: 116, RMSE on test set: 1.9156437909928545.\n",
      "iter: 117, RMSE on training set: 0.06719249997067205.\n",
      "iter: 117, RMSE on test set: 1.9156387135877286.\n",
      "iter: 118, RMSE on training set: 0.06719264320022317.\n",
      "iter: 118, RMSE on test set: 1.9156337538002823.\n",
      "iter: 119, RMSE on training set: 0.06719278418013395.\n",
      "iter: 119, RMSE on test set: 1.9156289076477848.\n",
      "iter: 120, RMSE on training set: 0.06719292293119793.\n",
      "iter: 120, RMSE on test set: 1.9156241713391216.\n",
      "iter: 121, RMSE on training set: 0.06719305947700263.\n",
      "iter: 121, RMSE on test set: 1.9156195412623223.\n",
      "iter: 122, RMSE on training set: 0.06719319384347232.\n",
      "iter: 122, RMSE on test set: 1.9156150139730985.\n",
      "iter: 123, RMSE on training set: 0.06719332605846816.\n",
      "iter: 123, RMSE on test set: 1.915610586184295.\n",
      "iter: 124, RMSE on training set: 0.06719345615143546.\n",
      "iter: 124, RMSE on test set: 1.9156062547561763.\n",
      "iter: 125, RMSE on training set: 0.06719358415309644.\n",
      "iter: 125, RMSE on test set: 1.9156020166874699.\n",
      "iter: 126, RMSE on training set: 0.067193710095181.\n",
      "iter: 126, RMSE on test set: 1.9155978691070947.\n",
      "iter: 127, RMSE on training set: 0.06719383401019127.\n",
      "iter: 127, RMSE on test set: 1.915593809266523.\n",
      "iter: 128, RMSE on training set: 0.06719395593119723.\n",
      "iter: 128, RMSE on test set: 1.915589834532708.\n",
      "iter: 129, RMSE on training set: 0.06719407589165825.\n",
      "iter: 129, RMSE on test set: 1.9155859423815353.\n",
      "iter: 130, RMSE on training set: 0.06719419392526958.\n",
      "iter: 130, RMSE on test set: 1.9155821303917528.\n",
      "iter: 131, RMSE on training set: 0.06719431006582748.\n",
      "iter: 131, RMSE on test set: 1.9155783962393327.\n",
      "iter: 132, RMSE on training set: 0.06719442434711521.\n",
      "iter: 132, RMSE on test set: 1.9155747376922285.\n",
      "iter: 133, RMSE on training set: 0.06719453680280406.\n",
      "iter: 133, RMSE on test set: 1.9155711526055088.\n",
      "iter: 134, RMSE on training set: 0.0671946474663677.\n",
      "iter: 134, RMSE on test set: 1.915567638916806.\n",
      "iter: 135, RMSE on training set: 0.0671947563710112.\n",
      "iter: 135, RMSE on test set: 1.9155641946420912.\n",
      "iter: 136, RMSE on training set: 0.06719486354960959.\n",
      "iter: 136, RMSE on test set: 1.9155608178717167.\n",
      "iter: 137, RMSE on training set: 0.06719496903465685.\n",
      "iter: 137, RMSE on test set: 1.9155575067667259.\n",
      "iter: 138, RMSE on training set: 0.06719507285822297.\n",
      "iter: 138, RMSE on test set: 1.915554259555387.\n",
      "iter: 139, RMSE on training set: 0.06719517505191867.\n",
      "iter: 139, RMSE on test set: 1.9155510745299742.\n",
      "iter: 140, RMSE on training set: 0.067195275646867.\n",
      "iter: 140, RMSE on test set: 1.9155479500437136.\n",
      "iter: 141, RMSE on training set: 0.06719537467368034.\n",
      "iter: 141, RMSE on test set: 1.9155448845079484.\n",
      "iter: 142, RMSE on training set: 0.0671954721624422.\n",
      "iter: 142, RMSE on test set: 1.9155418763894554.\n",
      "iter: 143, RMSE on training set: 0.06719556814269331.\n",
      "iter: 143, RMSE on test set: 1.9155389242079328.\n",
      "iter: 144, RMSE on training set: 0.06719566264342301.\n",
      "iter: 144, RMSE on test set: 1.915536026533631.\n",
      "iter: 145, RMSE on training set: 0.0671957556930609.\n",
      "iter: 145, RMSE on test set: 1.9155331819851174.\n",
      "iter: 146, RMSE on training set: 0.06719584731947453.\n",
      "iter: 146, RMSE on test set: 1.9155303892271744.\n",
      "iter: 147, RMSE on training set: 0.06719593754996757.\n",
      "iter: 147, RMSE on test set: 1.9155276469688114.\n",
      "iter: 148, RMSE on training set: 0.06719602641128045.\n",
      "iter: 148, RMSE on test set: 1.915524953961388.\n",
      "iter: 149, RMSE on training set: 0.06719611392959307.\n",
      "iter: 149, RMSE on test set: 1.9155223089968338.\n",
      "iter: 150, RMSE on training set: 0.06719620013052895.\n",
      "iter: 150, RMSE on test set: 1.9155197109059738.\n",
      "iter: 151, RMSE on training set: 0.06719628503916074.\n",
      "iter: 151, RMSE on test set: 1.9155171585569357.\n",
      "iter: 152, RMSE on training set: 0.06719636868001645.\n",
      "iter: 152, RMSE on test set: 1.9155146508536347.\n",
      "iter: 153, RMSE on training set: 0.06719645107708697.\n",
      "iter: 153, RMSE on test set: 1.9155121867343503.\n",
      "iter: 154, RMSE on training set: 0.06719653225383407.\n",
      "iter: 154, RMSE on test set: 1.9155097651703605.\n",
      "iter: 155, RMSE on training set: 0.06719661223319946.\n",
      "iter: 155, RMSE on test set: 1.9155073851646482.\n",
      "iter: 156, RMSE on training set: 0.06719669103761378.\n",
      "iter: 156, RMSE on test set: 1.9155050457506873.\n",
      "iter: 157, RMSE on training set: 0.06719676868900591.\n",
      "iter: 157, RMSE on test set: 1.915502745991252.\n",
      "iter: 158, RMSE on training set: 0.0671968452088136.\n",
      "iter: 158, RMSE on test set: 1.9155004849773236.\n",
      "iter: 159, RMSE on training set: 0.06719692061799284.\n",
      "iter: 159, RMSE on test set: 1.9154982618270204.\n",
      "iter: 160, RMSE on training set: 0.06719699493702866.\n",
      "iter: 160, RMSE on test set: 1.9154960756845867.\n",
      "iter: 161, RMSE on training set: 0.06719706818594497.\n",
      "iter: 161, RMSE on test set: 1.915493925719433.\n",
      "iter: 162, RMSE on training set: 0.06719714038431557.\n",
      "iter: 162, RMSE on test set: 1.9154918111252188.\n",
      "iter: 163, RMSE on training set: 0.0671972115512738.\n",
      "iter: 163, RMSE on test set: 1.915489731118962.\n",
      "iter: 164, RMSE on training set: 0.06719728170552346.\n",
      "iter: 164, RMSE on test set: 1.915487684940211.\n",
      "iter: 165, RMSE on training set: 0.06719735086534868.\n",
      "iter: 165, RMSE on test set: 1.9154856718502355.\n",
      "iter: 166, RMSE on training set: 0.06719741904862399.\n",
      "iter: 166, RMSE on test set: 1.9154836911312645.\n",
      "iter: 167, RMSE on training set: 0.0671974862728246.\n",
      "iter: 167, RMSE on test set: 1.915481742085742.\n",
      "iter: 168, RMSE on training set: 0.06719755255503623.\n",
      "iter: 168, RMSE on test set: 1.9154798240356379.\n",
      "iter: 169, RMSE on training set: 0.06719761791196424.\n",
      "iter: 169, RMSE on test set: 1.9154779363217564.\n",
      "iter: 170, RMSE on training set: 0.06719768235994439.\n",
      "iter: 170, RMSE on test set: 1.91547607830311.\n",
      "iter: 171, RMSE on training set: 0.06719774591495034.\n",
      "iter: 171, RMSE on test set: 1.915474249356284.\n",
      "iter: 172, RMSE on training set: 0.06719780859260474.\n",
      "iter: 172, RMSE on test set: 1.9154724488748516.\n",
      "iter: 173, RMSE on training set: 0.06719787040818659.\n",
      "iter: 173, RMSE on test set: 1.9154706762688034.\n",
      "iter: 174, RMSE on training set: 0.06719793137664094.\n",
      "iter: 174, RMSE on test set: 1.9154689309639983.\n",
      "iter: 175, RMSE on training set: 0.06719799151258718.\n",
      "iter: 175, RMSE on test set: 1.9154672124016436.\n",
      "iter: 176, RMSE on training set: 0.06719805083032707.\n",
      "iter: 176, RMSE on test set: 1.9154655200377824.\n",
      "iter: 177, RMSE on training set: 0.06719810934385309.\n",
      "iter: 177, RMSE on test set: 1.9154638533428234.\n",
      "iter: 178, RMSE on training set: 0.06719816706685673.\n",
      "iter: 178, RMSE on test set: 1.9154622118010585.\n",
      "iter: 179, RMSE on training set: 0.0671982240127353.\n",
      "iter: 179, RMSE on test set: 1.915460594910228.\n",
      "iter: 180, RMSE on training set: 0.06719828019460039.\n",
      "iter: 180, RMSE on test set: 1.9154590021810864.\n",
      "iter: 181, RMSE on training set: 0.06719833562528461.\n",
      "iter: 181, RMSE on test set: 1.9154574331369862.\n",
      "iter: 182, RMSE on training set: 0.06719839031734884.\n",
      "iter: 182, RMSE on test set: 1.9154558873134802.\n",
      "iter: 183, RMSE on training set: 0.06719844428308909.\n",
      "iter: 183, RMSE on test set: 1.915454364257942.\n",
      "iter: 184, RMSE on training set: 0.06719849753454359.\n",
      "iter: 184, RMSE on test set: 1.915452863529195.\n",
      "iter: 185, RMSE on training set: 0.06719855008349884.\n",
      "iter: 185, RMSE on test set: 1.915451384697149.\n",
      "iter: 186, RMSE on training set: 0.06719860194149604.\n",
      "iter: 186, RMSE on test set: 1.9154499273424752.\n",
      "iter: 187, RMSE on training set: 0.067198653119838.\n",
      "iter: 187, RMSE on test set: 1.915448491056255.\n",
      "iter: 188, RMSE on training set: 0.06719870362959347.\n",
      "iter: 188, RMSE on test set: 1.9154470754396775.\n",
      "iter: 189, RMSE on training set: 0.06719875348160566.\n",
      "iter: 189, RMSE on test set: 1.9154456801037278.\n",
      "iter: 190, RMSE on training set: 0.0671988026864949.\n",
      "iter: 190, RMSE on test set: 1.9154443046688878.\n",
      "iter: 191, RMSE on training set: 0.06719885125466629.\n",
      "iter: 191, RMSE on test set: 1.9154429487648543.\n",
      "iter: 192, RMSE on training set: 0.06719889919631426.\n",
      "iter: 192, RMSE on test set: 1.915441612030272.\n",
      "iter: 193, RMSE on training set: 0.06719894652142776.\n",
      "iter: 193, RMSE on test set: 1.915440294112449.\n",
      "iter: 194, RMSE on training set: 0.06719899323979553.\n",
      "iter: 194, RMSE on test set: 1.9154389946671138.\n",
      "iter: 195, RMSE on training set: 0.06719903936101095.\n",
      "iter: 195, RMSE on test set: 1.915437713358172.\n",
      "iter: 196, RMSE on training set: 0.06719908489447662.\n",
      "iter: 196, RMSE on test set: 1.9154364498574503.\n",
      "iter: 197, RMSE on training set: 0.06719912984940951.\n",
      "iter: 197, RMSE on test set: 1.9154352038444873.\n",
      "iter: 198, RMSE on training set: 0.06719917423484459.\n",
      "iter: 198, RMSE on test set: 1.9154339750062936.\n",
      "iter: 199, RMSE on training set: 0.06719921805964021.\n",
      "iter: 199, RMSE on test set: 1.9154327630371402.\n",
      "iter: 200, RMSE on training set: 0.06719926133248175.\n",
      "iter: 200, RMSE on test set: 1.9154315676383638.\n",
      "iter: 201, RMSE on training set: 0.06719930406188569.\n",
      "iter: 201, RMSE on test set: 1.9154303885181465.\n",
      "iter: 202, RMSE on training set: 0.06719934625620419.\n",
      "iter: 202, RMSE on test set: 1.9154292253913359.\n",
      "iter: 203, RMSE on training set: 0.0671993879236285.\n",
      "iter: 203, RMSE on test set: 1.915428077979246.\n",
      "iter: 204, RMSE on training set: 0.0671994290721926.\n",
      "iter: 204, RMSE on test set: 1.9154269460094848.\n",
      "iter: 205, RMSE on training set: 0.06719946970977764.\n",
      "iter: 205, RMSE on test set: 1.915425829215779.\n",
      "iter: 206, RMSE on training set: 0.06719950984411507.\n",
      "iter: 206, RMSE on test set: 1.9154247273377953.\n",
      "iter: 207, RMSE on training set: 0.06719954948278994.\n",
      "iter: 207, RMSE on test set: 1.9154236401209856.\n",
      "iter: 208, RMSE on training set: 0.06719958863324439.\n",
      "iter: 208, RMSE on test set: 1.9154225673164247.\n",
      "iter: 209, RMSE on training set: 0.06719962730278141.\n",
      "iter: 209, RMSE on test set: 1.9154215086806503.\n",
      "iter: 210, RMSE on training set: 0.06719966549856753.\n",
      "iter: 210, RMSE on test set: 1.9154204639755241.\n",
      "iter: 211, RMSE on training set: 0.06719970322763621.\n",
      "iter: 211, RMSE on test set: 1.9154194329680834.\n",
      "iter: 212, RMSE on training set: 0.06719974049689069.\n",
      "iter: 212, RMSE on test set: 1.915418415430401.\n",
      "iter: 213, RMSE on training set: 0.06719977731310682.\n",
      "iter: 213, RMSE on test set: 1.9154174111394506.\n",
      "iter: 214, RMSE on training set: 0.06719981368293713.\n",
      "iter: 214, RMSE on test set: 1.9154164198769739.\n",
      "iter: 215, RMSE on training set: 0.06719984961291131.\n",
      "iter: 215, RMSE on test set: 1.9154154414293592.\n",
      "iter: 216, RMSE on training set: 0.06719988510944103.\n",
      "iter: 216, RMSE on test set: 1.915414475587515.\n",
      "iter: 217, RMSE on training set: 0.06719992017882218.\n",
      "iter: 217, RMSE on test set: 1.915413522146748.\n",
      "iter: 218, RMSE on training set: 0.06719995482723638.\n",
      "iter: 218, RMSE on test set: 1.915412580906657.\n",
      "iter: 219, RMSE on training set: 0.06719998906075486.\n",
      "iter: 219, RMSE on test set: 1.915411651671015.\n",
      "iter: 220, RMSE on training set: 0.06720002288533969.\n",
      "iter: 220, RMSE on test set: 1.9154107342476556.\n",
      "iter: 221, RMSE on training set: 0.06720005630684744.\n",
      "iter: 221, RMSE on test set: 1.9154098284483796.\n",
      "iter: 222, RMSE on training set: 0.06720008933103014.\n",
      "iter: 222, RMSE on test set: 1.9154089340888485.\n",
      "iter: 223, RMSE on training set: 0.06720012196353858.\n",
      "iter: 223, RMSE on test set: 1.9154080509884805.\n",
      "iter: 224, RMSE on training set: 0.06720015420992442.\n",
      "iter: 224, RMSE on test set: 1.9154071789703606.\n",
      "iter: 225, RMSE on training set: 0.06720018607564142.\n",
      "iter: 225, RMSE on test set: 1.9154063178611513.\n",
      "iter: 226, RMSE on training set: 0.06720021756604859.\n",
      "iter: 226, RMSE on test set: 1.9154054674909933.\n",
      "iter: 227, RMSE on training set: 0.06720024868641192.\n",
      "iter: 227, RMSE on test set: 1.9154046276934265.\n",
      "iter: 228, RMSE on training set: 0.06720027944190594.\n",
      "iter: 228, RMSE on test set: 1.915403798305295.\n",
      "iter: 229, RMSE on training set: 0.06720030983761587.\n",
      "iter: 229, RMSE on test set: 1.9154029791666767.\n",
      "iter: 230, RMSE on training set: 0.06720033987854013.\n",
      "iter: 230, RMSE on test set: 1.9154021701208013.\n",
      "iter: 231, RMSE on training set: 0.06720036956959109.\n",
      "iter: 231, RMSE on test set: 1.915401371013964.\n",
      "iter: 232, RMSE on training set: 0.06720039891559812.\n",
      "iter: 232, RMSE on test set: 1.915400581695457.\n",
      "iter: 233, RMSE on training set: 0.06720042792130797.\n",
      "iter: 233, RMSE on test set: 1.9153998020174967.\n",
      "iter: 234, RMSE on training set: 0.06720045659138724.\n",
      "iter: 234, RMSE on test set: 1.9153990318351524.\n",
      "iter: 235, RMSE on training set: 0.06720048493042431.\n",
      "iter: 235, RMSE on test set: 1.9153982710062771.\n",
      "iter: 236, RMSE on training set: 0.0672005129429302.\n",
      "iter: 236, RMSE on test set: 1.915397519391439.\n",
      "iter: 237, RMSE on training set: 0.06720054063334077.\n",
      "iter: 237, RMSE on test set: 1.9153967768538571.\n",
      "iter: 238, RMSE on training set: 0.06720056800601805.\n",
      "iter: 238, RMSE on test set: 1.9153960432593402.\n",
      "iter: 239, RMSE on training set: 0.06720059506525176.\n",
      "iter: 239, RMSE on test set: 1.9153953184762251.\n",
      "iter: 240, RMSE on training set: 0.06720062181526042.\n",
      "iter: 240, RMSE on test set: 1.9153946023753121.\n",
      "iter: 241, RMSE on training set: 0.0672006482601937.\n",
      "iter: 241, RMSE on test set: 1.9153938948298168.\n",
      "iter: 242, RMSE on training set: 0.06720067440413256.\n",
      "iter: 242, RMSE on test set: 1.9153931957153034.\n",
      "iter: 243, RMSE on training set: 0.06720070025109176.\n",
      "iter: 243, RMSE on test set: 1.9153925049096372.\n",
      "iter: 244, RMSE on training set: 0.06720072580502075.\n",
      "iter: 244, RMSE on test set: 1.915391822292926.\n",
      "iter: 245, RMSE on training set: 0.06720075106980454.\n",
      "iter: 245, RMSE on test set: 1.9153911477474765.\n",
      "iter: 246, RMSE on training set: 0.06720077604926536.\n",
      "iter: 246, RMSE on test set: 1.9153904811577354.\n",
      "iter: 247, RMSE on training set: 0.06720080074716472.\n",
      "iter: 247, RMSE on test set: 1.915389822410238.\n",
      "iter: 248, RMSE on training set: 0.06720082516720283.\n",
      "iter: 248, RMSE on test set: 1.9153891713935793.\n",
      "iter: 249, RMSE on training set: 0.06720084931302123.\n",
      "iter: 249, RMSE on test set: 1.9153885279983447.\n",
      "Final RMSE on test data: 1.9153885279983447.\n",
      "train:  (1103,)\n",
      "test:  (123,)\n",
      "[   9   17   26   32   61   64   72   75   79   80   81   91   93  101  117\n",
      "  119  122  124  148  155  160  167  173  185  221  222  254  296  307  325\n",
      "  328  345  352  356  364  367  399  408  413  417  421  424  462  465  517\n",
      "  528  534  541  543  551  559  561  567  601  614  626  633  634  639  643\n",
      "  648  661  663  670  675  680  704  715  725  735  739  763  765  770  772\n",
      "  778  784  788  799  819  825  835  836  842  845  878  879  911  913  915\n",
      "  927  930  931  938  941  943  949  955  956  965  970  974  982 1007 1018\n",
      " 1022 1029 1071 1085 1101 1127 1130 1138 1143 1147 1159 1180 1188 1194 1202\n",
      " 1211 1212 1222]\n",
      "Percentage of train and test reps. :  89.9673735725938 %  10.0326264274062 %\n",
      "train index size =  1103\n",
      "test index size =  123\n",
      "nonzero elems in train:  1103\n",
      "nonzero elems in test:  123\n",
      "Original data shape:  (100, 101)\n",
      "iter: 0, RMSE on training set: 1.149321150349452.\n",
      "iter: 0, RMSE on test set: 3.4830740692516686.\n",
      "iter: 1, RMSE on training set: 0.36519267965900215.\n",
      "iter: 1, RMSE on test set: 2.312630971488335.\n",
      "iter: 2, RMSE on training set: 0.22523439975638554.\n",
      "iter: 2, RMSE on test set: 2.08467867749011.\n",
      "iter: 3, RMSE on training set: 0.1672023896638436.\n",
      "iter: 3, RMSE on test set: 1.9779895793259763.\n",
      "iter: 4, RMSE on training set: 0.13648831576326087.\n",
      "iter: 4, RMSE on test set: 1.9113946964415764.\n",
      "iter: 5, RMSE on training set: 0.1180641278080153.\n",
      "iter: 5, RMSE on test set: 1.8633054924948573.\n",
      "iter: 6, RMSE on training set: 0.10606755227774632.\n",
      "iter: 6, RMSE on test set: 1.8266590943947556.\n",
      "iter: 7, RMSE on training set: 0.09775336102761502.\n",
      "iter: 7, RMSE on test set: 1.7981906684599809.\n",
      "iter: 8, RMSE on training set: 0.09171412155443028.\n",
      "iter: 8, RMSE on test set: 1.7750326107204195.\n",
      "iter: 9, RMSE on training set: 0.08717098402927433.\n",
      "iter: 9, RMSE on test set: 1.7552490861215346.\n",
      "iter: 10, RMSE on training set: 0.08366241151852272.\n",
      "iter: 10, RMSE on test set: 1.7377892821125112.\n",
      "iter: 11, RMSE on training set: 0.08089834185588862.\n",
      "iter: 11, RMSE on test set: 1.722134398858218.\n",
      "iter: 12, RMSE on training set: 0.07868682409637023.\n",
      "iter: 12, RMSE on test set: 1.708018506718425.\n",
      "iter: 13, RMSE on training set: 0.0768952339774905.\n",
      "iter: 13, RMSE on test set: 1.6952812096368566.\n",
      "iter: 14, RMSE on training set: 0.07542880493653836.\n",
      "iter: 14, RMSE on test set: 1.6838064636210928.\n",
      "iter: 15, RMSE on training set: 0.07421801787832509.\n",
      "iter: 15, RMSE on test set: 1.673499588912509.\n",
      "iter: 16, RMSE on training set: 0.07321075446709875.\n",
      "iter: 16, RMSE on test set: 1.664276447213832.\n",
      "iter: 17, RMSE on training set: 0.07236721851859589.\n",
      "iter: 17, RMSE on test set: 1.6560565379888466.\n",
      "iter: 18, RMSE on training set: 0.07165656164448465.\n",
      "iter: 18, RMSE on test set: 1.648759126150486.\n",
      "iter: 19, RMSE on training set: 0.07105458280447864.\n",
      "iter: 19, RMSE on test set: 1.6423023486176846.\n",
      "iter: 20, RMSE on training set: 0.07054211282643834.\n",
      "iter: 20, RMSE on test set: 1.6366044112852045.\n",
      "iter: 21, RMSE on training set: 0.0701038457688314.\n",
      "iter: 21, RMSE on test set: 1.6315856445446364.\n",
      "iter: 22, RMSE on training set: 0.06972747245021836.\n",
      "iter: 22, RMSE on test set: 1.6271704723783456.\n",
      "iter: 23, RMSE on training set: 0.06940302661181691.\n",
      "iter: 23, RMSE on test set: 1.6232888292814491.\n",
      "iter: 24, RMSE on training set: 0.06912238580281813.\n",
      "iter: 24, RMSE on test set: 1.6198769318636015.\n",
      "iter: 25, RMSE on training set: 0.06887888747076741.\n",
      "iter: 25, RMSE on test set: 1.6168775064775607.\n",
      "iter: 26, RMSE on training set: 0.0686670320197392.\n",
      "iter: 26, RMSE on test set: 1.614239634065033.\n",
      "iter: 27, RMSE on training set: 0.06848225202344964.\n",
      "iter: 27, RMSE on test set: 1.6119183623102444.\n",
      "iter: 28, RMSE on training set: 0.06832073198526449.\n",
      "iter: 28, RMSE on test set: 1.6098741989779346.\n",
      "iter: 29, RMSE on training set: 0.06817926684448401.\n",
      "iter: 29, RMSE on test set: 1.6080725628348422.\n",
      "iter: 30, RMSE on training set: 0.06805515027730957.\n",
      "iter: 30, RMSE on test set: 1.6064832387501198.\n",
      "iter: 31, RMSE on training set: 0.06794608599251196.\n",
      "iter: 31, RMSE on test set: 1.6050798627902514.\n",
      "iter: 32, RMSE on training set: 0.06785011685023143.\n",
      "iter: 32, RMSE on test set: 1.6038394498074082.\n",
      "iter: 33, RMSE on training set: 0.06776556786383231.\n",
      "iter: 33, RMSE on test set: 1.6027419680175028.\n",
      "iter: 34, RMSE on training set: 0.06769100007499813.\n",
      "iter: 34, RMSE on test set: 1.6017699605310567.\n",
      "iter: 35, RMSE on training set: 0.0676251729946093.\n",
      "iter: 35, RMSE on test set: 1.600908211404936.\n",
      "iter: 36, RMSE on training set: 0.06756701383261453.\n",
      "iter: 36, RMSE on test set: 1.6001434526502702.\n",
      "iter: 37, RMSE on training set: 0.06751559214184555.\n",
      "iter: 37, RMSE on test set: 1.5994641082175372.\n",
      "iter: 38, RMSE on training set: 0.06747009880579176.\n",
      "iter: 38, RMSE on test set: 1.5988600709597258.\n",
      "iter: 39, RMSE on training set: 0.06742982853293936.\n",
      "iter: 39, RMSE on test set: 1.5983225087606137.\n",
      "iter: 40, RMSE on training set: 0.06739416519845234.\n",
      "iter: 40, RMSE on test set: 1.5978436963001734.\n",
      "iter: 41, RMSE on training set: 0.06736256951116305.\n",
      "iter: 41, RMSE on test set: 1.5974168692536985.\n",
      "iter: 42, RMSE on training set: 0.06733456859005967.\n",
      "iter: 42, RMSE on test set: 1.597036098052028.\n",
      "iter: 43, RMSE on training set: 0.06730974711716214.\n",
      "iter: 43, RMSE on test set: 1.5966961786498892.\n",
      "iter: 44, RMSE on training set: 0.06728773979843239.\n",
      "iter: 44, RMSE on test set: 1.59639253804863.\n",
      "iter: 45, RMSE on training set: 0.0672682249153557.\n",
      "iter: 45, RMSE on test set: 1.5961211525943142.\n",
      "iter: 46, RMSE on training set: 0.0672509187902048.\n",
      "iter: 46, RMSE on test set: 1.5958784773208743.\n",
      "iter: 47, RMSE on training set: 0.06723557102012032.\n",
      "iter: 47, RMSE on test set: 1.5956613848309653.\n",
      "iter: 48, RMSE on training set: 0.06722196036086159.\n",
      "iter: 48, RMSE on test set: 1.595467112405306.\n",
      "iter: 49, RMSE on training set: 0.06720989116175252.\n",
      "iter: 49, RMSE on test set: 1.5952932162063982.\n",
      "iter: 50, RMSE on training set: 0.06719919027005539.\n",
      "iter: 50, RMSE on test set: 1.595137531596326.\n",
      "iter: 51, RMSE on training set: 0.06718970433657775.\n",
      "iter: 51, RMSE on test set: 1.5949981387227827.\n",
      "iter: 52, RMSE on training set: 0.06718129746538047.\n",
      "iter: 52, RMSE on test set: 1.5948733326446085.\n",
      "iter: 53, RMSE on training set: 0.06717384915953056.\n",
      "iter: 53, RMSE on test set: 1.5947615973698144.\n",
      "iter: 54, RMSE on training set: 0.06716725252231438.\n",
      "iter: 54, RMSE on test set: 1.5946615832669766.\n",
      "iter: 55, RMSE on training set: 0.06716141267949328.\n",
      "iter: 55, RMSE on test set: 1.5945720873868303.\n",
      "iter: 56, RMSE on training set: 0.06715624539332529.\n",
      "iter: 56, RMSE on test set: 1.594492036296308.\n",
      "iter: 57, RMSE on training set: 0.06715167584333862.\n",
      "iter: 57, RMSE on test set: 1.594420471083456.\n",
      "iter: 58, RMSE on training set: 0.06714763755243901.\n",
      "iter: 58, RMSE on test set: 1.59435653423996.\n",
      "iter: 59, RMSE on training set: 0.06714407143993217.\n",
      "iter: 59, RMSE on test set: 1.594299458169371.\n",
      "iter: 60, RMSE on training set: 0.06714092498558734.\n",
      "iter: 60, RMSE on test set: 1.5942485551046122.\n",
      "iter: 61, RMSE on training set: 0.06713815149102478.\n",
      "iter: 61, RMSE on test set: 1.59420320824873.\n",
      "iter: 62, RMSE on training set: 0.06713570942653187.\n",
      "iter: 62, RMSE on test set: 1.594162863978756.\n",
      "iter: 63, RMSE on training set: 0.06713356185297906.\n",
      "iter: 63, RMSE on test set: 1.5941270249749373.\n",
      "iter: 64, RMSE on training set: 0.06713167590984075.\n",
      "iter: 64, RMSE on test set: 1.594095244156476.\n",
      "iter: 65, RMSE on training set: 0.06713002236147328.\n",
      "iter: 65, RMSE on test set: 1.594067119321321.\n",
      "iter: 66, RMSE on training set: 0.0671285751947906.\n",
      "iter: 66, RMSE on test set: 1.5940422884014913.\n",
      "iter: 67, RMSE on training set: 0.06712731126232763.\n",
      "iter: 67, RMSE on test set: 1.5940204252573227.\n",
      "iter: 68, RMSE on training set: 0.06712620996542619.\n",
      "iter: 68, RMSE on test set: 1.5940012359443572.\n",
      "iter: 69, RMSE on training set: 0.06712525297291026.\n",
      "iter: 69, RMSE on test set: 1.5939844553953364.\n",
      "iter: 70, RMSE on training set: 0.06712442397118168.\n",
      "iter: 70, RMSE on test set: 1.5939698444673533.\n",
      "iter: 71, RMSE on training set: 0.06712370844215072.\n",
      "iter: 71, RMSE on test set: 1.5939571873107257.\n",
      "iter: 72, RMSE on training set: 0.06712309346583716.\n",
      "iter: 72, RMSE on test set: 1.5939462890216876.\n",
      "iter: 73, RMSE on training set: 0.06712256754485547.\n",
      "iter: 73, RMSE on test set: 1.5939369735459459.\n",
      "iter: 74, RMSE on training set: 0.06712212044831392.\n",
      "iter: 74, RMSE on test set: 1.593929081804163.\n",
      "iter: 75, RMSE on training set: 0.06712174307295239.\n",
      "iter: 75, RMSE on test set: 1.5939224700141494.\n",
      "iter: 76, RMSE on training set: 0.06712142731958635.\n",
      "iter: 76, RMSE on test set: 1.5939170081875873.\n",
      "iter: 77, RMSE on training set: 0.06712116598315221.\n",
      "iter: 77, RMSE on test set: 1.5939125787818702.\n",
      "iter: 78, RMSE on training set: 0.06712095265483707.\n",
      "iter: 78, RMSE on test set: 1.5939090754899121.\n",
      "iter: 79, RMSE on training set: 0.06712078163495437.\n",
      "iter: 79, RMSE on test set: 1.5939064021529072.\n",
      "iter: 80, RMSE on training set: 0.06712064785537333.\n",
      "iter: 80, RMSE on test set: 1.593904471782719.\n",
      "iter: 81, RMSE on training set: 0.0671205468104468.\n",
      "iter: 81, RMSE on test set: 1.5939032056821718.\n",
      "iter: 82, RMSE on training set: 0.06712047449549796.\n",
      "iter: 82, RMSE on test set: 1.593902532652864.\n",
      "iter: 83, RMSE on training set: 0.06712042735203701.\n",
      "iter: 83, RMSE on test set: 1.5939023882812837.\n",
      "iter: 84, RMSE on training set: 0.06712040221896277.\n",
      "iter: 84, RMSE on test set: 1.5939027142950897.\n",
      "iter: 85, RMSE on training set: 0.06712039628909414.\n",
      "iter: 85, RMSE on test set: 1.5939034579822868.\n",
      "iter: 86, RMSE on training set: 0.06712040707044727.\n",
      "iter: 86, RMSE on test set: 1.5939045716668356.\n",
      "iter: 87, RMSE on training set: 0.06712043235173744.\n",
      "iter: 87, RMSE on test set: 1.5939060122349797.\n",
      "iter: 88, RMSE on training set: 0.06712047017164129.\n",
      "iter: 88, RMSE on test set: 1.5939077407071538.\n",
      "iter: 89, RMSE on training set: 0.06712051879141213.\n",
      "iter: 89, RMSE on test set: 1.593909721850875.\n",
      "iter: 90, RMSE on training set: 0.06712057667047856.\n",
      "iter: 90, RMSE on test set: 1.5939119238305806.\n",
      "iter: 91, RMSE on training set: 0.0671206424447006.\n",
      "iter: 91, RMSE on test set: 1.593914317890706.\n",
      "iter: 92, RMSE on training set: 0.06712071490699668.\n",
      "iter: 92, RMSE on test set: 1.59391687806877.\n",
      "iter: 93, RMSE on training set: 0.06712079299007852.\n",
      "iter: 93, RMSE on test set: 1.593919580935487.\n",
      "iter: 94, RMSE on training set: 0.06712087575106734.\n",
      "iter: 94, RMSE on test set: 1.5939224053593224.\n",
      "iter: 95, RMSE on training set: 0.06712096235778482.\n",
      "iter: 95, RMSE on test set: 1.5939253322930838.\n",
      "iter: 96, RMSE on training set: 0.06712105207653793.\n",
      "iter: 96, RMSE on test set: 1.5939283445804604.\n",
      "iter: 97, RMSE on training set: 0.06712114426123263.\n",
      "iter: 97, RMSE on test set: 1.5939314267805509.\n",
      "iter: 98, RMSE on training set: 0.06712123834367495.\n",
      "iter: 98, RMSE on test set: 1.5939345650087264.\n",
      "iter: 99, RMSE on training set: 0.06712133382492733.\n",
      "iter: 99, RMSE on test set: 1.5939377467921998.\n",
      "iter: 100, RMSE on training set: 0.06712143026760767.\n",
      "iter: 100, RMSE on test set: 1.59394096093898.\n",
      "iter: 101, RMSE on training set: 0.06712152728902862.\n",
      "iter: 101, RMSE on test set: 1.5939441974188961.\n",
      "iter: 102, RMSE on training set: 0.06712162455508291.\n",
      "iter: 102, RMSE on test set: 1.5939474472555846.\n",
      "iter: 103, RMSE on training set: 0.06712172177479847.\n",
      "iter: 103, RMSE on test set: 1.5939507024284048.\n",
      "iter: 104, RMSE on training set: 0.06712181869548482.\n",
      "iter: 104, RMSE on test set: 1.5939539557833498.\n",
      "iter: 105, RMSE on training set: 0.06712191509841367.\n",
      "iter: 105, RMSE on test set: 1.5939572009521261.\n",
      "iter: 106, RMSE on training set: 0.06712201079496727.\n",
      "iter: 106, RMSE on test set: 1.593960432278636.\n",
      "iter: 107, RMSE on training set: 0.06712210562321189.\n",
      "iter: 107, RMSE on test set: 1.5939636447521823.\n",
      "iter: 108, RMSE on training set: 0.06712219944484404.\n",
      "iter: 108, RMSE on test set: 1.5939668339467596.\n",
      "iter: 109, RMSE on training set: 0.06712229214247224.\n",
      "iter: 109, RMSE on test set: 1.5939699959659035.\n",
      "iter: 110, RMSE on training set: 0.06712238361719819.\n",
      "iter: 110, RMSE on test set: 1.5939731273925528.\n",
      "iter: 111, RMSE on training set: 0.06712247378646119.\n",
      "iter: 111, RMSE on test set: 1.5939762252434808.\n",
      "iter: 112, RMSE on training set: 0.06712256258212279.\n",
      "iter: 112, RMSE on test set: 1.5939792869278824.\n",
      "iter: 113, RMSE on training set: 0.06712264994876008.\n",
      "iter: 113, RMSE on test set: 1.593982310209725.\n",
      "iter: 114, RMSE on training set: 0.06712273584214873.\n",
      "iter: 114, RMSE on test set: 1.593985293173533.\n",
      "iter: 115, RMSE on training set: 0.06712282022791341.\n",
      "iter: 115, RMSE on test set: 1.5939882341932738.\n",
      "iter: 116, RMSE on training set: 0.06712290308032805.\n",
      "iter: 116, RMSE on test set: 1.5939911319040916.\n",
      "iter: 117, RMSE on training set: 0.06712298438124915.\n",
      "iter: 117, RMSE on test set: 1.5939939851766078.\n",
      "iter: 118, RMSE on training set: 0.06712306411916859.\n",
      "iter: 118, RMSE on test set: 1.5939967930935668.\n",
      "iter: 119, RMSE on training set: 0.06712314228837105.\n",
      "iter: 119, RMSE on test set: 1.5939995549286001.\n",
      "iter: 120, RMSE on training set: 0.06712321888818656.\n",
      "iter: 120, RMSE on test set: 1.5940022701269474.\n",
      "iter: 121, RMSE on training set: 0.06712329392232859.\n",
      "iter: 121, RMSE on test set: 1.594004938287919.\n",
      "iter: 122, RMSE on training set: 0.06712336739830367.\n",
      "iter: 122, RMSE on test set: 1.5940075591489815.\n",
      "iter: 123, RMSE on training set: 0.06712343932689041.\n",
      "iter: 123, RMSE on test set: 1.5940101325712928.\n",
      "iter: 124, RMSE on training set: 0.06712350972167769.\n",
      "iter: 124, RMSE on test set: 1.5940126585265555.\n",
      "iter: 125, RMSE on training set: 0.06712357859865464.\n",
      "iter: 125, RMSE on test set: 1.5940151370851086.\n",
      "iter: 126, RMSE on training set: 0.06712364597584883.\n",
      "iter: 126, RMSE on test set: 1.5940175684050752.\n",
      "iter: 127, RMSE on training set: 0.06712371187300553.\n",
      "iter: 127, RMSE on test set: 1.594019952722556.\n",
      "iter: 128, RMSE on training set: 0.06712377631130503.\n",
      "iter: 128, RMSE on test set: 1.594022290342704.\n",
      "iter: 129, RMSE on training set: 0.06712383931311316.\n",
      "iter: 129, RMSE on test set: 1.5940245816316416.\n",
      "iter: 130, RMSE on training set: 0.06712390090176086.\n",
      "iter: 130, RMSE on test set: 1.5940268270091258.\n",
      "iter: 131, RMSE on training set: 0.06712396110135097.\n",
      "iter: 131, RMSE on test set: 1.5940290269419093.\n",
      "iter: 132, RMSE on training set: 0.06712401993658687.\n",
      "iter: 132, RMSE on test set: 1.5940311819377222.\n",
      "iter: 133, RMSE on training set: 0.06712407743262422.\n",
      "iter: 133, RMSE on test set: 1.5940332925398168.\n",
      "iter: 134, RMSE on training set: 0.0671241336149389.\n",
      "iter: 134, RMSE on test set: 1.5940353593220449.\n",
      "iter: 135, RMSE on training set: 0.06712418850921238.\n",
      "iter: 135, RMSE on test set: 1.5940373828844012.\n",
      "iter: 136, RMSE on training set: 0.06712424214123316.\n",
      "iter: 136, RMSE on test set: 1.5940393638489847.\n",
      "iter: 137, RMSE on training set: 0.0671242945368075.\n",
      "iter: 137, RMSE on test set: 1.5940413028563747.\n",
      "iter: 138, RMSE on training set: 0.06712434572168559.\n",
      "iter: 138, RMSE on test set: 1.594043200562334.\n",
      "iter: 139, RMSE on training set: 0.06712439572149548.\n",
      "iter: 139, RMSE on test set: 1.5940450576348673.\n",
      "iter: 140, RMSE on training set: 0.06712444456168688.\n",
      "iter: 140, RMSE on test set: 1.594046874751546.\n",
      "iter: 141, RMSE on training set: 0.06712449226748292.\n",
      "iter: 141, RMSE on test set: 1.594048652597117.\n",
      "iter: 142, RMSE on training set: 0.06712453886383873.\n",
      "iter: 142, RMSE on test set: 1.5940503918613502.\n",
      "iter: 143, RMSE on training set: 0.06712458437540679.\n",
      "iter: 143, RMSE on test set: 1.5940520932371116.\n",
      "iter: 144, RMSE on training set: 0.06712462882650726.\n",
      "iter: 144, RMSE on test set: 1.594053757418634.\n",
      "iter: 145, RMSE on training set: 0.06712467224110354.\n",
      "iter: 145, RMSE on test set: 1.594055385099969.\n",
      "iter: 146, RMSE on training set: 0.06712471464278248.\n",
      "iter: 146, RMSE on test set: 1.5940569769736164.\n",
      "iter: 147, RMSE on training set: 0.06712475605473706.\n",
      "iter: 147, RMSE on test set: 1.5940585337292894.\n",
      "iter: 148, RMSE on training set: 0.06712479649975436.\n",
      "iter: 148, RMSE on test set: 1.5940600560528317.\n",
      "iter: 149, RMSE on training set: 0.0671248360002045.\n",
      "iter: 149, RMSE on test set: 1.594061544625249.\n",
      "iter: 150, RMSE on training set: 0.06712487457803418.\n",
      "iter: 150, RMSE on test set: 1.5940630001218608.\n",
      "iter: 151, RMSE on training set: 0.06712491225476007.\n",
      "iter: 151, RMSE on test set: 1.5940644232115508.\n",
      "iter: 152, RMSE on training set: 0.06712494905146682.\n",
      "iter: 152, RMSE on test set: 1.5940658145561135.\n",
      "iter: 153, RMSE on training set: 0.06712498498880495.\n",
      "iter: 153, RMSE on test set: 1.5940671748096866.\n",
      "iter: 154, RMSE on training set: 0.06712502008699003.\n",
      "iter: 154, RMSE on test set: 1.594068504618249.\n",
      "iter: 155, RMSE on training set: 0.06712505436580501.\n",
      "iter: 155, RMSE on test set: 1.5940698046192077.\n",
      "iter: 156, RMSE on training set: 0.06712508784460201.\n",
      "iter: 156, RMSE on test set: 1.5940710754410257.\n",
      "iter: 157, RMSE on training set: 0.0671251205423054.\n",
      "iter: 157, RMSE on test set: 1.5940723177029292.\n",
      "iter: 158, RMSE on training set: 0.06712515247741642.\n",
      "iter: 158, RMSE on test set: 1.5940735320146433.\n",
      "iter: 159, RMSE on training set: 0.06712518366801726.\n",
      "iter: 159, RMSE on test set: 1.5940747189761924.\n",
      "iter: 160, RMSE on training set: 0.06712521413177701.\n",
      "iter: 160, RMSE on test set: 1.59407587917773.\n",
      "iter: 161, RMSE on training set: 0.06712524388595774.\n",
      "iter: 161, RMSE on test set: 1.5940770131994175.\n",
      "iter: 162, RMSE on training set: 0.06712527294741945.\n",
      "iter: 162, RMSE on test set: 1.5940781216113231.\n",
      "iter: 163, RMSE on training set: 0.06712530133262874.\n",
      "iter: 163, RMSE on test set: 1.5940792049733608.\n",
      "iter: 164, RMSE on training set: 0.06712532905766391.\n",
      "iter: 164, RMSE on test set: 1.594080263835257.\n",
      "iter: 165, RMSE on training set: 0.06712535613822293.\n",
      "iter: 165, RMSE on test set: 1.5940812987365394.\n",
      "iter: 166, RMSE on training set: 0.06712538258963036.\n",
      "iter: 166, RMSE on test set: 1.5940823102065418.\n",
      "iter: 167, RMSE on training set: 0.0671254084268452.\n",
      "iter: 167, RMSE on test set: 1.5940832987644376.\n",
      "iter: 168, RMSE on training set: 0.06712543366446755.\n",
      "iter: 168, RMSE on test set: 1.594084264919284.\n",
      "iter: 169, RMSE on training set: 0.06712545831674752.\n",
      "iter: 169, RMSE on test set: 1.5940852091700883.\n",
      "iter: 170, RMSE on training set: 0.06712548239759052.\n",
      "iter: 170, RMSE on test set: 1.594086132005881.\n",
      "iter: 171, RMSE on training set: 0.06712550592056755.\n",
      "iter: 171, RMSE on test set: 1.594087033905806.\n",
      "iter: 172, RMSE on training set: 0.06712552889891994.\n",
      "iter: 172, RMSE on test set: 1.5940879153392133.\n",
      "iter: 173, RMSE on training set: 0.06712555134556884.\n",
      "iter: 173, RMSE on test set: 1.5940887767657785.\n",
      "iter: 174, RMSE on training set: 0.06712557327312148.\n",
      "iter: 174, RMSE on test set: 1.5940896186356044.\n",
      "iter: 175, RMSE on training set: 0.06712559469387891.\n",
      "iter: 175, RMSE on test set: 1.5940904413893517.\n",
      "iter: 176, RMSE on training set: 0.06712561561984301.\n",
      "iter: 176, RMSE on test set: 1.594091245458368.\n",
      "iter: 177, RMSE on training set: 0.06712563606272372.\n",
      "iter: 177, RMSE on test set: 1.5940920312648188.\n",
      "iter: 178, RMSE on training set: 0.0671256560339458.\n",
      "iter: 178, RMSE on test set: 1.5940927992218252.\n",
      "iter: 179, RMSE on training set: 0.06712567554465594.\n",
      "iter: 179, RMSE on test set: 1.5940935497336086.\n",
      "iter: 180, RMSE on training set: 0.06712569460572927.\n",
      "iter: 180, RMSE on test set: 1.594094283195633.\n",
      "iter: 181, RMSE on training set: 0.0671257132277766.\n",
      "iter: 181, RMSE on test set: 1.5940949999947485.\n",
      "iter: 182, RMSE on training set: 0.06712573142114975.\n",
      "iter: 182, RMSE on test set: 1.5940957005093428.\n",
      "iter: 183, RMSE on training set: 0.06712574919594912.\n",
      "iter: 183, RMSE on test set: 1.5940963851094938.\n",
      "iter: 184, RMSE on training set: 0.06712576656202872.\n",
      "iter: 184, RMSE on test set: 1.594097054157116.\n",
      "iter: 185, RMSE on training set: 0.06712578352900385.\n",
      "iter: 185, RMSE on test set: 1.594097708006105.\n",
      "iter: 186, RMSE on training set: 0.06712580010625535.\n",
      "iter: 186, RMSE on test set: 1.5940983470025036.\n",
      "iter: 187, RMSE on training set: 0.0671258163029368.\n",
      "iter: 187, RMSE on test set: 1.5940989714846334.\n",
      "iter: 188, RMSE on training set: 0.06712583212797897.\n",
      "iter: 188, RMSE on test set: 1.594099581783267.\n",
      "iter: 189, RMSE on training set: 0.06712584759009653.\n",
      "iter: 189, RMSE on test set: 1.5941001782217548.\n",
      "iter: 190, RMSE on training set: 0.06712586269779282.\n",
      "iter: 190, RMSE on test set: 1.5941007611161884.\n",
      "iter: 191, RMSE on training set: 0.06712587745936528.\n",
      "iter: 191, RMSE on test set: 1.5941013307755403.\n",
      "iter: 192, RMSE on training set: 0.06712589188291047.\n",
      "iter: 192, RMSE on test set: 1.594101887501813.\n",
      "iter: 193, RMSE on training set: 0.0671259059763296.\n",
      "iter: 193, RMSE on test set: 1.5941024315901808.\n",
      "iter: 194, RMSE on training set: 0.0671259197473328.\n",
      "iter: 194, RMSE on test set: 1.5941029633291348.\n",
      "iter: 195, RMSE on training set: 0.06712593320344434.\n",
      "iter: 195, RMSE on test set: 1.5941034830006233.\n",
      "iter: 196, RMSE on training set: 0.06712594635200717.\n",
      "iter: 196, RMSE on test set: 1.5941039908801844.\n",
      "iter: 197, RMSE on training set: 0.06712595920018723.\n",
      "iter: 197, RMSE on test set: 1.5941044872370944.\n",
      "iter: 198, RMSE on training set: 0.06712597175497881.\n",
      "iter: 198, RMSE on test set: 1.5941049723344887.\n",
      "iter: 199, RMSE on training set: 0.06712598402320726.\n",
      "iter: 199, RMSE on test set: 1.5941054464295121.\n",
      "iter: 200, RMSE on training set: 0.06712599601153496.\n",
      "iter: 200, RMSE on test set: 1.5941059097734327.\n",
      "iter: 201, RMSE on training set: 0.06712600772646392.\n",
      "iter: 201, RMSE on test set: 1.594106362611777.\n",
      "iter: 202, RMSE on training set: 0.06712601917434101.\n",
      "iter: 202, RMSE on test set: 1.5941068051844627.\n",
      "iter: 203, RMSE on training set: 0.06712603036136115.\n",
      "iter: 203, RMSE on test set: 1.5941072377259098.\n",
      "iter: 204, RMSE on training set: 0.06712604129357079.\n",
      "iter: 204, RMSE on test set: 1.5941076604651754.\n",
      "iter: 205, RMSE on training set: 0.06712605197687273.\n",
      "iter: 205, RMSE on test set: 1.5941080736260635.\n",
      "iter: 206, RMSE on training set: 0.06712606241702841.\n",
      "iter: 206, RMSE on test set: 1.5941084774272511.\n",
      "iter: 207, RMSE on training set: 0.06712607261966266.\n",
      "iter: 207, RMSE on test set: 1.594108872082392.\n",
      "iter: 208, RMSE on training set: 0.06712608259026595.\n",
      "iter: 208, RMSE on test set: 1.5941092578002432.\n",
      "iter: 209, RMSE on training set: 0.06712609233419863.\n",
      "iter: 209, RMSE on test set: 1.5941096347847679.\n",
      "iter: 210, RMSE on training set: 0.06712610185669364.\n",
      "iter: 210, RMSE on test set: 1.59411000323524.\n",
      "iter: 211, RMSE on training set: 0.06712611116285994.\n",
      "iter: 211, RMSE on test set: 1.5941103633463616.\n",
      "iter: 212, RMSE on training set: 0.06712612025768523.\n",
      "iter: 212, RMSE on test set: 1.5941107153083571.\n",
      "iter: 213, RMSE on training set: 0.06712612914603941.\n",
      "iter: 213, RMSE on test set: 1.5941110593070813.\n",
      "iter: 214, RMSE on training set: 0.06712613783267717.\n",
      "iter: 214, RMSE on test set: 1.5941113955241173.\n",
      "iter: 215, RMSE on training set: 0.06712614632224076.\n",
      "iter: 215, RMSE on test set: 1.5941117241368712.\n",
      "iter: 216, RMSE on training set: 0.06712615461926318.\n",
      "iter: 216, RMSE on test set: 1.5941120453186735.\n",
      "iter: 217, RMSE on training set: 0.06712616272817025.\n",
      "iter: 217, RMSE on test set: 1.594112359238868.\n",
      "iter: 218, RMSE on training set: 0.06712617065328402.\n",
      "iter: 218, RMSE on test set: 1.5941126660629084.\n",
      "iter: 219, RMSE on training set: 0.06712617839882451.\n",
      "iter: 219, RMSE on test set: 1.5941129659524367.\n",
      "iter: 220, RMSE on training set: 0.06712618596891251.\n",
      "iter: 220, RMSE on test set: 1.59411325906539.\n",
      "iter: 221, RMSE on training set: 0.06712619336757247.\n",
      "iter: 221, RMSE on test set: 1.5941135455560638.\n",
      "iter: 222, RMSE on training set: 0.06712620059873405.\n",
      "iter: 222, RMSE on test set: 1.5941138255752134.\n",
      "iter: 223, RMSE on training set: 0.06712620766623473.\n",
      "iter: 223, RMSE on test set: 1.594114099270123.\n",
      "iter: 224, RMSE on training set: 0.06712621457382269.\n",
      "iter: 224, RMSE on test set: 1.594114366784703.\n",
      "iter: 225, RMSE on training set: 0.06712622132515815.\n",
      "iter: 225, RMSE on test set: 1.5941146282595444.\n",
      "iter: 226, RMSE on training set: 0.06712622792381538.\n",
      "iter: 226, RMSE on test set: 1.5941148838320154.\n",
      "iter: 227, RMSE on training set: 0.06712623437328576.\n",
      "iter: 227, RMSE on test set: 1.5941151336363295.\n",
      "iter: 228, RMSE on training set: 0.06712624067697907.\n",
      "iter: 228, RMSE on test set: 1.5941153778036101.\n",
      "iter: 229, RMSE on training set: 0.06712624683822505.\n",
      "iter: 229, RMSE on test set: 1.5941156164619772.\n",
      "iter: 230, RMSE on training set: 0.06712625286027679.\n",
      "iter: 230, RMSE on test set: 1.5941158497366095.\n",
      "iter: 231, RMSE on training set: 0.06712625874631105.\n",
      "iter: 231, RMSE on test set: 1.5941160777498093.\n",
      "iter: 232, RMSE on training set: 0.06712626449943083.\n",
      "iter: 232, RMSE on test set: 1.594116300621071.\n",
      "iter: 233, RMSE on training set: 0.06712627012266704.\n",
      "iter: 233, RMSE on test set: 1.594116518467151.\n",
      "iter: 234, RMSE on training set: 0.06712627561898044.\n",
      "iter: 234, RMSE on test set: 1.5941167314021276.\n",
      "iter: 235, RMSE on training set: 0.0671262809912628.\n",
      "iter: 235, RMSE on test set: 1.594116939537461.\n",
      "iter: 236, RMSE on training set: 0.06712628624233882.\n",
      "iter: 236, RMSE on test set: 1.5941171429820595.\n",
      "iter: 237, RMSE on training set: 0.06712629137496806.\n",
      "iter: 237, RMSE on test set: 1.594117341842335.\n",
      "iter: 238, RMSE on training set: 0.0671262963918459.\n",
      "iter: 238, RMSE on test set: 1.5941175362222586.\n",
      "iter: 239, RMSE on training set: 0.06712630129560551.\n",
      "iter: 239, RMSE on test set: 1.5941177262234205.\n",
      "iter: 240, RMSE on training set: 0.06712630608881923.\n",
      "iter: 240, RMSE on test set: 1.5941179119450872.\n",
      "iter: 241, RMSE on training set: 0.06712631077399973.\n",
      "iter: 241, RMSE on test set: 1.5941180934842496.\n",
      "iter: 242, RMSE on training set: 0.0671263153536021.\n",
      "iter: 242, RMSE on test set: 1.5941182709356847.\n",
      "iter: 243, RMSE on training set: 0.06712631983002453.\n",
      "iter: 243, RMSE on test set: 1.5941184443919905.\n",
      "iter: 244, RMSE on training set: 0.06712632420560968.\n",
      "iter: 244, RMSE on test set: 1.5941186139436583.\n",
      "iter: 245, RMSE on training set: 0.0671263284826467.\n",
      "iter: 245, RMSE on test set: 1.594118779679105.\n",
      "iter: 246, RMSE on training set: 0.06712633266337172.\n",
      "iter: 246, RMSE on test set: 1.594118941684726.\n",
      "iter: 247, RMSE on training set: 0.06712633674996947.\n",
      "iter: 247, RMSE on test set: 1.5941191000449444.\n",
      "iter: 248, RMSE on training set: 0.06712634074457453.\n",
      "iter: 248, RMSE on test set: 1.5941192548422545.\n",
      "iter: 249, RMSE on training set: 0.06712634464927254.\n",
      "iter: 249, RMSE on test set: 1.5941194061572654.\n",
      "Final RMSE on test data: 1.5941194061572654.\n",
      "train:  (1103,)\n",
      "test:  (123,)\n",
      "[   5   14   18   30   31   37   38   63   90   99  102  106  107  108  109\n",
      "  111  113  126  134  137  146  166  172  177  182  189  217  224  225  230\n",
      "  233  236  240  241  243  244  261  267  269  280  285  315  340  344  357\n",
      "  358  363  369  372  373  386  405  432  439  443  446  448  449  453  455\n",
      "  492  499  502  515  524  526  537  550  558  568  588  605  621  623  627\n",
      "  635  647  684  694  696  711  718  728  734  740  742  748  777  798  839\n",
      "  840  864  891  899  905  907  909  919  932  937  951  967  998 1006 1038\n",
      " 1042 1045 1087 1091 1103 1110 1124 1134 1135 1148 1163 1168 1173 1184 1197\n",
      " 1213 1214 1224]\n",
      "Percentage of train and test reps. :  89.9673735725938 %  10.0326264274062 %\n",
      "train index size =  1103\n",
      "test index size =  123\n",
      "nonzero elems in train:  1103\n",
      "nonzero elems in test:  123\n",
      "Original data shape:  (100, 101)\n",
      "iter: 0, RMSE on training set: 1.0239816651450422.\n",
      "iter: 0, RMSE on test set: 3.610782513860428.\n",
      "iter: 1, RMSE on training set: 0.3519357594462633.\n",
      "iter: 1, RMSE on test set: 2.635852350717492.\n",
      "iter: 2, RMSE on training set: 0.2199430579363656.\n",
      "iter: 2, RMSE on test set: 2.279842325682508.\n",
      "iter: 3, RMSE on training set: 0.16392924577079893.\n",
      "iter: 3, RMSE on test set: 2.081090658697873.\n",
      "iter: 4, RMSE on training set: 0.13416827825439975.\n",
      "iter: 4, RMSE on test set: 1.9604022857815016.\n",
      "iter: 5, RMSE on training set: 0.11626830841438898.\n",
      "iter: 5, RMSE on test set: 1.8832585246213422.\n",
      "iter: 6, RMSE on training set: 0.10455779993706342.\n",
      "iter: 6, RMSE on test set: 1.830225526154572.\n",
      "iter: 7, RMSE on training set: 0.09642620343258113.\n",
      "iter: 7, RMSE on test set: 1.7914777986309536.\n",
      "iter: 8, RMSE on training set: 0.09052978007977358.\n",
      "iter: 8, RMSE on test set: 1.761765416236216.\n",
      "iter: 9, RMSE on training set: 0.0861118764406375.\n",
      "iter: 9, RMSE on test set: 1.7382238185334746.\n",
      "iter: 10, RMSE on training set: 0.0827149324228335.\n",
      "iter: 10, RMSE on test set: 1.7191953615601443.\n",
      "iter: 11, RMSE on training set: 0.08004739345799157.\n",
      "iter: 11, RMSE on test set: 1.7036104994481087.\n",
      "iter: 12, RMSE on training set: 0.07791610702038884.\n",
      "iter: 12, RMSE on test set: 1.6907039899772978.\n",
      "iter: 13, RMSE on training set: 0.07618885065507756.\n",
      "iter: 13, RMSE on test set: 1.6798967046457836.\n",
      "iter: 14, RMSE on training set: 0.07477239171951575.\n",
      "iter: 14, RMSE on test set: 1.6707440052943126.\n",
      "iter: 15, RMSE on training set: 0.07359922419128459.\n",
      "iter: 15, RMSE on test set: 1.6629062047104366.\n",
      "iter: 16, RMSE on training set: 0.07261932992786992.\n",
      "iter: 16, RMSE on test set: 1.6561259143708442.\n",
      "iter: 17, RMSE on training set: 0.07179491045650824.\n",
      "iter: 17, RMSE on test set: 1.6502087909932597.\n",
      "iter: 18, RMSE on training set: 0.07109691681267051.\n",
      "iter: 18, RMSE on test set: 1.6450075021229666.\n",
      "iter: 19, RMSE on training set: 0.0705026987230023.\n",
      "iter: 19, RMSE on test set: 1.6404090462380214.\n",
      "iter: 20, RMSE on training set: 0.06999437139062557.\n",
      "iter: 20, RMSE on test set: 1.6363252074611114.\n",
      "iter: 21, RMSE on training set: 0.06955765542788396.\n",
      "iter: 21, RMSE on test set: 1.632685643515519.\n",
      "iter: 22, RMSE on training set: 0.06918103686415827.\n",
      "iter: 22, RMSE on test set: 1.6294330172775124.\n",
      "iter: 23, RMSE on training set: 0.06885514870394815.\n",
      "iter: 23, RMSE on test set: 1.6265196265268558.\n",
      "iter: 24, RMSE on training set: 0.06857230897125743.\n",
      "iter: 24, RMSE on test set: 1.623905087860926.\n",
      "iter: 25, RMSE on training set: 0.0683261712473361.\n",
      "iter: 25, RMSE on test set: 1.621554740606215.\n",
      "iter: 26, RMSE on training set: 0.06811145731492144.\n",
      "iter: 26, RMSE on test set: 1.619438532186122.\n",
      "iter: 27, RMSE on training set: 0.06792375052103759.\n",
      "iter: 27, RMSE on test set: 1.617530220997997.\n",
      "iter: 28, RMSE on training set: 0.0677593345574705.\n",
      "iter: 28, RMSE on test set: 1.6158067873279798.\n",
      "iter: 29, RMSE on training set: 0.06761506656040318.\n",
      "iter: 29, RMSE on test set: 1.614247980890226.\n",
      "iter: 30, RMSE on training set: 0.06748827638411042.\n",
      "iter: 30, RMSE on test set: 1.6128359593090684.\n",
      "iter: 31, RMSE on training set: 0.06737668601080234.\n",
      "iter: 31, RMSE on test set: 1.611554988803712.\n",
      "iter: 32, RMSE on training set: 0.06727834458090799.\n",
      "iter: 32, RMSE on test set: 1.6103911892249452.\n",
      "iter: 33, RMSE on training set: 0.06719157563900245.\n",
      "iter: 33, RMSE on test set: 1.6093323124311745.\n",
      "iter: 34, RMSE on training set: 0.06711493400823204.\n",
      "iter: 34, RMSE on test set: 1.6083675471790935.\n",
      "iter: 35, RMSE on training set: 0.06704717031241286.\n",
      "iter: 35, RMSE on test set: 1.607487346199553.\n",
      "iter: 36, RMSE on training set: 0.06698720161765499.\n",
      "iter: 36, RMSE on test set: 1.6066832725702842.\n",
      "iter: 37, RMSE on training set: 0.06693408700557371.\n",
      "iter: 37, RMSE on test set: 1.605947863300532.\n",
      "iter: 38, RMSE on training set: 0.06688700714759005.\n",
      "iter: 38, RMSE on test set: 1.6052745084747808.\n",
      "iter: 39, RMSE on training set: 0.06684524714595844.\n",
      "iter: 39, RMSE on test set: 1.604657344529511.\n",
      "iter: 40, RMSE on training set: 0.06680818205769343.\n",
      "iter: 40, RMSE on test set: 1.6040911603580494.\n",
      "iter: 41, RMSE on training set: 0.06677526463396533.\n",
      "iter: 41, RMSE on test set: 1.6035713150124364.\n",
      "iter: 42, RMSE on training set: 0.06674601489820199.\n",
      "iter: 42, RMSE on test set: 1.6030936658293362.\n",
      "iter: 43, RMSE on training set: 0.06672001125731686.\n",
      "iter: 43, RMSE on test set: 1.6026545058647335.\n",
      "iter: 44, RMSE on training set: 0.0666968828967579.\n",
      "iter: 44, RMSE on test set: 1.60225050958626.\n",
      "iter: 45, RMSE on training set: 0.06667630325489729.\n",
      "iter: 45, RMSE on test set: 1.6018786858431784.\n",
      "iter: 46, RMSE on training set: 0.06665798440820249.\n",
      "iter: 46, RMSE on test set: 1.6015363372112381.\n",
      "iter: 47, RMSE on training set: 0.0666416722276206.\n",
      "iter: 47, RMSE on test set: 1.6012210248902152.\n",
      "iter: 48, RMSE on training set: 0.06662714219011047.\n",
      "iter: 48, RMSE on test set: 1.6009305384134476.\n",
      "iter: 49, RMSE on training set: 0.0666141957484409.\n",
      "iter: 49, RMSE on test set: 1.6006628695087628.\n",
      "iter: 50, RMSE on training set: 0.06660265717807806.\n",
      "iter: 50, RMSE on test set: 1.6004161895268638.\n",
      "iter: 51, RMSE on training set: 0.06659237083292359.\n",
      "iter: 51, RMSE on test set: 1.6001888299253815.\n",
      "iter: 52, RMSE on training set: 0.06658319875234393.\n",
      "iter: 52, RMSE on test set: 1.5999792653632405.\n",
      "iter: 53, RMSE on training set: 0.06657501857080003.\n",
      "iter: 53, RMSE on test set: 1.5997860990203896.\n",
      "iter: 54, RMSE on training set: 0.066567721688755.\n",
      "iter: 54, RMSE on test set: 1.5996080498122178.\n",
      "iter: 55, RMSE on training set: 0.06656121166970323.\n",
      "iter: 55, RMSE on test set: 1.5994439412159764.\n",
      "iter: 56, RMSE on training set: 0.06655540283331284.\n",
      "iter: 56, RMSE on test set: 1.599292691468799.\n",
      "iter: 57, RMSE on training set: 0.06655021901901587.\n",
      "iter: 57, RMSE on test set: 1.5991533049335933.\n",
      "iter: 58, RMSE on training set: 0.06654559249802079.\n",
      "iter: 58, RMSE on test set: 1.5990248644608125.\n",
      "iter: 59, RMSE on training set: 0.06654146301481327.\n",
      "iter: 59, RMSE on test set: 1.5989065246012972.\n",
      "iter: 60, RMSE on training set: 0.0665377769418187.\n",
      "iter: 60, RMSE on test set: 1.5987975055485166.\n",
      "iter: 61, RMSE on training set: 0.06653448653312172.\n",
      "iter: 61, RMSE on test set: 1.5986970877081448.\n",
      "iter: 62, RMSE on training set: 0.06653154926502804.\n",
      "iter: 62, RMSE on test set: 1.5986046068094544.\n",
      "iter: 63, RMSE on training set: 0.06652892725286759.\n",
      "iter: 63, RMSE on test set: 1.598519449486804.\n",
      "iter: 64, RMSE on training set: 0.06652658673482097.\n",
      "iter: 64, RMSE on test set: 1.5984410492711492.\n",
      "iter: 65, RMSE on training set: 0.06652449761473951.\n",
      "iter: 65, RMSE on test set: 1.5983688829410319.\n",
      "iter: 66, RMSE on training set: 0.0665226330569461.\n",
      "iter: 66, RMSE on test set: 1.598302467190597.\n",
      "iter: 67, RMSE on training set: 0.06652096912689257.\n",
      "iter: 67, RMSE on test set: 1.5982413555787138.\n",
      "iter: 68, RMSE on training set: 0.06651948447230123.\n",
      "iter: 68, RMSE on test set: 1.598185135728809.\n",
      "iter: 69, RMSE on training set: 0.06651816004008775.\n",
      "iter: 69, RMSE on test set: 1.5981334267534877.\n",
      "iter: 70, RMSE on training set: 0.06651697882492921.\n",
      "iter: 70, RMSE on test set: 1.5980858768817234.\n",
      "iter: 71, RMSE on training set: 0.06651592564584277.\n",
      "iter: 71, RMSE on test set: 1.598042161269537.\n",
      "iter: 72, RMSE on training set: 0.06651498694757385.\n",
      "iter: 72, RMSE on test set: 1.5980019799775342.\n",
      "iter: 73, RMSE on training set: 0.06651415062397412.\n",
      "iter: 73, RMSE on test set: 1.597965056100808.\n",
      "iter: 74, RMSE on training set: 0.0665134058608791.\n",
      "iter: 74, RMSE on test set: 1.597931134038482.\n",
      "iter: 75, RMSE on training set: 0.0665127429962876.\n",
      "iter: 75, RMSE on test set: 1.5978999778915346.\n",
      "iter: 76, RMSE on training set: 0.06651215339590069.\n",
      "iter: 76, RMSE on test set: 1.5978713699788696.\n",
      "iter: 77, RMSE on training set: 0.06651162934230369.\n",
      "iter: 77, RMSE on test set: 1.5978451094625319.\n",
      "iter: 78, RMSE on training set: 0.06651116393626444.\n",
      "iter: 78, RMSE on test set: 1.5978210110738742.\n",
      "iter: 79, RMSE on training set: 0.06651075100880774.\n",
      "iter: 79, RMSE on test set: 1.597798903933235.\n",
      "iter: 80, RMSE on training set: 0.06651038504286597.\n",
      "iter: 80, RMSE on test set: 1.5977786304563277.\n",
      "iter: 81, RMSE on training set: 0.06651006110345019.\n",
      "iter: 81, RMSE on test set: 1.597760045341042.\n",
      "iter: 82, RMSE on training set: 0.06650977477539986.\n",
      "iter: 82, RMSE on test set: 1.5977430146289744.\n",
      "iter: 83, RMSE on training set: 0.06650952210787915.\n",
      "iter: 83, RMSE on test set: 1.5977274148363094.\n",
      "iter: 84, RMSE on training set: 0.0665092995648772.\n",
      "iter: 84, RMSE on test set: 1.5977131321491356.\n",
      "iter: 85, RMSE on training set: 0.06650910398105667.\n",
      "iter: 85, RMSE on test set: 1.5977000616786465.\n",
      "iter: 86, RMSE on training set: 0.06650893252236152.\n",
      "iter: 86, RMSE on test set: 1.5976881067719453.\n",
      "iter: 87, RMSE on training set: 0.06650878265086993.\n",
      "iter: 87, RMSE on test set: 1.5976771783744947.\n",
      "iter: 88, RMSE on training set: 0.06650865209342453.\n",
      "iter: 88, RMSE on test set: 1.5976671944405505.\n",
      "iter: 89, RMSE on training set: 0.06650853881363182.\n",
      "iter: 89, RMSE on test set: 1.5976580793880795.\n",
      "iter: 90, RMSE on training set: 0.06650844098686379.\n",
      "iter: 90, RMSE on test set: 1.5976497635950548.\n",
      "iter: 91, RMSE on training set: 0.06650835697793601.\n",
      "iter: 91, RMSE on test set: 1.5976421829340166.\n",
      "iter: 92, RMSE on training set: 0.06650828532117246.\n",
      "iter: 92, RMSE on test set: 1.597635278342216.\n",
      "iter: 93, RMSE on training set: 0.0665082247025984.\n",
      "iter: 93, RMSE on test set: 1.5976289954246832.\n",
      "iter: 94, RMSE on training set: 0.06650817394403177.\n",
      "iter: 94, RMSE on test set: 1.597623284087815.\n",
      "iter: 95, RMSE on training set: 0.06650813198886862.\n",
      "iter: 95, RMSE on test set: 1.5976180982012345.\n",
      "iter: 96, RMSE on training set: 0.06650809788937871.\n",
      "iter: 96, RMSE on test set: 1.5976133952858118.\n",
      "iter: 97, RMSE on training set: 0.06650807079535086.\n",
      "iter: 97, RMSE on test set: 1.597609136225897.\n",
      "iter: 98, RMSE on training set: 0.06650804994393993.\n",
      "iter: 98, RMSE on test set: 1.5976052850039468.\n",
      "iter: 99, RMSE on training set: 0.06650803465058995.\n",
      "iter: 99, RMSE on test set: 1.597601808455857.\n",
      "iter: 100, RMSE on training set: 0.06650802430091558.\n",
      "iter: 100, RMSE on test set: 1.5975986760454246.\n",
      "iter: 101, RMSE on training set: 0.06650801834343985.\n",
      "iter: 101, RMSE on test set: 1.5975958596565072.\n",
      "iter: 102, RMSE on training set: 0.06650801628309715.\n",
      "iter: 102, RMSE on test set: 1.5975933334014836.\n",
      "iter: 103, RMSE on training set: 0.06650801767541867.\n",
      "iter: 103, RMSE on test set: 1.5975910734448142.\n",
      "iter: 104, RMSE on training set: 0.06650802212132974.\n",
      "iter: 104, RMSE on test set: 1.5975890578404888.\n",
      "iter: 105, RMSE on training set: 0.06650802926248932.\n",
      "iter: 105, RMSE on test set: 1.597587266382335.\n",
      "iter: 106, RMSE on training set: 0.06650803877712132.\n",
      "iter: 106, RMSE on test set: 1.5975856804661501.\n",
      "iter: 107, RMSE on training set: 0.06650805037627662.\n",
      "iter: 107, RMSE on test set: 1.5975842829627431.\n",
      "iter: 108, RMSE on training set: 0.06650806380048774.\n",
      "iter: 108, RMSE on test set: 1.597583058101068.\n",
      "iter: 109, RMSE on training set: 0.06650807881676982.\n",
      "iter: 109, RMSE on test set: 1.5975819913606033.\n",
      "iter: 110, RMSE on training set: 0.06650809521593377.\n",
      "iter: 110, RMSE on test set: 1.5975810693723071.\n",
      "iter: 111, RMSE on training set: 0.0665081128101771.\n",
      "iter: 111, RMSE on test set: 1.5975802798274064.\n",
      "iter: 112, RMSE on training set: 0.06650813143092414.\n",
      "iter: 112, RMSE on test set: 1.5975796113934917.\n",
      "iter: 113, RMSE on training set: 0.06650815092688858.\n",
      "iter: 113, RMSE on test set: 1.597579053637245.\n",
      "iter: 114, RMSE on training set: 0.06650817116233612.\n",
      "iter: 114, RMSE on test set: 1.5975785969533298.\n",
      "iter: 115, RMSE on training set: 0.06650819201552409.\n",
      "iter: 115, RMSE on test set: 1.5975782324989656.\n",
      "iter: 116, RMSE on training set: 0.06650821337730295.\n",
      "iter: 116, RMSE on test set: 1.5975779521336784.\n",
      "iter: 117, RMSE on training set: 0.06650823514985817.\n",
      "iter: 117, RMSE on test set: 1.5975777483638696.\n",
      "iter: 118, RMSE on training set: 0.06650825724558312.\n",
      "iter: 118, RMSE on test set: 1.5975776142917941.\n",
      "iter: 119, RMSE on training set: 0.06650827958606317.\n",
      "iter: 119, RMSE on test set: 1.5975775435685993.\n",
      "iter: 120, RMSE on training set: 0.06650830210116625.\n",
      "iter: 120, RMSE on test set: 1.5975775303510982.\n",
      "iter: 121, RMSE on training set: 0.06650832472822261.\n",
      "iter: 121, RMSE on test set: 1.5975775692619845.\n",
      "iter: 122, RMSE on training set: 0.06650834741128887.\n",
      "iter: 122, RMSE on test set: 1.5975776553532095.\n",
      "iter: 123, RMSE on training set: 0.06650837010048544.\n",
      "iter: 123, RMSE on test set: 1.597577784072269.\n",
      "iter: 124, RMSE on training set: 0.06650839275139984.\n",
      "iter: 124, RMSE on test set: 1.597577951231168.\n",
      "iter: 125, RMSE on training set: 0.06650841532455096.\n",
      "iter: 125, RMSE on test set: 1.5975781529778468.\n",
      "iter: 126, RMSE on training set: 0.06650843778490537.\n",
      "iter: 126, RMSE on test set: 1.597578385769874.\n",
      "iter: 127, RMSE on training set: 0.0665084601014422.\n",
      "iter: 127, RMSE on test set: 1.5975786463502202.\n",
      "iter: 128, RMSE on training set: 0.06650848224676144.\n",
      "iter: 128, RMSE on test set: 1.5975789317249494.\n",
      "iter: 129, RMSE on training set: 0.06650850419672999.\n",
      "iter: 129, RMSE on test set: 1.5975792391426704.\n",
      "iter: 130, RMSE on training set: 0.06650852593016311.\n",
      "iter: 130, RMSE on test set: 1.5975795660756225.\n",
      "iter: 131, RMSE on training set: 0.06650854742853703.\n",
      "iter: 131, RMSE on test set: 1.5975799102022383.\n",
      "iter: 132, RMSE on training set: 0.06650856867572978.\n",
      "iter: 132, RMSE on test set: 1.597580269391088.\n",
      "iter: 133, RMSE on training set: 0.06650858965778586.\n",
      "iter: 133, RMSE on test set: 1.5975806416860736.\n",
      "iter: 134, RMSE on training set: 0.0665086103627068.\n",
      "iter: 134, RMSE on test set: 1.5975810252928149.\n",
      "iter: 135, RMSE on training set: 0.06650863078025843.\n",
      "iter: 135, RMSE on test set: 1.5975814185660675.\n",
      "iter: 136, RMSE on training set: 0.06650865090179817.\n",
      "iter: 136, RMSE on test set: 1.5975818199981444.\n",
      "iter: 137, RMSE on training set: 0.06650867072012003.\n",
      "iter: 137, RMSE on test set: 1.59758222820825.\n",
      "iter: 138, RMSE on training set: 0.06650869022931273.\n",
      "iter: 138, RMSE on test set: 1.5975826419326278.\n",
      "iter: 139, RMSE on training set: 0.06650870942463204.\n",
      "iter: 139, RMSE on test set: 1.5975830600154894.\n",
      "iter: 140, RMSE on training set: 0.06650872830238491.\n",
      "iter: 140, RMSE on test set: 1.5975834814006433.\n",
      "iter: 141, RMSE on training set: 0.06650874685982504.\n",
      "iter: 141, RMSE on test set: 1.5975839051237803.\n",
      "iter: 142, RMSE on training set: 0.06650876509505821.\n",
      "iter: 142, RMSE on test set: 1.597584330305345.\n",
      "iter: 143, RMSE on training set: 0.06650878300695601.\n",
      "iter: 143, RMSE on test set: 1.597584756143972.\n",
      "iter: 144, RMSE on training set: 0.06650880059507781.\n",
      "iter: 144, RMSE on test set: 1.5975851819104188.\n",
      "iter: 145, RMSE on training set: 0.06650881785960046.\n",
      "iter: 145, RMSE on test set: 1.5975856069419756.\n",
      "iter: 146, RMSE on training set: 0.06650883480125454.\n",
      "iter: 146, RMSE on test set: 1.5975860306372982.\n",
      "iter: 147, RMSE on training set: 0.06650885142126506.\n",
      "iter: 147, RMSE on test set: 1.5975864524516459.\n",
      "iter: 148, RMSE on training set: 0.06650886772129969.\n",
      "iter: 148, RMSE on test set: 1.597586871892468.\n",
      "iter: 149, RMSE on training set: 0.06650888370342097.\n",
      "iter: 149, RMSE on test set: 1.5975872885153504.\n",
      "iter: 150, RMSE on training set: 0.06650889937004205.\n",
      "iter: 150, RMSE on test set: 1.5975877019202591.\n",
      "iter: 151, RMSE on training set: 0.06650891472388755.\n",
      "iter: 151, RMSE on test set: 1.5975881117480666.\n",
      "iter: 152, RMSE on training set: 0.06650892976795826.\n",
      "iter: 152, RMSE on test set: 1.5975885176773603.\n",
      "iter: 153, RMSE on training set: 0.06650894450549712.\n",
      "iter: 153, RMSE on test set: 1.5975889194214696.\n",
      "iter: 154, RMSE on training set: 0.06650895893996138.\n",
      "iter: 154, RMSE on test set: 1.5975893167257362.\n",
      "iter: 155, RMSE on training set: 0.06650897307499429.\n",
      "iter: 155, RMSE on test set: 1.5975897093649851.\n",
      "iter: 156, RMSE on training set: 0.06650898691440114.\n",
      "iter: 156, RMSE on test set: 1.5975900971411976.\n",
      "iter: 157, RMSE on training set: 0.06650900046212685.\n",
      "iter: 157, RMSE on test set: 1.59759047988133.\n",
      "iter: 158, RMSE on training set: 0.06650901372223682.\n",
      "iter: 158, RMSE on test set: 1.5975908574353357.\n",
      "iter: 159, RMSE on training set: 0.06650902669889683.\n",
      "iter: 159, RMSE on test set: 1.5975912296743098.\n",
      "iter: 160, RMSE on training set: 0.06650903939635751.\n",
      "iter: 160, RMSE on test set: 1.5975915964887797.\n",
      "iter: 161, RMSE on training set: 0.0665090518189386.\n",
      "iter: 161, RMSE on test set: 1.5975919577871351.\n",
      "iter: 162, RMSE on training set: 0.0665090639710162.\n",
      "iter: 162, RMSE on test set: 1.5975923134941488.\n",
      "iter: 163, RMSE on training set: 0.06650907585700909.\n",
      "iter: 163, RMSE on test set: 1.5975926635496511.\n",
      "iter: 164, RMSE on training set: 0.06650908748136812.\n",
      "iter: 164, RMSE on test set: 1.5975930079072556.\n",
      "iter: 165, RMSE on training set: 0.06650909884856555.\n",
      "iter: 165, RMSE on test set: 1.5975933465332106.\n",
      "iter: 166, RMSE on training set: 0.06650910996308688.\n",
      "iter: 166, RMSE on test set: 1.5975936794053376.\n",
      "iter: 167, RMSE on training set: 0.06650912082942.\n",
      "iter: 167, RMSE on test set: 1.5975940065120273.\n",
      "iter: 168, RMSE on training set: 0.06650913145205024.\n",
      "iter: 168, RMSE on test set: 1.5975943278513391.\n",
      "iter: 169, RMSE on training set: 0.06650914183545238.\n",
      "iter: 169, RMSE on test set: 1.5975946434301413.\n",
      "iter: 170, RMSE on training set: 0.06650915198408368.\n",
      "iter: 170, RMSE on test set: 1.5975949532633271.\n",
      "iter: 171, RMSE on training set: 0.06650916190237993.\n",
      "iter: 171, RMSE on test set: 1.5975952573730974.\n",
      "iter: 172, RMSE on training set: 0.06650917159474859.\n",
      "iter: 172, RMSE on test set: 1.5975955557882842.\n",
      "iter: 173, RMSE on training set: 0.06650918106556637.\n",
      "iter: 173, RMSE on test set: 1.5975958485437332.\n",
      "iter: 174, RMSE on training set: 0.06650919031917317.\n",
      "iter: 174, RMSE on test set: 1.5975961356797252.\n",
      "iter: 175, RMSE on training set: 0.06650919935986979.\n",
      "iter: 175, RMSE on test set: 1.5975964172414472.\n",
      "iter: 176, RMSE on training set: 0.06650920819191442.\n",
      "iter: 176, RMSE on test set: 1.5975966932784988.\n",
      "iter: 177, RMSE on training set: 0.0665092168195189.\n",
      "iter: 177, RMSE on test set: 1.5975969638444458.\n",
      "iter: 178, RMSE on training set: 0.0665092252468484.\n",
      "iter: 178, RMSE on test set: 1.5975972289963958.\n",
      "iter: 179, RMSE on training set: 0.06650923347801622.\n",
      "iter: 179, RMSE on test set: 1.5975974887945994.\n",
      "iter: 180, RMSE on training set: 0.06650924151708448.\n",
      "iter: 180, RMSE on test set: 1.5975977433021127.\n",
      "iter: 181, RMSE on training set: 0.06650924936806131.\n",
      "iter: 181, RMSE on test set: 1.5975979925844508.\n",
      "iter: 182, RMSE on training set: 0.06650925703489909.\n",
      "iter: 182, RMSE on test set: 1.5975982367092865.\n",
      "iter: 183, RMSE on training set: 0.06650926452149382.\n",
      "iter: 183, RMSE on test set: 1.5975984757461712.\n",
      "iter: 184, RMSE on training set: 0.06650927183168374.\n",
      "iter: 184, RMSE on test set: 1.5975987097662643.\n",
      "Final RMSE on test data: 1.5975984757461712.\n",
      "train:  (1103,)\n",
      "test:  (123,)\n",
      "[  34   46   50   53   57   58   60   76   84   89   97  123  144  145  149\n",
      "  151  157  175  176  186  199  228  234  256  257  265  274  288  298  317\n",
      "  319  336  349  360  414  422  430  435  440  450  451  457  460  468  475\n",
      "  488  491  513  519  520  521  522  527  538  539  587  602  603  606  629\n",
      "  630  645  652  653  657  658  664  665  668  681  682  688  697  701  743\n",
      "  755  766  792  802  812  850  851  852  856  867  871  872  880  886  896\n",
      "  897  900  910  917  926  935  952  964  977  984  985  992 1009 1017 1037\n",
      " 1051 1063 1068 1095 1100 1104 1115 1125 1131 1146 1156 1157 1160 1169 1178\n",
      " 1187 1198 1216]\n",
      "Percentage of train and test reps. :  89.9673735725938 %  10.0326264274062 %\n",
      "train index size =  1103\n",
      "test index size =  123\n",
      "nonzero elems in train:  1103\n",
      "nonzero elems in test:  123\n",
      "Original data shape:  (100, 101)\n",
      "iter: 0, RMSE on training set: 1.0854607398278282.\n",
      "iter: 0, RMSE on test set: 3.412505990539157.\n",
      "iter: 1, RMSE on training set: 0.34524755688631903.\n",
      "iter: 1, RMSE on test set: 2.42949353826909.\n",
      "iter: 2, RMSE on training set: 0.21409155744276176.\n",
      "iter: 2, RMSE on test set: 2.143623657984435.\n",
      "iter: 3, RMSE on training set: 0.1608195979312426.\n",
      "iter: 3, RMSE on test set: 1.9783586531279795.\n",
      "iter: 4, RMSE on training set: 0.13250359832482939.\n",
      "iter: 4, RMSE on test set: 1.8876067149180538.\n",
      "iter: 5, RMSE on training set: 0.11530978825081016.\n",
      "iter: 5, RMSE on test set: 1.8342498585652973.\n",
      "iter: 6, RMSE on training set: 0.10408002232317894.\n",
      "iter: 6, RMSE on test set: 1.7995396615510006.\n",
      "iter: 7, RMSE on training set: 0.09632515763397198.\n",
      "iter: 7, RMSE on test set: 1.7741531777847672.\n",
      "iter: 8, RMSE on training set: 0.09070946618348652.\n",
      "iter: 8, RMSE on test set: 1.7534532921716048.\n",
      "iter: 9, RMSE on training set: 0.08648825516362811.\n",
      "iter: 9, RMSE on test set: 1.7354331823437412.\n",
      "iter: 10, RMSE on training set: 0.08322423514130531.\n",
      "iter: 10, RMSE on test set: 1.7193120306575156.\n",
      "iter: 11, RMSE on training set: 0.08064495671914008.\n",
      "iter: 11, RMSE on test set: 1.7047919777157778.\n",
      "iter: 12, RMSE on training set: 0.07857155507908975.\n",
      "iter: 12, RMSE on test set: 1.6917436828746615.\n",
      "iter: 13, RMSE on training set: 0.07688163941156895.\n",
      "iter: 13, RMSE on test set: 1.6800847679089292.\n",
      "iter: 14, RMSE on training set: 0.07548869687221872.\n",
      "iter: 14, RMSE on test set: 1.6697352438799276.\n",
      "iter: 15, RMSE on training set: 0.07432990442281764.\n",
      "iter: 15, RMSE on test set: 1.6606050261812193.\n",
      "iter: 16, RMSE on training set: 0.07335854729122981.\n",
      "iter: 16, RMSE on test set: 1.6525949572282024.\n",
      "iter: 17, RMSE on training set: 0.07253914197767469.\n",
      "iter: 17, RMSE on test set: 1.6456020933244688.\n",
      "iter: 18, RMSE on training set: 0.07184422325911942.\n",
      "iter: 18, RMSE on test set: 1.639524748240142.\n",
      "iter: 19, RMSE on training set: 0.07125218278439904.\n",
      "iter: 19, RMSE on test set: 1.6342657799842477.\n",
      "iter: 20, RMSE on training set: 0.07074578387002012.\n",
      "iter: 20, RMSE on test set: 1.6297342067107627.\n",
      "iter: 21, RMSE on training set: 0.07031111793194844.\n",
      "iter: 21, RMSE on test set: 1.6258457641354098.\n",
      "iter: 22, RMSE on training set: 0.06993685430026648.\n",
      "iter: 22, RMSE on test set: 1.6225229635389335.\n",
      "iter: 23, RMSE on training set: 0.06961368874693011.\n",
      "iter: 23, RMSE on test set: 1.6196949799532279.\n",
      "iter: 24, RMSE on training set: 0.06933392957404585.\n",
      "iter: 24, RMSE on test set: 1.6172975015953552.\n",
      "iter: 25, RMSE on training set: 0.0690911811940558.\n",
      "iter: 25, RMSE on test set: 1.6152725600635716.\n",
      "iter: 26, RMSE on training set: 0.06888009846355946.\n",
      "iter: 26, RMSE on test set: 1.6135683209690777.\n",
      "iter: 27, RMSE on training set: 0.06869619352095033.\n",
      "iter: 27, RMSE on test set: 1.6121388144950337.\n",
      "iter: 28, RMSE on training set: 0.068535682350048.\n",
      "iter: 28, RMSE on test set: 1.6109435994358043.\n",
      "iter: 29, RMSE on training set: 0.06839536188761601.\n",
      "iter: 29, RMSE on test set: 1.6099473684988728.\n",
      "iter: 30, RMSE on training set: 0.06827251091632532.\n",
      "iter: 30, RMSE on test set: 1.6091195116872712.\n",
      "iter: 31, RMSE on training set: 0.06816480966639347.\n",
      "iter: 31, RMSE on test set: 1.6084336579195795.\n",
      "iter: 32, RMSE on training set: 0.06807027425005124.\n",
      "iter: 32, RMSE on test set: 1.6078672141598755.\n",
      "iter: 33, RMSE on training set: 0.06798720293279893.\n",
      "iter: 33, RMSE on test set: 1.6074009180488869.\n",
      "iter: 34, RMSE on training set: 0.06791413190348577.\n",
      "iter: 34, RMSE on test set: 1.6070184158341874.\n",
      "iter: 35, RMSE on training set: 0.06784979870530272.\n",
      "iter: 35, RMSE on test set: 1.60670587326223.\n",
      "iter: 36, RMSE on training set: 0.06779311187422221.\n",
      "iter: 36, RMSE on test set: 1.606451623534583.\n",
      "iter: 37, RMSE on training set: 0.06774312562954207.\n",
      "iter: 37, RMSE on test set: 1.6062458536414261.\n",
      "iter: 38, RMSE on training set: 0.06769901869387884.\n",
      "iter: 38, RMSE on test set: 1.606080328368459.\n",
      "iter: 39, RMSE on training set: 0.06766007650256892.\n",
      "iter: 39, RMSE on test set: 1.6059481499320105.\n",
      "iter: 40, RMSE on training set: 0.06762567620638955.\n",
      "iter: 40, RMSE on test set: 1.605843550397301.\n",
      "iter: 41, RMSE on training set: 0.06759527398548244.\n",
      "iter: 41, RMSE on test set: 1.605761713641439.\n",
      "iter: 42, RMSE on training set: 0.06756839428297122.\n",
      "iter: 42, RMSE on test set: 1.6056986235169706.\n",
      "iter: 43, RMSE on training set: 0.06754462063906273.\n",
      "iter: 43, RMSE on test set: 1.6056509349576173.\n",
      "iter: 44, RMSE on training set: 0.06752358786436696.\n",
      "iter: 44, RMSE on test set: 1.6056158649713292.\n",
      "iter: 45, RMSE on training set: 0.0675049753377624.\n",
      "iter: 45, RMSE on test set: 1.6055911007339967.\n",
      "iter: 46, RMSE on training set: 0.06748850125176485.\n",
      "iter: 46, RMSE on test set: 1.605574722292392.\n",
      "iter: 47, RMSE on training set: 0.06747391765884947.\n",
      "iter: 47, RMSE on test set: 1.605565137682786.\n",
      "iter: 48, RMSE on training set: 0.06746100619698599.\n",
      "iter: 48, RMSE on test set: 1.6055610285568345.\n",
      "iter: 49, RMSE on training set: 0.06744957439289244.\n",
      "iter: 49, RMSE on test set: 1.6055613046696013.\n",
      "iter: 50, RMSE on training set: 0.06743945245812204.\n",
      "iter: 50, RMSE on test set: 1.6055650658221614.\n",
      "iter: 51, RMSE on training set: 0.06743049050672648.\n",
      "iter: 51, RMSE on test set: 1.6055715700614295.\n",
      "iter: 52, RMSE on training set: 0.06742255613451163.\n",
      "iter: 52, RMSE on test set: 1.6055802071234735.\n",
      "iter: 53, RMSE on training set: 0.06741553230921155.\n",
      "iter: 53, RMSE on test set: 1.6055904762652053.\n",
      "iter: 54, RMSE on training set: 0.06740931552865577.\n",
      "iter: 54, RMSE on test set: 1.605601967765283.\n",
      "iter: 55, RMSE on training set: 0.06740381421045369.\n",
      "iter: 55, RMSE on test set: 1.605614347490818.\n",
      "iter: 56, RMSE on training set: 0.06739894728211299.\n",
      "iter: 56, RMSE on test set: 1.6056273440244535.\n",
      "iter: 57, RMSE on training set: 0.06739464294503597.\n",
      "iter: 57, RMSE on test set: 1.6056407379291058.\n",
      "iter: 58, RMSE on training set: 0.06739083758963854.\n",
      "iter: 58, RMSE on test set: 1.6056543527971083.\n",
      "iter: 59, RMSE on training set: 0.06738747484205382.\n",
      "iter: 59, RMSE on test set: 1.6056680477888627.\n",
      "iter: 60, RMSE on training set: 0.06738450472559393.\n",
      "iter: 60, RMSE on test set: 1.6056817114147828.\n",
      "iter: 61, RMSE on training set: 0.06738188292245609.\n",
      "iter: 61, RMSE on test set: 1.605695256355175.\n",
      "iter: 62, RMSE on training set: 0.06737957012311616.\n",
      "iter: 62, RMSE on test set: 1.605708615146685.\n",
      "iter: 63, RMSE on training set: 0.0673775314525363.\n",
      "iter: 63, RMSE on test set: 1.6057217365923044.\n",
      "iter: 64, RMSE on training set: 0.0673757359637365.\n",
      "iter: 64, RMSE on test set: 1.6057345827756444.\n",
      "iter: 65, RMSE on training set: 0.06737415619052227.\n",
      "iter: 65, RMSE on test set: 1.6057471265798602.\n",
      "iter: 66, RMSE on training set: 0.06737276775220442.\n",
      "iter: 66, RMSE on test set: 1.6057593496280904.\n",
      "iter: 67, RMSE on training set: 0.0673715490040703.\n",
      "iter: 67, RMSE on test set: 1.6057712405759204.\n",
      "iter: 68, RMSE on training set: 0.06737048072814726.\n",
      "iter: 68, RMSE on test set: 1.6057827936978437.\n",
      "iter: 69, RMSE on training set: 0.06736954585947742.\n",
      "iter: 69, RMSE on test set: 1.6057940077192312.\n",
      "iter: 70, RMSE on training set: 0.06736872924372284.\n",
      "iter: 70, RMSE on test set: 1.6058048848532038.\n",
      "iter: 71, RMSE on training set: 0.0673680174224229.\n",
      "iter: 71, RMSE on test set: 1.6058154300084935.\n",
      "iter: 72, RMSE on training set: 0.06736739844268154.\n",
      "iter: 72, RMSE on test set: 1.6058256501399122.\n",
      "iter: 73, RMSE on training set: 0.06736686168844616.\n",
      "iter: 73, RMSE on test set: 1.60583555371763.\n",
      "iter: 74, RMSE on training set: 0.06736639773088032.\n",
      "iter: 74, RMSE on test set: 1.6058451502954134.\n",
      "iter: 75, RMSE on training set: 0.06736599819563356.\n",
      "iter: 75, RMSE on test set: 1.6058544501611265.\n",
      "iter: 76, RMSE on training set: 0.0673656556450674.\n",
      "iter: 76, RMSE on test set: 1.6058634640555824.\n",
      "iter: 77, RMSE on training set: 0.06736536347372846.\n",
      "iter: 77, RMSE on test set: 1.6058722029480572.\n",
      "iter: 78, RMSE on training set: 0.06736511581555671.\n",
      "iter: 78, RMSE on test set: 1.6058806778586858.\n",
      "iter: 79, RMSE on training set: 0.06736490746149634.\n",
      "iter: 79, RMSE on test set: 1.6058888997195906.\n",
      "iter: 80, RMSE on training set: 0.06736473378633005.\n",
      "iter: 80, RMSE on test set: 1.6058968792678765.\n",
      "iter: 81, RMSE on training set: 0.06736459068368902.\n",
      "iter: 81, RMSE on test set: 1.605904626964798.\n",
      "iter: 82, RMSE on training set: 0.06736447450832343.\n",
      "iter: 82, RMSE on test set: 1.6059121529363158.\n",
      "iter: 83, RMSE on training set: 0.06736438202480902.\n",
      "iter: 83, RMSE on test set: 1.6059194669310477.\n",
      "iter: 84, RMSE on training set: 0.0673643103619695.\n",
      "iter: 84, RMSE on test set: 1.6059265782923269.\n",
      "iter: 85, RMSE on training set: 0.06736425697237307.\n",
      "iter: 85, RMSE on test set: 1.6059334959415756.\n",
      "iter: 86, RMSE on training set: 0.06736421959633326.\n",
      "iter: 86, RMSE on test set: 1.6059402283707127.\n",
      "iter: 87, RMSE on training set: 0.06736419622991265.\n",
      "iter: 87, RMSE on test set: 1.6059467836417067.\n",
      "iter: 88, RMSE on training set: 0.0673641850964789.\n",
      "iter: 88, RMSE on test set: 1.605953169391671.\n",
      "iter: 89, RMSE on training set: 0.0673641846214196.\n",
      "iter: 89, RMSE on test set: 1.6059593928422422.\n",
      "iter: 90, RMSE on training set: 0.06736419340966261.\n",
      "iter: 90, RMSE on test set: 1.6059654608121556.\n",
      "iter: 91, RMSE on training set: 0.06736421022568975.\n",
      "iter: 91, RMSE on test set: 1.6059713797321489.\n",
      "iter: 92, RMSE on training set: 0.06736423397576594.\n",
      "iter: 92, RMSE on test set: 1.6059771556614924.\n",
      "iter: 93, RMSE on training set: 0.06736426369213935.\n",
      "iter: 93, RMSE on test set: 1.6059827943055665.\n",
      "iter: 94, RMSE on training set: 0.06736429851899108.\n",
      "iter: 94, RMSE on test set: 1.6059883010340348.\n",
      "iter: 95, RMSE on training set: 0.06736433769994163.\n",
      "iter: 95, RMSE on test set: 1.605993680899236.\n",
      "iter: 96, RMSE on training set: 0.06736438056694319.\n",
      "iter: 96, RMSE on test set: 1.6059989386545015.\n",
      "iter: 97, RMSE on training set: 0.06736442653040003.\n",
      "iter: 97, RMSE on test set: 1.6060040787721943.\n",
      "iter: 98, RMSE on training set: 0.06736447507038511.\n",
      "iter: 98, RMSE on test set: 1.6060091054612629.\n",
      "iter: 99, RMSE on training set: 0.0673645257288291.\n",
      "iter: 99, RMSE on test set: 1.6060140226842368.\n",
      "iter: 100, RMSE on training set: 0.06736457810257555.\n",
      "iter: 100, RMSE on test set: 1.6060188341735013.\n",
      "iter: 101, RMSE on training set: 0.06736463183720352.\n",
      "iter: 101, RMSE on test set: 1.6060235434468553.\n",
      "iter: 102, RMSE on training set: 0.06736468662153886.\n",
      "iter: 102, RMSE on test set: 1.6060281538222507.\n",
      "iter: 103, RMSE on training set: 0.06736474218277007.\n",
      "iter: 103, RMSE on test set: 1.6060326684317585.\n",
      "iter: 104, RMSE on training set: 0.06736479828210919.\n",
      "iter: 104, RMSE on test set: 1.6060370902346979.\n",
      "iter: 105, RMSE on training set: 0.06736485471093157.\n",
      "iter: 105, RMSE on test set: 1.6060414220299666.\n",
      "iter: 106, RMSE on training set: 0.06736491128734666.\n",
      "iter: 106, RMSE on test set: 1.6060456664675795.\n",
      "iter: 107, RMSE on training set: 0.06736496785314797.\n",
      "iter: 107, RMSE on test set: 1.606049826059434.\n",
      "iter: 108, RMSE on training set: 0.06736502427110322.\n",
      "iter: 108, RMSE on test set: 1.606053903189343.\n",
      "iter: 109, RMSE on training set: 0.06736508042254588.\n",
      "iter: 109, RMSE on test set: 1.6060579001223525.\n",
      "iter: 110, RMSE on training set: 0.06736513620523443.\n",
      "iter: 110, RMSE on test set: 1.606061819013369.\n",
      "iter: 111, RMSE on training set: 0.06736519153145046.\n",
      "iter: 111, RMSE on test set: 1.6060656619151812.\n",
      "iter: 112, RMSE on training set: 0.0673652463263096.\n",
      "iter: 112, RMSE on test set: 1.6060694307858485.\n",
      "iter: 113, RMSE on training set: 0.06736530052626014.\n",
      "iter: 113, RMSE on test set: 1.6060731274955156.\n",
      "iter: 114, RMSE on training set: 0.06736535407775014.\n",
      "iter: 114, RMSE on test set: 1.6060767538327294.\n",
      "iter: 115, RMSE on training set: 0.06736540693604465.\n",
      "iter: 115, RMSE on test set: 1.606080311510215.\n",
      "iter: 116, RMSE on training set: 0.06736545906417364.\n",
      "iter: 116, RMSE on test set: 1.6060838021702248.\n",
      "iter: 117, RMSE on training set: 0.06736551043199986.\n",
      "iter: 117, RMSE on test set: 1.6060872273894358.\n",
      "iter: 118, RMSE on training set: 0.06736556101539173.\n",
      "iter: 118, RMSE on test set: 1.6060905886834482.\n",
      "iter: 119, RMSE on training set: 0.06736561079548752.\n",
      "iter: 119, RMSE on test set: 1.6060938875109234.\n",
      "iter: 120, RMSE on training set: 0.06736565975804606.\n",
      "iter: 120, RMSE on test set: 1.606097125277379.\n",
      "iter: 121, RMSE on training set: 0.0673657078928675.\n",
      "iter: 121, RMSE on test set: 1.6061003033386556.\n",
      "iter: 122, RMSE on training set: 0.06736575519328242.\n",
      "iter: 122, RMSE on test set: 1.6061034230041111.\n",
      "iter: 123, RMSE on training set: 0.067365801655698.\n",
      "iter: 123, RMSE on test set: 1.6061064855395388.\n",
      "iter: 124, RMSE on training set: 0.06736584727919731.\n",
      "iter: 124, RMSE on test set: 1.6061094921698407.\n",
      "iter: 125, RMSE on training set: 0.06736589206518179.\n",
      "iter: 125, RMSE on test set: 1.6061124440814796.\n",
      "iter: 126, RMSE on training set: 0.06736593601705919.\n",
      "iter: 126, RMSE on test set: 1.6061153424247208.\n",
      "iter: 127, RMSE on training set: 0.06736597913996306.\n",
      "iter: 127, RMSE on test set: 1.6061181883156979.\n",
      "iter: 128, RMSE on training set: 0.06736602144050885.\n",
      "iter: 128, RMSE on test set: 1.6061209828382839.\n",
      "iter: 129, RMSE on training set: 0.06736606292657506.\n",
      "iter: 129, RMSE on test set: 1.6061237270458362.\n",
      "iter: 130, RMSE on training set: 0.06736610360711284.\n",
      "iter: 130, RMSE on test set: 1.6061264219627684.\n",
      "iter: 131, RMSE on training set: 0.0673661434919767.\n",
      "iter: 131, RMSE on test set: 1.6061290685860103.\n",
      "iter: 132, RMSE on training set: 0.067366182591775.\n",
      "iter: 132, RMSE on test set: 1.6061316678863484.\n",
      "iter: 133, RMSE on training set: 0.06736622091773971.\n",
      "iter: 133, RMSE on test set: 1.606134220809648.\n",
      "iter: 134, RMSE on training set: 0.06736625848161056.\n",
      "iter: 134, RMSE on test set: 1.6061367282779793.\n",
      "iter: 135, RMSE on training set: 0.06736629529553433.\n",
      "iter: 135, RMSE on test set: 1.6061391911906797.\n",
      "iter: 136, RMSE on training set: 0.06736633137197584.\n",
      "iter: 136, RMSE on test set: 1.6061416104252884.\n",
      "iter: 137, RMSE on training set: 0.0673663667236398.\n",
      "iter: 137, RMSE on test set: 1.6061439868384424.\n",
      "iter: 138, RMSE on training set: 0.0673664013634045.\n",
      "iter: 138, RMSE on test set: 1.6061463212667044.\n",
      "iter: 139, RMSE on training set: 0.06736643530426026.\n",
      "iter: 139, RMSE on test set: 1.606148614527312.\n",
      "iter: 140, RMSE on training set: 0.06736646855926029.\n",
      "iter: 140, RMSE on test set: 1.606150867418886.\n",
      "iter: 141, RMSE on training set: 0.06736650114147308.\n",
      "iter: 141, RMSE on test set: 1.6061530807220885.\n",
      "iter: 142, RMSE on training set: 0.06736653306394697.\n",
      "iter: 142, RMSE on test set: 1.606155255200219.\n",
      "iter: 143, RMSE on training set: 0.06736656433967371.\n",
      "iter: 143, RMSE on test set: 1.6061573915998049.\n",
      "iter: 144, RMSE on training set: 0.06736659498156211.\n",
      "iter: 144, RMSE on test set: 1.6061594906511152.\n",
      "iter: 145, RMSE on training set: 0.06736662500241074.\n",
      "iter: 145, RMSE on test set: 1.6061615530686684.\n",
      "iter: 146, RMSE on training set: 0.06736665441489015.\n",
      "iter: 146, RMSE on test set: 1.6061635795516942.\n",
      "iter: 147, RMSE on training set: 0.06736668323152273.\n",
      "iter: 147, RMSE on test set: 1.6061655707845799.\n",
      "iter: 148, RMSE on training set: 0.06736671146466831.\n",
      "iter: 148, RMSE on test set: 1.606167527437283.\n",
      "iter: 149, RMSE on training set: 0.06736673912651298.\n",
      "iter: 149, RMSE on test set: 1.6061694501657262.\n",
      "iter: 150, RMSE on training set: 0.06736676622905755.\n",
      "iter: 150, RMSE on test set: 1.6061713396121642.\n",
      "iter: 151, RMSE on training set: 0.06736679278410962.\n",
      "iter: 151, RMSE on test set: 1.6061731964055395.\n",
      "iter: 152, RMSE on training set: 0.06736681880327768.\n",
      "iter: 152, RMSE on test set: 1.6061750211618253.\n",
      "iter: 153, RMSE on training set: 0.06736684429796516.\n",
      "iter: 153, RMSE on test set: 1.6061768144843325.\n",
      "iter: 154, RMSE on training set: 0.06736686927936755.\n",
      "iter: 154, RMSE on test set: 1.6061785769640315.\n",
      "iter: 155, RMSE on training set: 0.06736689375846848.\n",
      "iter: 155, RMSE on test set: 1.6061803091798328.\n",
      "iter: 156, RMSE on training set: 0.06736691774604006.\n",
      "iter: 156, RMSE on test set: 1.606182011698879.\n",
      "iter: 157, RMSE on training set: 0.0673669412526397.\n",
      "iter: 157, RMSE on test set: 1.6061836850768076.\n",
      "iter: 158, RMSE on training set: 0.06736696428861216.\n",
      "iter: 158, RMSE on test set: 1.606185329858014.\n",
      "iter: 159, RMSE on training set: 0.06736698686408842.\n",
      "iter: 159, RMSE on test set: 1.6061869465759093.\n",
      "iter: 160, RMSE on training set: 0.06736700898898818.\n",
      "iter: 160, RMSE on test set: 1.6061885357531578.\n",
      "iter: 161, RMSE on training set: 0.06736703067302065.\n",
      "iter: 161, RMSE on test set: 1.6061900979019055.\n",
      "iter: 162, RMSE on training set: 0.0673670519256873.\n",
      "iter: 162, RMSE on test set: 1.6061916335240243.\n",
      "iter: 163, RMSE on training set: 0.06736707275628297.\n",
      "iter: 163, RMSE on test set: 1.6061931431113174.\n",
      "iter: 164, RMSE on training set: 0.06736709317389958.\n",
      "iter: 164, RMSE on test set: 1.6061946271457512.\n",
      "iter: 165, RMSE on training set: 0.0673671131874287.\n",
      "iter: 165, RMSE on test set: 1.606196086099641.\n",
      "iter: 166, RMSE on training set: 0.06736713280556474.\n",
      "iter: 166, RMSE on test set: 1.6061975204358792.\n",
      "iter: 167, RMSE on training set: 0.06736715203680751.\n",
      "iter: 167, RMSE on test set: 1.6061989306081128.\n",
      "iter: 168, RMSE on training set: 0.06736717088946662.\n",
      "iter: 168, RMSE on test set: 1.6062003170609522.\n",
      "iter: 169, RMSE on training set: 0.06736718937166428.\n",
      "iter: 169, RMSE on test set: 1.606201680230148.\n",
      "iter: 170, RMSE on training set: 0.06736720749133875.\n",
      "iter: 170, RMSE on test set: 1.6062030205427833.\n",
      "iter: 171, RMSE on training set: 0.06736722525624832.\n",
      "iter: 171, RMSE on test set: 1.6062043384174487.\n",
      "iter: 172, RMSE on training set: 0.0673672426739749.\n",
      "iter: 172, RMSE on test set: 1.6062056342644209.\n",
      "iter: 173, RMSE on training set: 0.0673672597519271.\n",
      "iter: 173, RMSE on test set: 1.6062069084858317.\n",
      "iter: 174, RMSE on training set: 0.06736727649734488.\n",
      "iter: 174, RMSE on test set: 1.6062081614758412.\n",
      "iter: 175, RMSE on training set: 0.06736729291730187.\n",
      "iter: 175, RMSE on test set: 1.606209393620792.\n",
      "iter: 176, RMSE on training set: 0.0673673090187108.\n",
      "iter: 176, RMSE on test set: 1.606210605299382.\n",
      "iter: 177, RMSE on training set: 0.067367324808325.\n",
      "iter: 177, RMSE on test set: 1.6062117968828162.\n",
      "iter: 178, RMSE on training set: 0.0673673402927434.\n",
      "iter: 178, RMSE on test set: 1.6062129687349629.\n",
      "iter: 179, RMSE on training set: 0.06736735547841367.\n",
      "iter: 179, RMSE on test set: 1.6062141212125014.\n",
      "iter: 180, RMSE on training set: 0.06736737037163586.\n",
      "iter: 180, RMSE on test set: 1.6062152546650719.\n",
      "iter: 181, RMSE on training set: 0.06736738497856527.\n",
      "iter: 181, RMSE on test set: 1.606216369435427.\n",
      "iter: 182, RMSE on training set: 0.06736739930521668.\n",
      "iter: 182, RMSE on test set: 1.6062174658595645.\n",
      "iter: 183, RMSE on training set: 0.06736741335746671.\n",
      "iter: 183, RMSE on test set: 1.6062185442668715.\n",
      "iter: 184, RMSE on training set: 0.06736742714105765.\n",
      "iter: 184, RMSE on test set: 1.6062196049802597.\n",
      "iter: 185, RMSE on training set: 0.06736744066160126.\n",
      "iter: 185, RMSE on test set: 1.6062206483163037.\n",
      "iter: 186, RMSE on training set: 0.06736745392458.\n",
      "iter: 186, RMSE on test set: 1.6062216745853621.\n",
      "iter: 187, RMSE on training set: 0.06736746693535252.\n",
      "iter: 187, RMSE on test set: 1.6062226840917164.\n",
      "iter: 188, RMSE on training set: 0.0673674796991547.\n",
      "iter: 188, RMSE on test set: 1.606223677133694.\n",
      "iter: 189, RMSE on training set: 0.06736749222110378.\n",
      "iter: 189, RMSE on test set: 1.6062246540037868.\n",
      "iter: 190, RMSE on training set: 0.06736750450620053.\n",
      "iter: 190, RMSE on test set: 1.6062256149887788.\n",
      "iter: 191, RMSE on training set: 0.06736751655933232.\n",
      "iter: 191, RMSE on test set: 1.6062265603698602.\n",
      "iter: 192, RMSE on training set: 0.0673675283852763.\n",
      "iter: 192, RMSE on test set: 1.606227490422747.\n",
      "iter: 193, RMSE on training set: 0.06736753998870137.\n",
      "iter: 193, RMSE on test set: 1.60622840541779.\n",
      "iter: 194, RMSE on training set: 0.06736755137417103.\n",
      "iter: 194, RMSE on test set: 1.6062293056200945.\n",
      "iter: 195, RMSE on training set: 0.06736756254614652.\n",
      "iter: 195, RMSE on test set: 1.6062301912896197.\n",
      "iter: 196, RMSE on training set: 0.06736757350898825.\n",
      "iter: 196, RMSE on test set: 1.6062310626812932.\n",
      "iter: 197, RMSE on training set: 0.067367584266959.\n",
      "iter: 197, RMSE on test set: 1.6062319200451074.\n",
      "iter: 198, RMSE on training set: 0.06736759482422654.\n",
      "iter: 198, RMSE on test set: 1.6062327636262348.\n",
      "iter: 199, RMSE on training set: 0.06736760518486472.\n",
      "iter: 199, RMSE on test set: 1.6062335936651182.\n",
      "iter: 200, RMSE on training set: 0.06736761535285728.\n",
      "iter: 200, RMSE on test set: 1.6062344103975714.\n",
      "iter: 201, RMSE on training set: 0.06736762533209881.\n",
      "iter: 201, RMSE on test set: 1.606235214054878.\n",
      "iter: 202, RMSE on training set: 0.06736763512639758.\n",
      "iter: 202, RMSE on test set: 1.6062360048638842.\n",
      "iter: 203, RMSE on training set: 0.06736764473947753.\n",
      "iter: 203, RMSE on test set: 1.6062367830470898.\n",
      "iter: 204, RMSE on training set: 0.06736765417498009.\n",
      "iter: 204, RMSE on test set: 1.6062375488227414.\n",
      "iter: 205, RMSE on training set: 0.06736766343646647.\n",
      "iter: 205, RMSE on test set: 1.6062383024049198.\n",
      "iter: 206, RMSE on training set: 0.06736767252741906.\n",
      "iter: 206, RMSE on test set: 1.6062390440036276.\n",
      "iter: 207, RMSE on training set: 0.06736768145124436.\n",
      "iter: 207, RMSE on test set: 1.60623977382487.\n",
      "iter: 208, RMSE on training set: 0.06736769021127331.\n",
      "iter: 208, RMSE on test set: 1.6062404920707427.\n",
      "iter: 209, RMSE on training set: 0.06736769881076494.\n",
      "iter: 209, RMSE on test set: 1.606241198939512.\n",
      "iter: 210, RMSE on training set: 0.0673677072529062.\n",
      "iter: 210, RMSE on test set: 1.6062418946256964.\n",
      "iter: 211, RMSE on training set: 0.0673677155408152.\n",
      "iter: 211, RMSE on test set: 1.606242579320136.\n",
      "iter: 212, RMSE on training set: 0.0673677236775418.\n",
      "iter: 212, RMSE on test set: 1.6062432532100783.\n",
      "iter: 213, RMSE on training set: 0.06736773166607016.\n",
      "iter: 213, RMSE on test set: 1.6062439164792448.\n",
      "iter: 214, RMSE on training set: 0.06736773950931944.\n",
      "iter: 214, RMSE on test set: 1.60624456930792.\n",
      "iter: 215, RMSE on training set: 0.06736774721014571.\n",
      "iter: 215, RMSE on test set: 1.6062452118729915.\n",
      "iter: 216, RMSE on training set: 0.06736775477134384.\n",
      "iter: 216, RMSE on test set: 1.6062458443480563.\n",
      "iter: 217, RMSE on training set: 0.06736776219564841.\n",
      "iter: 217, RMSE on test set: 1.6062464669034644.\n",
      "iter: 218, RMSE on training set: 0.06736776948573503.\n",
      "iter: 218, RMSE on test set: 1.6062470797063944.\n",
      "iter: 219, RMSE on training set: 0.06736777664422262.\n",
      "iter: 219, RMSE on test set: 1.606247682920917.\n",
      "iter: 220, RMSE on training set: 0.0673677836736734.\n",
      "iter: 220, RMSE on test set: 1.6062482767080601.\n",
      "iter: 221, RMSE on training set: 0.06736779057659545.\n",
      "iter: 221, RMSE on test set: 1.6062488612258714.\n",
      "iter: 222, RMSE on training set: 0.06736779735544321.\n",
      "iter: 222, RMSE on test set: 1.6062494366294788.\n",
      "iter: 223, RMSE on training set: 0.06736780401261942.\n",
      "iter: 223, RMSE on test set: 1.6062500030711473.\n",
      "iter: 224, RMSE on training set: 0.06736781055047544.\n",
      "iter: 224, RMSE on test set: 1.6062505607003437.\n",
      "iter: 225, RMSE on training set: 0.06736781697131325.\n",
      "iter: 225, RMSE on test set: 1.6062511096637937.\n",
      "iter: 226, RMSE on training set: 0.06736782327738608.\n",
      "iter: 226, RMSE on test set: 1.6062516501055266.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2644\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'ndim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-b148d6a5e97b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mratings_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-113-b148d6a5e97b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(ratings)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     test_avg_cost, train_avg_cost = cross_validation(\n\u001b[0;32m---> 29\u001b[0;31m         sratings, 10, num_features, lambdas, stop_criterion, error_list, rng)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mratings_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-6db8357ab582>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(ratings, n_of_splits, num_features, lambdas, stop_criterion, error_list, rng)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             rmse_tr, rmse_te = ALS (train , test, num_features,lambda_user, \n\u001b[0;32m---> 53\u001b[0;31m                                     lambda_item, stop_criterion,error_list,rng )\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mavg_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrmse_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-cf748e2cd253>\u001b[0m in \u001b[0;36mALS\u001b[0;34m(train, test, num_features, lambda_user, lambda_item, stop_criterion, error_list, rng)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AAAAA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         item_features_new = update_item_feature(\n\u001b[0;32m---> 37\u001b[0;31m             train, user_features_new, lambda_item, nz_row_colindices)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-8962580e269c>\u001b[0m in \u001b[0;36mupdate_item_feature\u001b[0;34m(train, user_features, lambda_item, nnz_users_per_item)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(\"elementebis raodenoba = \", value.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m#print(\"shape of new train data = \", X.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mZt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_index_for_memoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid use of shape parameter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36misshape\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(ratings):\n",
    "    \n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    num_features = 20   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.1\n",
    "    stop_criterion = 1e-4\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    rng = 250\n",
    "    \n",
    "    #initialization\n",
    "    sratings = sp.lil_matrix(ratings)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    lambdas_user = np.linspace( 0.1, 1, 5)\n",
    "    lambdas_item = np.linspace( 0.1, 1, 5)\n",
    "    lambdas = [(x, y) for x in lambdas_user for y in lambdas_item]\n",
    "   \n",
    "    print(\"number of different pairs of lambdas : \",len(lambdas))\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    \n",
    "    test_avg_cost, train_avg_cost = cross_validation(\n",
    "        sratings, 10, num_features, lambdas, stop_criterion, error_list, rng)\n",
    "    \n",
    "    ratings_full = np.dot(np.transpose(item_features),user_features)\n",
    "    \n",
    "    return ratings_full, train_errors, test_errors\n",
    "    \n",
    "\n",
    "\n",
    "ratings_full, train_errors, test_errors = main(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_errors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-7201c3397a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ALS-WR Learning Curve'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_errors' is not defined"
     ]
    }
   ],
   "source": [
    "# Check performance by plotting train and test errors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(rng), train_errors, marker='o', label='Training Data');\n",
    "plt.plot(range(rng), test_errors, marker='v', label='Test Data');\n",
    "plt.title('ALS-WR Learning Curve')\n",
    "plt.xlabel('Number of Epochs');\n",
    "plt.ylabel('RMSE');\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"../results/test_train_rmse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n",
      "(10000, 1000)\n",
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "print(ratings_full.shape)\n",
    "print(ratings.shape)\n",
    "path_dataset = \"../submission/sampleSubmission.csv\"\n",
    "ratings_nonzero  = load_data(path_dataset)\n",
    "(rows, cols, data) = sp.find(ratings_nonzero)\n",
    "assert 1176952 == len(rows), \"Not enough predictions!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from our_helpers import create_submission\n",
    "path_output = \"submission.csv\"\n",
    "\n",
    "create_submission(path_output, rows, cols, ratings_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test ALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_items_per_user shape:  (1000,)\n",
      "num_users_per_item:  (10000,)\n",
      "(9990,)\n",
      "(100, 101)\n",
      "Total number of nonzero elements in origial data:1226\n",
      "Total number of nonzero elements in valid data:1226\n",
      "Total number of nonzero elements in train data:1103\n",
      "Total number of nonzero elements in test data:123\n",
      "Percentage  train and test =  89.9673735725938 %,  10.0326264274062 %\n",
      "Original data shape:  (100, 101)\n",
      "iter: 0, RMSE on training set: 0.7913093892098664.\n",
      "iter: 0, RMSE on test set: 2.9781570716647927.\n",
      "iter: 1, RMSE on training set: 0.3277872515224942.\n",
      "iter: 1, RMSE on test set: 2.622742020295455.\n",
      "iter: 2, RMSE on training set: 0.2222857010770707.\n",
      "iter: 2, RMSE on test set: 2.5270773702013813.\n",
      "iter: 3, RMSE on training set: 0.17595125564000597.\n",
      "iter: 3, RMSE on test set: 2.460270317007652.\n",
      "iter: 4, RMSE on training set: 0.14978420283358082.\n",
      "iter: 4, RMSE on test set: 2.3863512376035283.\n",
      "iter: 5, RMSE on training set: 0.1327626986836276.\n",
      "iter: 5, RMSE on test set: 2.324323809192857.\n",
      "iter: 6, RMSE on training set: 0.12098752619203355.\n",
      "iter: 6, RMSE on test set: 2.27684614172026.\n",
      "iter: 7, RMSE on training set: 0.11246334510811155.\n",
      "iter: 7, RMSE on test set: 2.24171746290155.\n",
      "iter: 8, RMSE on training set: 0.106074896007242.\n",
      "iter: 8, RMSE on test set: 2.214858977932817.\n",
      "iter: 9, RMSE on training set: 0.10115714721360157.\n",
      "iter: 9, RMSE on test set: 2.1925112588301507.\n",
      "iter: 10, RMSE on training set: 0.0972867898771793.\n",
      "iter: 10, RMSE on test set: 2.1723765391761978.\n",
      "iter: 11, RMSE on training set: 0.09418282562556086.\n",
      "iter: 11, RMSE on test set: 2.1533277119019405.\n",
      "iter: 12, RMSE on training set: 0.09165039485078309.\n",
      "iter: 12, RMSE on test set: 2.1351005457166163.\n",
      "iter: 13, RMSE on training set: 0.08955524515869077.\n",
      "iter: 13, RMSE on test set: 2.117879951991678.\n",
      "iter: 14, RMSE on training set: 0.0878061663769817.\n",
      "iter: 14, RMSE on test set: 2.101835009849667.\n",
      "iter: 15, RMSE on training set: 0.08633762500534464.\n",
      "iter: 15, RMSE on test set: 2.0869352757460633.\n",
      "iter: 16, RMSE on training set: 0.08509800857600702.\n",
      "iter: 16, RMSE on test set: 2.0730324878357815.\n",
      "iter: 17, RMSE on training set: 0.08404493088279993.\n",
      "iter: 17, RMSE on test set: 2.0599894493544064.\n",
      "iter: 18, RMSE on training set: 0.08314408213764604.\n",
      "iter: 18, RMSE on test set: 2.0477357472140594.\n",
      "iter: 19, RMSE on training set: 0.08236855362103271.\n",
      "iter: 19, RMSE on test set: 2.0362593382832643.\n",
      "iter: 20, RMSE on training set: 0.08169767580451574.\n",
      "iter: 20, RMSE on test set: 2.0255779986145463.\n",
      "iter: 21, RMSE on training set: 0.08111552017151208.\n",
      "iter: 21, RMSE on test set: 2.0157155666491398.\n",
      "iter: 22, RMSE on training set: 0.08060945307173113.\n",
      "iter: 22, RMSE on test set: 2.006687429690337.\n",
      "iter: 23, RMSE on training set: 0.08016902407526119.\n",
      "iter: 23, RMSE on test set: 1.9984926724334855.\n",
      "iter: 24, RMSE on training set: 0.07978526616081665.\n",
      "iter: 24, RMSE on test set: 1.9911107592310935.\n",
      "iter: 25, RMSE on training set: 0.07945032628413308.\n",
      "iter: 25, RMSE on test set: 1.9845017586094498.\n",
      "iter: 26, RMSE on training set: 0.07915729405663538.\n",
      "iter: 26, RMSE on test set: 1.9786092121619472.\n",
      "iter: 27, RMSE on training set: 0.07890012086781518.\n",
      "iter: 27, RMSE on test set: 1.973364533553202.\n",
      "iter: 28, RMSE on training set: 0.07867356585466388.\n",
      "iter: 28, RMSE on test set: 1.968691874766454.\n",
      "iter: 29, RMSE on training set: 0.07847313946828469.\n",
      "iter: 29, RMSE on test set: 1.964512702439556.\n",
      "iter: 30, RMSE on training set: 0.07829503576347223.\n",
      "iter: 30, RMSE on test set: 1.960749678636285.\n",
      "iter: 31, RMSE on training set: 0.07813605504442145.\n",
      "iter: 31, RMSE on test set: 1.9573297029715373.\n",
      "iter: 32, RMSE on training set: 0.07799352291063902.\n",
      "iter: 32, RMSE on test set: 1.9541861231823532.\n",
      "iter: 33, RMSE on training set: 0.07786521226947897.\n",
      "iter: 33, RMSE on test set: 1.9512601915995482.\n",
      "iter: 34, RMSE on training set: 0.07774927306058835.\n",
      "iter: 34, RMSE on test set: 1.9485018724565848.\n",
      "iter: 35, RMSE on training set: 0.07764417168275124.\n",
      "iter: 35, RMSE on test set: 1.9458701105559773.\n",
      "iter: 36, RMSE on training set: 0.07754863963493885.\n",
      "iter: 36, RMSE on test set: 1.9433326638904913.\n",
      "iter: 37, RMSE on training set: 0.07746162944800569.\n",
      "iter: 37, RMSE on test set: 1.9408655872203493.\n",
      "iter: 38, RMSE on training set: 0.0773822757871148.\n",
      "iter: 38, RMSE on test set: 1.938452438485275.\n",
      "iter: 39, RMSE on training set: 0.07730986033517276.\n",
      "iter: 39, RMSE on test set: 1.936083273226388.\n",
      "iter: 40, RMSE on training set: 0.07724378012913673.\n",
      "iter: 40, RMSE on test set: 1.9337534964150698.\n",
      "iter: 41, RMSE on training set: 0.07718351983592862.\n",
      "iter: 41, RMSE on test set: 1.9314626511135786.\n",
      "iter: 42, RMSE on training set: 0.07712862870790359.\n",
      "iter: 42, RMSE on test set: 1.929213229617473.\n",
      "iter: 43, RMSE on training set: 0.07707870267503035.\n",
      "iter: 43, RMSE on test set: 1.9270095871798318.\n",
      "iter: 44, RMSE on training set: 0.07703337147159513.\n",
      "iter: 44, RMSE on test set: 1.9248570193701413.\n",
      "iter: 45, RMSE on training set: 0.07699229015903276.\n",
      "iter: 45, RMSE on test set: 1.9227610363503664.\n",
      "iter: 46, RMSE on training set: 0.07695513408371327.\n",
      "iter: 46, RMSE on test set: 1.9207268386971632.\n",
      "iter: 47, RMSE on training set: 0.07692159623944425.\n",
      "iter: 47, RMSE on test set: 1.9187589766090754.\n",
      "iter: 48, RMSE on training set: 0.07689138612508213.\n",
      "iter: 48, RMSE on test set: 1.916861160686468.\n",
      "iter: 49, RMSE on training set: 0.0768642294005454.\n",
      "iter: 49, RMSE on test set: 1.9150361877371431.\n",
      "Final RMSE on test data: 1.9150361877371431.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-55cad175f82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m valid_ratings, train, test = split_data(\n\u001b[1;32m     56\u001b[0m     ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mratings_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"num_items_per_user shape: \", num_items_per_user.shape)\n",
    "    print(\"num_users_per_item: \", num_users_per_item.shape)\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    #print(np.where(num_items_per_user >= min_num_ratings))\n",
    "    # select user and item based on the condition.\n",
    "    #in our case where will return tuple, because there is just condition in parameters\n",
    "    #so it will be a tuple consisting of an array, so we need to extract it first\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    #valid_ratings = ratings[valid_items, :][: , valid_users] \n",
    "    valid_ratings = ratings\n",
    "    print( valid_items.shape)\n",
    "    print(valid_ratings.shape)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data and return train and test data. TODO\n",
    "    # NOTE: we only consider users and movies that have more\n",
    "    # than 10 ratings\n",
    "    # ***************************************************\n",
    "    \n",
    "    n = valid_ratings.shape[0]\n",
    "    m = valid_ratings.shape[1] \n",
    "    \n",
    "    train = sp.lil_matrix((n,m )) \n",
    "    test  = sp.lil_matrix((n,m)) \n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            \n",
    "            if np.random.ranf() > 0.9 :\n",
    "                test[i,j] = valid_ratings[i,j]\n",
    "    \n",
    "            else:\n",
    "                train[i,j] = valid_ratings[i,j]\n",
    "                \n",
    "    print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in valid data:{v}\".format(v=valid_ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    print(\"Percentage  train and test = \", train.nnz *100 /valid_ratings.nnz , \"%, \",test.nnz *100 /valid_ratings.nnz, \"%\" )\n",
    "    return valid_ratings, train, test\n",
    "\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "ratings_full, train_errors, test_errors = ALS(train, test,10, 0.1,0.1,1e-4, [0, 0], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
