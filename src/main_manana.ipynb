{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as skm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data and plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "shape of dataset: (10000, 1000)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f85444d2390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "from plots import plot_raw_data\n",
    "\n",
    "\n",
    "path_dataset = \"../data/data_train.csv\"\n",
    "subset = [100, 101]\n",
    "#ratings = load_data(path_dataset, subset)\n",
    "ratings, data = load_data(path_dataset)\n",
    "print(\"shape of dataset:\",ratings.shape)\n",
    "#print(ratings)\n",
    "#print( ratings[0,9])\n",
    "#print( ratings[0:5,0:5])\n",
    "#ratings\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run to estimate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of plots failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/manana/Machine_Learning_Recommender_Systems/src/plots.py\", line 75\n",
      "    \n",
      "    ^\n",
      "SyntaxError: unexpected EOF while parsing\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting data\n",
      "Percentage of nz train data:  0.8998, percentage of nz test data:  0.1002\n",
      "generating item and user feature matrices\n",
      "iter: 0, RMSE on training set: 1.9115616543069462.\n",
      "iter: 0, RMSE on test set: 2.091750565057215.\n",
      "iter: 1, RMSE on training set: 0.9075503445706536.\n",
      "iter: 1, RMSE on test set: 1.4378892121765305.\n",
      "iter: 2, RMSE on training set: 0.6066410844679037.\n",
      "iter: 2, RMSE on test set: 1.3535310884088354.\n",
      "iter: 3, RMSE on training set: 0.4903986605787287.\n",
      "iter: 3, RMSE on test set: 1.2691156778535482.\n",
      "iter: 4, RMSE on training set: 0.4339290256469746.\n",
      "iter: 4, RMSE on test set: 1.2078307861088209.\n",
      "iter: 5, RMSE on training set: 0.40181750410418815.\n",
      "iter: 5, RMSE on test set: 1.1656331086971545.\n",
      "iter: 6, RMSE on training set: 0.38144635295449.\n",
      "iter: 6, RMSE on test set: 1.135892015685141.\n",
      "iter: 7, RMSE on training set: 0.3675266266860978.\n",
      "iter: 7, RMSE on test set: 1.114255538077308.\n",
      "iter: 8, RMSE on training set: 0.35750877935731773.\n",
      "iter: 8, RMSE on test set: 1.0980688448344564.\n",
      "iter: 9, RMSE on training set: 0.35001963680572873.\n",
      "iter: 9, RMSE on test set: 1.0856695206324363.\n",
      "iter: 10, RMSE on training set: 0.3442543501644589.\n",
      "iter: 10, RMSE on test set: 1.0759777692104262.\n",
      "iter: 11, RMSE on training set: 0.3397098759933866.\n",
      "iter: 11, RMSE on test set: 1.0682691045695416.\n",
      "iter: 12, RMSE on training set: 0.33605627417691625.\n",
      "iter: 12, RMSE on test set: 1.0620437590335459.\n",
      "iter: 13, RMSE on training set: 0.3330691817742132.\n",
      "iter: 13, RMSE on test set: 1.0569486067281448.\n",
      "iter: 14, RMSE on training set: 0.3305916546229314.\n",
      "iter: 14, RMSE on test set: 1.0527287955745228.\n",
      "iter: 15, RMSE on training set: 0.3285112569511074.\n",
      "iter: 15, RMSE on test set: 1.0491968366875861.\n",
      "iter: 16, RMSE on training set: 0.32674568992279923.\n",
      "iter: 16, RMSE on test set: 1.046212355890074.\n",
      "iter: 17, RMSE on training set: 0.3252335004070733.\n",
      "iter: 17, RMSE on test set: 1.0436685761957065.\n",
      "iter: 18, RMSE on training set: 0.32392794105338246.\n",
      "iter: 18, RMSE on test set: 1.041483141570778.\n",
      "iter: 19, RMSE on training set: 0.3227928393683417.\n",
      "iter: 19, RMSE on test set: 1.0395917746393613.\n",
      "iter: 20, RMSE on training set: 0.3217997717027626.\n",
      "iter: 20, RMSE on test set: 1.037943802560151.\n",
      "iter: 21, RMSE on training set: 0.32092609697264085.\n",
      "iter: 21, RMSE on test set: 1.0364989313354869.\n",
      "iter: 22, RMSE on training set: 0.32015356404067496.\n",
      "iter: 22, RMSE on test set: 1.0352248711714627.\n",
      "iter: 23, RMSE on training set: 0.3194673069598959.\n",
      "iter: 23, RMSE on test set: 1.0340955566545105.\n",
      "iter: 24, RMSE on training set: 0.31885510646260296.\n",
      "iter: 24, RMSE on test set: 1.0330897939128176.\n",
      "iter: 25, RMSE on training set: 0.3183068374654932.\n",
      "iter: 25, RMSE on test set: 1.032190222038804.\n",
      "iter: 26, RMSE on training set: 0.3178140491068987.\n",
      "iter: 26, RMSE on test set: 1.0313825106570578.\n",
      "iter: 27, RMSE on training set: 0.3173696411147493.\n",
      "iter: 27, RMSE on test set: 1.0306547376801838.\n",
      "iter: 28, RMSE on training set: 0.3169676115097849.\n",
      "iter: 28, RMSE on test set: 1.029996905891175.\n",
      "iter: 29, RMSE on training set: 0.3166028579915126.\n",
      "iter: 29, RMSE on test set: 1.029400566978441.\n",
      "iter: 30, RMSE on training set: 0.31627102025734544.\n",
      "iter: 30, RMSE on test set: 1.028858528774013.\n",
      "iter: 31, RMSE on training set: 0.3159683538652638.\n",
      "iter: 31, RMSE on test set: 1.0283646267607403.\n",
      "iter: 32, RMSE on training set: 0.31569162861243544.\n",
      "iter: 32, RMSE on test set: 1.027913545021231.\n",
      "iter: 33, RMSE on training set: 0.31543804610869297.\n",
      "iter: 33, RMSE on test set: 1.0275006750618363.\n",
      "iter: 34, RMSE on training set: 0.31520517247848234.\n",
      "iter: 34, RMSE on test set: 1.0271220035551782.\n",
      "iter: 35, RMSE on training set: 0.314990883064307.\n",
      "iter: 35, RMSE on test set: 1.0267740221395307.\n",
      "iter: 36, RMSE on training set: 0.31479331671219374.\n",
      "iter: 36, RMSE on test set: 1.0264536540746334.\n",
      "iter: 37, RMSE on training set: 0.3146108377591262.\n",
      "iter: 37, RMSE on test set: 1.0261581938525108.\n",
      "iter: 38, RMSE on training set: 0.31444200425315993.\n",
      "iter: 38, RMSE on test set: 1.0258852568611876.\n",
      "iter: 39, RMSE on training set: 0.31428554125115044.\n",
      "iter: 39, RMSE on test set: 1.0256327369514027.\n",
      "iter: 40, RMSE on training set: 0.3141403182806911.\n",
      "iter: 40, RMSE on test set: 1.0253987703145147.\n",
      "iter: 41, RMSE on training set: 0.3140053302379577.\n",
      "iter: 41, RMSE on test set: 1.0251817044876386.\n",
      "iter: 42, RMSE on training set: 0.31387968113779935.\n",
      "iter: 42, RMSE on test set: 1.0249800715940192.\n",
      "iter: 43, RMSE on training set: 0.3137625702426598.\n",
      "iter: 43, RMSE on test set: 1.0247925651374208.\n",
      "iter: 44, RMSE on training set: 0.3136532801857522.\n",
      "iter: 44, RMSE on test set: 1.0246180198185812.\n",
      "iter: 45, RMSE on training set: 0.31355116677232914.\n",
      "iter: 45, RMSE on test set: 1.0244553939479306.\n",
      "iter: 46, RMSE on training set: 0.31345565019770305.\n",
      "iter: 46, RMSE on test set: 1.0243037541082396.\n",
      "iter: 47, RMSE on training set: 0.31336620746603533.\n",
      "iter: 47, RMSE on test set: 1.0241622617756871.\n",
      "iter: 48, RMSE on training set: 0.31328236582856683.\n",
      "iter: 48, RMSE on test set: 1.0240301616535759.\n",
      "iter: 49, RMSE on training set: 0.3132036970898552.\n",
      "iter: 49, RMSE on test set: 1.023906771504436.\n",
      "iter: 50, RMSE on training set: 0.3131298126552212.\n",
      "iter: 50, RMSE on test set: 1.0237914732959226.\n",
      "iter: 51, RMSE on training set: 0.3130603592113181.\n",
      "iter: 51, RMSE on test set: 1.023683705496081.\n",
      "iter: 52, RMSE on training set: 0.3129950149501232.\n",
      "iter: 52, RMSE on test set: 1.023582956374785.\n",
      "iter: 53, RMSE on training set: 0.31293348625891393.\n",
      "iter: 53, RMSE on test set: 1.0234887581839283.\n",
      "Final RMSE on test data: 1.023582956374785.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAGHCAYAAAD/QltcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fadfc024eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01567306874\n"
     ]
    }
   ],
   "source": [
    "import our_helpers as ohe\n",
    "import plots as pl\n",
    "\n",
    "#defining parameters\n",
    "\n",
    "lambda_ = 0.05\n",
    "K = 150\n",
    "stop_criterion = 1e-4\n",
    "\n",
    "\n",
    "print(\"splitting data\")\n",
    "valid_ratings, train, test = ohe.split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.1)\n",
    "np.random.seed(600)\n",
    "\n",
    "print(\"generating item and user feature matrices\")\n",
    "item_features, user_features, train_errors, test_errors = ohe.ALS(\n",
    "    train, test,K, lambda_, stop_criterion,  rng)\n",
    "\n",
    "#creating plot\n",
    "lambda_str = ('%f' % lambda_).rstrip('0')\n",
    "path = \"%s_%d_%d.jpg\"%(lambda_str, K, len(train_errors))\n",
    "pl.plot_train_test_errors(train_errors, test_errors, lambda_str , K , path, len(train_errors))\n",
    "\n",
    "#\n",
    "full_ratings = np.dot( item_features.transpose(), user_features)\n",
    "nz_rows, nz_cols = test.nonzero()\n",
    "nz_test = list( zip(nz_rows, nz_cols))\n",
    "\n",
    "full_ratings_b = ohe.bias_correction(full_ratings, ratings)\n",
    "print(compute_error2(test, full_ratings_b, nz_test) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "8\n",
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAGHCAYAAAAdnkAlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f72a1c762b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(errors))\n",
    "print(len(errors[0]))\n",
    "print(len(errors[1]))\n",
    "print(len(max(errors, key=len)))\n",
    "path = \"K%d/test_cv_rmse_l%d_nsp%d.jpg\"%(num_features, len(lambdas),n_splits )\n",
    "pl.plot_cv_errors(errors, lambdas, num_features, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different lambdas :  3\n",
      "1 / 3\n",
      "lambda  =  0.0001\n",
      "iter: 0, RMSE on training set: 0.01982251889665203.\n",
      "iter: 0, RMSE on test set: 4.221906828540927.\n",
      "iter: 1, RMSE on training set: 0.010248690953585327.\n",
      "iter: 1, RMSE on test set: 3.3032279071385573.\n",
      "iter: 2, RMSE on training set: 0.006606356532539318.\n",
      "iter: 2, RMSE on test set: 3.028867542155976.\n",
      "iter: 3, RMSE on training set: 0.004719207527488262.\n",
      "iter: 3, RMSE on test set: 2.8708033813008065.\n",
      "iter: 4, RMSE on training set: 0.0035893592534896415.\n",
      "iter: 4, RMSE on test set: 2.7660396710088584.\n",
      "iter: 5, RMSE on training set: 0.002847921553133279.\n",
      "iter: 5, RMSE on test set: 2.685478442221744.\n",
      "iter: 6, RMSE on training set: 0.002330004257972209.\n",
      "iter: 6, RMSE on test set: 2.6239282570962854.\n",
      "iter: 7, RMSE on training set: 0.0019512148023422772.\n",
      "iter: 7, RMSE on test set: 2.574020975268038.\n",
      "iter: 8, RMSE on training set: 0.0016642051098350577.\n",
      "iter: 8, RMSE on test set: 2.5328998688354543.\n",
      "iter: 9, RMSE on training set: 0.0014405674856189004.\n",
      "iter: 9, RMSE on test set: 2.49866431096991.\n",
      "iter: 10, RMSE on training set: 0.0012622967219467192.\n",
      "iter: 10, RMSE on test set: 2.469711000389975.\n",
      "iter: 11, RMSE on training set: 0.001117492423154828.\n",
      "iter: 11, RMSE on test set: 2.44506824699018.\n",
      "iter: 12, RMSE on training set: 0.0009979889998740408.\n",
      "iter: 12, RMSE on test set: 2.423945089389006.\n",
      "iter: 13, RMSE on training set: 0.000898012417634771.\n",
      "iter: 13, RMSE on test set: 2.405662955938669.\n",
      "iter: 14, RMSE on training set: 0.0008133816829683328.\n",
      "iter: 14, RMSE on test set: 2.389760022953725.\n",
      "iter: 15, RMSE on training set: 0.0007410013815293471.\n",
      "iter: 15, RMSE on test set: 2.375845975417103.\n",
      "iter: 16, RMSE on training set: 0.0006785354697314359.\n",
      "iter: 16, RMSE on test set: 2.363614261656278.\n",
      "iter: 17, RMSE on training set: 0.0006241909807079139.\n",
      "iter: 17, RMSE on test set: 2.352810379356603.\n",
      "iter: 18, RMSE on training set: 0.0005765713733456893.\n",
      "iter: 18, RMSE on test set: 2.343229978706546.\n",
      "iter: 19, RMSE on training set: 0.0005345740749794649.\n",
      "iter: 19, RMSE on test set: 2.334705392063008.\n",
      "iter: 20, RMSE on training set: 0.0004973182300111348.\n",
      "iter: 20, RMSE on test set: 2.327097611265689.\n",
      "iter: 21, RMSE on training set: 0.0004640926238326846.\n",
      "iter: 21, RMSE on test set: 2.320292027572966.\n",
      "iter: 22, RMSE on training set: 0.0004343173300598042.\n",
      "iter: 22, RMSE on test set: 2.3141936379310564.\n",
      "iter: 23, RMSE on training set: 0.0004075151652573778.\n",
      "iter: 23, RMSE on test set: 2.308722591160186.\n",
      "iter: 24, RMSE on training set: 0.00038329025675050605.\n",
      "iter: 24, RMSE on test set: 2.303810719408803.\n",
      "iter: 25, RMSE on training set: 0.0003613117456197519.\n",
      "iter: 25, RMSE on test set: 2.299399089136906.\n",
      "iter: 26, RMSE on training set: 0.0003413012329017799.\n",
      "iter: 26, RMSE on test set: 2.295436337956677.\n",
      "iter: 27, RMSE on training set: 0.0003230229926691171.\n",
      "iter: 27, RMSE on test set: 2.291877542526552.\n",
      "iter: 28, RMSE on training set: 0.0003062762616632115.\n",
      "iter: 28, RMSE on test set: 2.288683338567499.\n",
      "iter: 29, RMSE on training set: 0.00029088911283317657.\n",
      "iter: 29, RMSE on test set: 2.2858190729405283.\n",
      "iter: 30, RMSE on training set: 0.00027671355371804087.\n",
      "iter: 30, RMSE on test set: 2.2832539760663204.\n",
      "iter: 31, RMSE on training set: 0.0002636215815160216.\n",
      "iter: 31, RMSE on test set: 2.280960486696632.\n",
      "iter: 32, RMSE on training set: 0.000251501988208311.\n",
      "iter: 32, RMSE on test set: 2.278913796724767.\n",
      "iter: 33, RMSE on training set: 0.0002402577539591545.\n",
      "iter: 33, RMSE on test set: 2.2770915674487706.\n",
      "iter: 34, RMSE on training set: 0.00022980390447128088.\n",
      "iter: 34, RMSE on test set: 2.275473734139263.\n",
      "iter: 35, RMSE on training set: 0.00022006573844482878.\n",
      "iter: 35, RMSE on test set: 2.2740423342567064.\n",
      "iter: 36, RMSE on training set: 0.00021097735350362223.\n",
      "iter: 36, RMSE on test set: 2.272781322310801.\n",
      "iter: 37, RMSE on training set: 0.00020248041454799866.\n",
      "iter: 37, RMSE on test set: 2.2716763617294795.\n",
      "iter: 38, RMSE on training set: 0.00019452312000875014.\n",
      "iter: 38, RMSE on test set: 2.270714605587612.\n",
      "iter: 39, RMSE on training set: 0.00018705933044913038.\n",
      "iter: 39, RMSE on test set: 2.269884487069708.\n",
      "iter: 40, RMSE on training set: 0.00018004783101740058.\n",
      "iter: 40, RMSE on test set: 2.269175537193715.\n",
      "iter: 41, RMSE on training set: 0.00017345170475778623.\n",
      "iter: 41, RMSE on test set: 2.268578237656415.\n",
      "iter: 42, RMSE on training set: 0.0001672377980893395.\n",
      "iter: 42, RMSE on test set: 2.2680839070756567.\n",
      "iter: 43, RMSE on training set: 0.0001613762631831983.\n",
      "iter: 43, RMSE on test set: 2.267684613179912.\n",
      "iter: 44, RMSE on training set: 0.00015584016472951044.\n",
      "iter: 44, RMSE on test set: 2.2673731022067636.\n",
      "iter: 45, RMSE on training set: 0.00015060514082981032.\n",
      "iter: 45, RMSE on test set: 2.267142738423293.\n",
      "iter: 46, RMSE on training set: 0.00014564910957261623.\n",
      "iter: 46, RMSE on test set: 2.2669874493060944.\n",
      "iter: 47, RMSE on training set: 0.00014095201432337114.\n",
      "iter: 47, RMSE on test set: 2.2669016741794965.\n",
      "Final RMSE on test data: 2.2669874493060944.\n",
      "average train error =  1.45649109573e-05\n",
      "average test error =  0.226698744931\n",
      "2 / 3\n",
      "lambda  =  0.05005\n",
      "iter: 0, RMSE on training set: 3.7018382872781785.\n",
      "iter: 0, RMSE on test set: 4.251966045398792.\n",
      "iter: 1, RMSE on training set: 0.9797462046493781.\n",
      "iter: 1, RMSE on test set: 1.7219170258763052.\n",
      "iter: 2, RMSE on training set: 0.5621988301817268.\n",
      "iter: 2, RMSE on test set: 1.4459040295101464.\n",
      "iter: 3, RMSE on training set: 0.4327788272213451.\n",
      "iter: 3, RMSE on test set: 1.3061394331729008.\n",
      "iter: 4, RMSE on training set: 0.3779015862802422.\n",
      "iter: 4, RMSE on test set: 1.2208107574741933.\n",
      "iter: 5, RMSE on training set: 0.3487694426206575.\n",
      "iter: 5, RMSE on test set: 1.1656539219103002.\n",
      "iter: 6, RMSE on training set: 0.3309164930738995.\n",
      "iter: 6, RMSE on test set: 1.1280303929758055.\n",
      "iter: 7, RMSE on training set: 0.31896651874331433.\n",
      "iter: 7, RMSE on test set: 1.1012084514463307.\n",
      "iter: 8, RMSE on training set: 0.3104880973264255.\n",
      "iter: 8, RMSE on test set: 1.081394522997371.\n",
      "iter: 9, RMSE on training set: 0.30421706933536746.\n",
      "iter: 9, RMSE on test set: 1.0663300939009468.\n",
      "iter: 10, RMSE on training set: 0.2994299417610869.\n",
      "iter: 10, RMSE on test set: 1.0546036860729142.\n",
      "iter: 11, RMSE on training set: 0.29568329138955163.\n",
      "iter: 11, RMSE on test set: 1.045295539221023.\n",
      "iter: 12, RMSE on training set: 0.2926909114939452.\n",
      "iter: 12, RMSE on test set: 1.0377844381285832.\n",
      "iter: 13, RMSE on training set: 0.2902603059030666.\n",
      "iter: 13, RMSE on test set: 1.0316376922768902.\n",
      "iter: 14, RMSE on training set: 0.28825757320768103.\n",
      "iter: 14, RMSE on test set: 1.0265458524899644.\n",
      "iter: 15, RMSE on training set: 0.28658694828450815.\n",
      "iter: 15, RMSE on test set: 1.0222825325295313.\n",
      "iter: 16, RMSE on training set: 0.2851783382945707.\n",
      "iter: 16, RMSE on test set: 1.0186788498847084.\n",
      "iter: 17, RMSE on training set: 0.2839794203877589.\n",
      "iter: 17, RMSE on test set: 1.01560667358816.\n",
      "iter: 18, RMSE on training set: 0.28295045697978655.\n",
      "iter: 18, RMSE on test set: 1.0129673491711235.\n",
      "iter: 19, RMSE on training set: 0.28206079823626573.\n",
      "iter: 19, RMSE on test set: 1.0106839302710993.\n",
      "iter: 20, RMSE on training set: 0.2812864702648936.\n",
      "iter: 20, RMSE on test set: 1.0086957135405272.\n",
      "iter: 21, RMSE on training set: 0.2806084817062385.\n",
      "iter: 21, RMSE on test set: 1.006954320889757.\n",
      "iter: 22, RMSE on training set: 0.28001161555928933.\n",
      "iter: 22, RMSE on test set: 1.0054208426595022.\n",
      "iter: 23, RMSE on training set: 0.2794835539832589.\n",
      "iter: 23, RMSE on test set: 1.0040637222754658.\n",
      "iter: 24, RMSE on training set: 0.2790142347612665.\n",
      "iter: 24, RMSE on test set: 1.0028571685505603.\n",
      "iter: 25, RMSE on training set: 0.278595371126385.\n",
      "iter: 25, RMSE on test set: 1.0017799497677542.\n",
      "iter: 26, RMSE on training set: 0.2782200884029987.\n",
      "iter: 26, RMSE on test set: 1.0008144681523212.\n",
      "iter: 27, RMSE on training set: 0.27788264539576585.\n",
      "iter: 27, RMSE on test set: 0.9999460429881485.\n",
      "iter: 28, RMSE on training set: 0.27757821817065126.\n",
      "iter: 28, RMSE on test set: 0.9991623507877434.\n",
      "iter: 29, RMSE on training set: 0.277302730439621.\n",
      "iter: 29, RMSE on test set: 0.9984529848910632.\n",
      "iter: 30, RMSE on training set: 0.27705271924020813.\n",
      "iter: 30, RMSE on test set: 0.9978091067135344.\n",
      "iter: 31, RMSE on training set: 0.27682522768861284.\n",
      "iter: 31, RMSE on test set: 0.9972231679000233.\n",
      "iter: 32, RMSE on training set: 0.2766177187366795.\n",
      "iter: 32, RMSE on test set: 0.9966886877351405.\n",
      "iter: 33, RMSE on training set: 0.2764280053862374.\n",
      "iter: 33, RMSE on test set: 0.9962000738887383.\n",
      "iter: 34, RMSE on training set: 0.27625419390780537.\n",
      "iter: 34, RMSE on test set: 0.9957524773356108.\n",
      "iter: 35, RMSE on training set: 0.2760946374099314.\n",
      "iter: 35, RMSE on test set: 0.9953416743570389.\n",
      "iter: 36, RMSE on training set: 0.2759478976991281.\n",
      "iter: 36, RMSE on test set: 0.9949639700963339.\n",
      "iter: 37, RMSE on training set: 0.275812713816483.\n",
      "iter: 37, RMSE on test set: 0.9946161193386457.\n",
      "iter: 38, RMSE on training set: 0.27568797597798855.\n",
      "iter: 38, RMSE on test set: 0.9942952611088062.\n",
      "iter: 39, RMSE on training set: 0.2755727039064328.\n",
      "iter: 39, RMSE on test set: 0.9939988643966998.\n",
      "iter: 40, RMSE on training set: 0.27546602874623377.\n",
      "iter: 40, RMSE on test set: 0.9937246828800386.\n",
      "iter: 41, RMSE on training set: 0.27536717791071286.\n",
      "iter: 41, RMSE on test set: 0.9934707169523924.\n",
      "iter: 42, RMSE on training set: 0.27527546233625017.\n",
      "iter: 42, RMSE on test set: 0.9932351817078259.\n",
      "iter: 43, RMSE on training set: 0.27519026571760913.\n",
      "iter: 43, RMSE on test set: 0.9930164798065204.\n",
      "iter: 44, RMSE on training set: 0.2751110353772671.\n",
      "iter: 44, RMSE on test set: 0.9928131783586787.\n",
      "iter: 45, RMSE on training set: 0.27503727448707627.\n",
      "iter: 45, RMSE on test set: 0.9926239891347974.\n",
      "iter: 46, RMSE on training set: 0.2749685354114967.\n",
      "iter: 46, RMSE on test set: 0.9924477515442464.\n",
      "iter: 47, RMSE on training set: 0.2749044139849891.\n",
      "iter: 47, RMSE on test set: 0.9922834179306932.\n",
      "iter: 48, RMSE on training set: 0.2748445445704719.\n",
      "iter: 48, RMSE on test set: 0.9921300408185159.\n",
      "iter: 49, RMSE on training set: 0.2747885957728632.\n",
      "iter: 49, RMSE on test set: 0.9919867618095756.\n",
      "iter: 50, RMSE on training set: 0.2747362667067305.\n",
      "iter: 50, RMSE on test set: 0.9918528018859443.\n",
      "iter: 51, RMSE on training set: 0.2746872837329361.\n",
      "iter: 51, RMSE on test set: 0.9917274529138864.\n",
      "iter: 52, RMSE on training set: 0.2746413975959507.\n",
      "iter: 52, RMSE on test set: 0.9916100701810898.\n",
      "iter: 53, RMSE on training set: 0.2745983809045564.\n",
      "iter: 53, RMSE on test set: 0.9915000658249532.\n",
      "iter: 54, RMSE on training set: 0.2745580259084522.\n",
      "iter: 54, RMSE on test set: 0.991396903032361.\n",
      "iter: 55, RMSE on training set: 0.2745201425311747.\n",
      "iter: 55, RMSE on test set: 0.9913000909099503.\n",
      "Final RMSE on test data: 0.991396903032361.\n",
      "average train error =  0.0274558025908\n",
      "average test error =  0.0991396903032\n",
      "3 / 3\n",
      "lambda  =  0.1\n",
      "iter: 0, RMSE on training set: 8.460601066674686.\n",
      "iter: 0, RMSE on test set: 8.90146795067698.\n",
      "iter: 1, RMSE on training set: 1.262845255989458.\n",
      "iter: 1, RMSE on test set: 1.3881573510113838.\n",
      "iter: 2, RMSE on training set: 1.0429839248992332.\n",
      "iter: 2, RMSE on test set: 1.1850720770058227.\n",
      "iter: 3, RMSE on training set: 0.947708509190119.\n",
      "iter: 3, RMSE on test set: 1.1059483953265468.\n",
      "iter: 4, RMSE on training set: 0.8940849460438456.\n",
      "iter: 4, RMSE on test set: 1.0639081642557378.\n",
      "iter: 5, RMSE on training set: 0.8609217799701456.\n",
      "iter: 5, RMSE on test set: 1.0387908683292992.\n",
      "iter: 6, RMSE on training set: 0.8391573541782328.\n",
      "iter: 6, RMSE on test set: 1.0225792396253768.\n",
      "iter: 7, RMSE on training set: 0.8242420129494052.\n",
      "iter: 7, RMSE on test set: 1.0115079218290843.\n",
      "iter: 8, RMSE on training set: 0.8136657404293802.\n",
      "iter: 8, RMSE on test set: 1.003611515660273.\n",
      "iter: 9, RMSE on training set: 0.8059515321143339.\n",
      "iter: 9, RMSE on test set: 0.997782464816701.\n",
      "iter: 10, RMSE on training set: 0.8001877544530845.\n",
      "iter: 10, RMSE on test set: 0.9933581056744134.\n",
      "iter: 11, RMSE on training set: 0.7957904145790656.\n",
      "iter: 11, RMSE on test set: 0.9899221868020394.\n",
      "iter: 12, RMSE on training set: 0.7923737655942006.\n",
      "iter: 12, RMSE on test set: 0.9872024796346863.\n",
      "iter: 13, RMSE on training set: 0.7896762119554297.\n",
      "iter: 13, RMSE on test set: 0.9850147957742962.\n",
      "iter: 14, RMSE on training set: 0.7875161837705013.\n",
      "iter: 14, RMSE on test set: 0.9832308421797085.\n",
      "iter: 15, RMSE on training set: 0.7857649993503654.\n",
      "iter: 15, RMSE on test set: 0.9817589820241411.\n",
      "iter: 16, RMSE on training set: 0.7843297122458052.\n",
      "iter: 16, RMSE on test set: 0.9805323035486107.\n",
      "iter: 17, RMSE on training set: 0.7831420080235942.\n",
      "iter: 17, RMSE on test set: 0.9795009895897676.\n",
      "iter: 18, RMSE on training set: 0.7821508639312752.\n",
      "iter: 18, RMSE on test set: 0.9786273037653274.\n",
      "iter: 19, RMSE on training set: 0.7813176029535168.\n",
      "iter: 19, RMSE on test set: 0.9778822159048127.\n",
      "iter: 20, RMSE on training set: 0.7806125024291254.\n",
      "iter: 20, RMSE on test set: 0.9772430813889895.\n",
      "iter: 21, RMSE on training set: 0.7800124304517173.\n",
      "iter: 21, RMSE on test set: 0.9766920140136748.\n",
      "iter: 22, RMSE on training set: 0.779499173240841.\n",
      "iter: 22, RMSE on test set: 0.9762147249229043.\n",
      "iter: 23, RMSE on training set: 0.7790582343711047.\n",
      "iter: 23, RMSE on test set: 0.9757996807925903.\n",
      "iter: 24, RMSE on training set: 0.7786779610276767.\n",
      "iter: 24, RMSE on test set: 0.9754374845376533.\n",
      "iter: 25, RMSE on training set: 0.7783489001051992.\n",
      "iter: 25, RMSE on test set: 0.9751204136094984.\n",
      "iter: 26, RMSE on training set: 0.7780633180015829.\n",
      "iter: 26, RMSE on test set: 0.9748420715387953.\n",
      "iter: 27, RMSE on training set: 0.777814838460939.\n",
      "iter: 27, RMSE on test set: 0.9745971219540369.\n",
      "iter: 28, RMSE on training set: 0.7775981665591265.\n",
      "iter: 28, RMSE on test set: 0.9743810834136434.\n",
      "iter: 29, RMSE on training set: 0.7774088762459446.\n",
      "iter: 29, RMSE on test set: 0.9741901695936613.\n",
      "iter: 30, RMSE on training set: 0.7772432452746174.\n",
      "iter: 30, RMSE on test set: 0.9740211636635425.\n",
      "iter: 31, RMSE on training set: 0.7770981258111569.\n",
      "iter: 31, RMSE on test set: 0.9738713186868981.\n",
      "iter: 32, RMSE on training set: 0.7769708421569335.\n",
      "iter: 32, RMSE on test set: 0.9737382780183378.\n",
      "iter: 33, RMSE on training set: 0.7768591092612621.\n",
      "iter: 33, RMSE on test set: 0.973620011197025.\n",
      "iter: 34, RMSE on training set: 0.7767609673054607.\n",
      "iter: 34, RMSE on test set: 0.9735147619487708.\n",
      "iter: 35, RMSE on training set: 0.7766747288123084.\n",
      "iter: 35, RMSE on test set: 0.973421005722636.\n",
      "Final RMSE on test data: 0.9735147619487708.\n",
      "average train error =  0.0776760967305\n",
      "average test error =  0.0973514761949\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAGHCAYAAAAOSQDRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8qOhD5AjQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8517b5ba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smallest avg error:  0.0973514761949\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'stest' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c39d6c4cc84e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mvl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_per_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_users_per_item\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m item_features , user_features , rmse_tr, rmse_te = ALS (\n",
      "\u001b[0;32m/home/manana/Machine_Learning_Recommender_Systems/src/our_helpers.py\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(ratings, num_items_per_user, num_users_per_item, min_num_ratings, p_test, sparse)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mp_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mstest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# put the element with probability 0.9 in train set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'stest' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import our_helpers as ohe\n",
    "import plots as pl\n",
    "\n",
    "#ohe.main(ratings)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "# define parameters\n",
    "num_features = 300   # K in the lecture notes\n",
    "lambda_user = 0.3\n",
    "lambda_item = 0.3\n",
    "stop_criterion = 1e-4\n",
    "n_splits = 10\n",
    "\n",
    "    \n",
    "#initialization\n",
    "sratings = sp.lil_matrix(ratings)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "    \n",
    "lambdas = np.linspace( 0.0001, 0.1, 3)\n",
    "\n",
    "    \n",
    "print(\"number of different lambdas : \",len(lambdas))\n",
    "    \n",
    "# set seed\n",
    "np.random.seed(988)\n",
    "    \n",
    "    \n",
    "test_avg_cost, train_avg_cost , errors = ohe.cross_validation(\n",
    "sratings, n_splits, num_features, lambdas, stop_criterion)\n",
    "    \n",
    "#generating plot\n",
    "path = \"/K%d/l%d_nsp%d.jpg\"%(num_features, len(lambdas),n_splits )\n",
    "    \n",
    "pl.plot_cv_errors(errors, lambdas, num_features, path)\n",
    "    \n",
    "ind = np.argmin(test_avg_cost)\n",
    "print(\"smallest avg error: \",test_avg_cost[ind])\n",
    "    \n",
    "lambda_ = lambdas[ind]\n",
    "    \n",
    "    \n",
    "vl, train, test = ohe.split_data(ratings, num_items_per_user, num_users_per_item,0)\n",
    "    \n",
    "item_features , user_features , rmse_tr, rmse_te = ALS (\n",
    "train , test, num_features,lambda_[0], \n",
    "lambda_[1], stop_criterion,error_list, 250 )\n",
    "    \n",
    "ratings_full = np.dot(np.transpose(item_features),user_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sumbmissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "from our_helpers import create_submission\n",
    "path_output = \"submission.csv\"\n",
    "\n",
    "#create_submission(path_output, full_ratings_b)\n",
    "\n",
    "create_submission(path_output, X_filled_nnm_2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test ALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n",
      "1.03818504741\n"
     ]
    }
   ],
   "source": [
    "print(X_filled_nnm_2.shape)\n",
    "error = ohe.compute_error2(test, X_filled_nnm_2, nz_test)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n",
      "[SoftImpute] Max Singular Value of X_init = 141.828063\n",
      "[SoftImpute] Iter 1: observed MAE=0.064029 rank=999\n",
      "[SoftImpute] Iter 2: observed MAE=0.064057 rank=999\n",
      "[SoftImpute] Iter 3: observed MAE=0.064076 rank=998\n",
      "[SoftImpute] Iter 4: observed MAE=0.064087 rank=995\n",
      "[SoftImpute] Iter 5: observed MAE=0.064087 rank=991\n",
      "[SoftImpute] Iter 6: observed MAE=0.064082 rank=988\n",
      "[SoftImpute] Iter 7: observed MAE=0.064070 rank=984\n",
      "[SoftImpute] Iter 8: observed MAE=0.064049 rank=978\n",
      "[SoftImpute] Iter 9: observed MAE=0.064015 rank=970\n",
      "[SoftImpute] Iter 10: observed MAE=0.063974 rank=964\n",
      "[SoftImpute] Iter 11: observed MAE=0.063930 rank=957\n",
      "[SoftImpute] Iter 12: observed MAE=0.063880 rank=949\n",
      "[SoftImpute] Iter 13: observed MAE=0.063818 rank=939\n",
      "[SoftImpute] Iter 14: observed MAE=0.063751 rank=932\n",
      "[SoftImpute] Iter 15: observed MAE=0.063686 rank=924\n",
      "[SoftImpute] Iter 16: observed MAE=0.063619 rank=916\n",
      "[SoftImpute] Iter 17: observed MAE=0.063544 rank=906\n",
      "[SoftImpute] Iter 18: observed MAE=0.063469 rank=899\n",
      "[SoftImpute] Iter 19: observed MAE=0.063394 rank=891\n",
      "[SoftImpute] Iter 20: observed MAE=0.063319 rank=882\n",
      "[SoftImpute] Iter 21: observed MAE=0.063244 rank=874\n",
      "[SoftImpute] Iter 22: observed MAE=0.063171 rank=867\n",
      "[SoftImpute] Iter 23: observed MAE=0.063099 rank=859\n",
      "[SoftImpute] Iter 24: observed MAE=0.063029 rank=853\n",
      "[SoftImpute] Iter 25: observed MAE=0.062960 rank=844\n",
      "[SoftImpute] Stopped after iteration 25 for lambda=2.836561\n",
      "1.02343583384\n"
     ]
    }
   ],
   "source": [
    "#scored 1.05\n",
    "import fancyimpute as fi\n",
    "import our_helpers as ohe\n",
    "\n",
    "valid_ratings, train, test,stest = ohe.split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.1,sparse= False)\n",
    "\n",
    "print(data.shape)\n",
    "nz_rows, nz_cols = stest.nonzero()\n",
    "nz_test = list( zip(nz_rows, nz_cols))\n",
    "train[train == 0 ] = 'nan'\n",
    "X_filled_nnm_1 = fi.SoftImpute().complete(train)\n",
    "error = ohe.compute_error2(test, X_filled_nnm_1, nz_test)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4b8c51d37fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mour_helpers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mohe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_filled_nnm_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNuclearNormMinimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_error2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_filled_nnm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/fancyimpute-0.0.4-py3.5.egg/fancyimpute/solver.py\u001b[0m in \u001b[0;36mcomplete\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mcompleted\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mwithout\u001b[0m \u001b[0many\u001b[0m \u001b[0mNaNs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mimputations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_imputations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimputations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/fancyimpute-0.0.4-py3.5.egg/fancyimpute/solver.py\u001b[0m in \u001b[0;36mmultiple_imputations\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mGenerate\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0mimputations\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mincomplete\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_imputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_imputations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/fancyimpute-0.0.4-py3.5.egg/fancyimpute/solver.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mGenerate\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0mimputations\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mincomplete\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_imputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_imputations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/fancyimpute-0.0.4-py3.5.egg/fancyimpute/solver.py\u001b[0m in \u001b[0;36msingle_imputation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    252\u001b[0m                     type(X_filled)))\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mX_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_filled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/fancyimpute-0.0.4-py3.5.egg/fancyimpute/nuclear_norm_minimization.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, X, missing_mask)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# SCS solver is known to be faster but less exact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             solver=cvxpy.SCS if self.fast_but_approximate else None)\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/cvxpy-0.4.8-py3.5.egg/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/cvxpy-0.4.8-py3.5.egg/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, solver, ignore_dcp, warm_start, verbose, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m             results_dict = solver.solve(objective, constraints,\n\u001b[1;32m    331\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                                         kwargs)\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;31m# Presolve determined problem was unbounded or infeasible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/cvxpy-0.4.8-py3.5.egg/cvxpy/problems/solvers/scs_intf.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, objective, constraints, cached_data, warm_start, verbose, solver_opts)\u001b[0m\n\u001b[1;32m     97\u001b[0m         data = self.get_problem_data(objective,\n\u001b[1;32m     98\u001b[0m                                      \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                                      cached_data)\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Set the options to be VERBOSE plus any user-specific options.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0msolver_opts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"verbose\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/cvxpy-0.4.8-py3.5.egg/cvxpy/problems/solvers/solver.py\u001b[0m in \u001b[0;36mget_problem_data\u001b[0;34m(self, objective, constraints, cached_data)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0msym_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sym_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         matrix_data = self.get_matrix_data(objective, constraints,\n\u001b[0;32m--> 229\u001b[0;31m                                            cached_data)\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOFFSET\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/cvxpy-0.4.8-py3.5.egg/cvxpy/problems/solvers/solver.py\u001b[0m in \u001b[0;36mget_matrix_data\u001b[0;34m(self, objective, constraints, cached_data)\u001b[0m\n\u001b[1;32m    205\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_intf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                                                self.nonlin_constr())\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprob_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/cvxpy-0.4.8-py3.5.egg/cvxpy/problems/problem_data/matrix_data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sym_data, matrix_intf, vec_intf, solver, nonlin)\u001b[0m\n\u001b[1;32m     87\u001b[0m         self.eq_cache = self._init_matrix_cache(eq_constr,\n\u001b[1;32m     88\u001b[0m                                                 self.sym_data.x_length)\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lin_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Inequality constraints.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         self.ineq_cache = self._init_matrix_cache(ineq_constr,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/cvxpy-0.4.8-py3.5.egg/cvxpy/problems/problem_data/matrix_data.py\u001b[0m in \u001b[0;36m_lin_matrix\u001b[0;34m(self, mat_cache, caching)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mactive_constr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msym_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_offsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0mconstr_offsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             )\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m# Convert the constant offset to the correct data type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/CVXcanon-0.1.1-py3.5-linux-x86_64.egg/canonInterface.py\u001b[0m in \u001b[0;36mget_problem_matrix\u001b[0;34m(constrs, id_to_col, constr_offsets)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinOps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_lin_op_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mlin_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/CVXcanon-0.1.1-py3.5-linux-x86_64.egg/canonInterface.py\u001b[0m in \u001b[0;36mbuild_lin_op_tree\u001b[0;34m(root_linPy, tmp)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mlinC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dense_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scalar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mset_matrix_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinPy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot_linC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/CVXcanon-0.1.1-py3.5-linux-x86_64.egg/canonInterface.py\u001b[0m in \u001b[0;36mset_matrix_data\u001b[0;34m(linC, linPy)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mcoo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sparse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             linC.set_sparse_data(coo.data, coo.row.astype(float),\n\u001b[0;32m---> 98\u001b[0;31m                                  coo.col.astype(float), coo.shape[0], coo.shape[1])\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlinPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dense_const'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mlinC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dense_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/CVXcanon-0.1.1-py3.5-linux-x86_64.egg/CVXcanon.py\u001b[0m in \u001b[0;36mset_sparse_data\u001b[0;34m(self, data, row_idxs, col_idxs, rows, cols)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_sparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_CVXcanon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinOp_set_sparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CVXcanon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_LinOp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0m__del__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import fancyimpute as fi\n",
    "import our_helpers as ohe\n",
    "\n",
    "X_filled_nnm_2 = fi.NuclearNormMinimization().complete(train)\n",
    "error = ohe.compute_error2(test, X_filled_nnm, nz_test)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 2016-12-18 00:09:08 downhill.dataset:174 train: 1 of 1 mini-batches from (10000, 1000)\n",
      "I 2016-12-18 00:09:08 downhill.base:389 -- patience = 5\n",
      "I 2016-12-18 00:09:08 downhill.base:390 -- validate_every = 10\n",
      "I 2016-12-18 00:09:08 downhill.base:391 -- max_updates = None\n",
      "I 2016-12-18 00:09:08 downhill.base:392 -- min_improvement = 0.005\n",
      "I 2016-12-18 00:09:08 downhill.base:393 -- max_gradient_norm = 10\n",
      "I 2016-12-18 00:09:08 downhill.base:394 -- max_gradient_elem = 0\n",
      "I 2016-12-18 00:09:08 downhill.base:395 -- learning_rate = 0.001\n",
      "I 2016-12-18 00:09:08 downhill.base:396 -- momentum = 0\n",
      "I 2016-12-18 00:09:08 downhill.base:397 -- nesterov = False\n",
      "I 2016-12-18 00:09:08 downhill.adaptive:220 -- rms_halflife = 14\n",
      "I 2016-12-18 00:09:08 downhill.adaptive:221 -- rms_regularizer = 1e-08\n",
      "I 2016-12-18 00:09:08 downhill.base:118 compiling evaluation function\n",
      "I 2016-12-18 00:09:09 downhill.base:124 compiling Adam function\n",
      "I 2016-12-18 00:09:11 downhill.base:232 validation 0 loss=0.429184 error=0.314141 grad(U)=0.000014 grad(V)=0.000211 *\n",
      "I 2016-12-18 00:09:11 downhill.base:232 Adam 1 loss=0.429184 error=0.314141 grad(U)=0.000014 grad(V)=0.000211\n",
      "I 2016-12-18 00:09:11 downhill.base:232 Adam 2 loss=0.428353 error=0.313476 grad(U)=0.000014 grad(V)=0.000210\n",
      "I 2016-12-18 00:09:11 downhill.base:232 Adam 3 loss=0.427235 error=0.312581 grad(U)=0.000014 grad(V)=0.000209\n",
      "I 2016-12-18 00:09:11 downhill.base:232 Adam 4 loss=0.425930 error=0.311538 grad(U)=0.000014 grad(V)=0.000207\n",
      "I 2016-12-18 00:09:12 downhill.base:232 Adam 5 loss=0.424494 error=0.310391 grad(U)=0.000014 grad(V)=0.000206\n",
      "I 2016-12-18 00:09:12 downhill.base:232 Adam 6 loss=0.422965 error=0.309170 grad(U)=0.000014 grad(V)=0.000204\n",
      "I 2016-12-18 00:09:12 downhill.base:232 Adam 7 loss=0.421368 error=0.307895 grad(U)=0.000014 grad(V)=0.000202\n",
      "I 2016-12-18 00:09:12 downhill.base:232 Adam 8 loss=0.419722 error=0.306583 grad(U)=0.000013 grad(V)=0.000201\n",
      "I 2016-12-18 00:09:12 downhill.base:232 Adam 9 loss=0.418043 error=0.305244 grad(U)=0.000013 grad(V)=0.000199\n",
      "I 2016-12-18 00:09:13 downhill.base:232 Adam 10 loss=0.416342 error=0.303890 grad(U)=0.000013 grad(V)=0.000197\n",
      "I 2016-12-18 00:09:13 downhill.base:232 validation 1 loss=0.414628 error=0.302527 grad(U)=0.000013 grad(V)=0.000195 *\n",
      "I 2016-12-18 00:09:13 downhill.base:232 Adam 11 loss=0.414628 error=0.302527 grad(U)=0.000013 grad(V)=0.000195\n",
      "I 2016-12-18 00:09:13 downhill.base:232 Adam 12 loss=0.412910 error=0.301161 grad(U)=0.000013 grad(V)=0.000194\n",
      "I 2016-12-18 00:09:13 downhill.base:232 Adam 13 loss=0.411193 error=0.299797 grad(U)=0.000013 grad(V)=0.000192\n",
      "I 2016-12-18 00:09:14 downhill.base:232 Adam 14 loss=0.409482 error=0.298439 grad(U)=0.000013 grad(V)=0.000190\n",
      "I 2016-12-18 00:09:14 downhill.base:232 Adam 15 loss=0.407781 error=0.297089 grad(U)=0.000013 grad(V)=0.000188\n",
      "I 2016-12-18 00:09:14 downhill.base:232 Adam 16 loss=0.406093 error=0.295751 grad(U)=0.000012 grad(V)=0.000186\n",
      "I 2016-12-18 00:09:14 downhill.base:232 Adam 17 loss=0.404420 error=0.294427 grad(U)=0.000012 grad(V)=0.000185\n",
      "I 2016-12-18 00:09:14 downhill.base:232 Adam 18 loss=0.402764 error=0.293117 grad(U)=0.000012 grad(V)=0.000183\n",
      "I 2016-12-18 00:09:15 downhill.base:232 Adam 19 loss=0.401128 error=0.291823 grad(U)=0.000012 grad(V)=0.000181\n",
      "I 2016-12-18 00:09:15 downhill.base:232 Adam 20 loss=0.399512 error=0.290547 grad(U)=0.000012 grad(V)=0.000180\n",
      "I 2016-12-18 00:09:15 downhill.base:232 validation 2 loss=0.397917 error=0.289288 grad(U)=0.000012 grad(V)=0.000178 *\n",
      "I 2016-12-18 00:09:15 downhill.base:232 Adam 21 loss=0.397917 error=0.289288 grad(U)=0.000012 grad(V)=0.000178\n",
      "I 2016-12-18 00:09:15 downhill.base:232 Adam 22 loss=0.396343 error=0.288047 grad(U)=0.000012 grad(V)=0.000177\n",
      "I 2016-12-18 00:09:16 downhill.base:232 Adam 23 loss=0.394792 error=0.286824 grad(U)=0.000012 grad(V)=0.000175\n",
      "I 2016-12-18 00:09:16 downhill.base:232 Adam 24 loss=0.393263 error=0.285620 grad(U)=0.000012 grad(V)=0.000173\n",
      "I 2016-12-18 00:09:16 downhill.base:232 Adam 25 loss=0.391756 error=0.284435 grad(U)=0.000011 grad(V)=0.000172\n",
      "I 2016-12-18 00:09:16 downhill.base:232 Adam 26 loss=0.390272 error=0.283268 grad(U)=0.000011 grad(V)=0.000170\n",
      "I 2016-12-18 00:09:16 downhill.base:232 Adam 27 loss=0.388810 error=0.282120 grad(U)=0.000011 grad(V)=0.000169\n",
      "I 2016-12-18 00:09:17 downhill.base:232 Adam 28 loss=0.387370 error=0.280990 grad(U)=0.000011 grad(V)=0.000168\n",
      "I 2016-12-18 00:09:17 downhill.base:232 Adam 29 loss=0.385952 error=0.279878 grad(U)=0.000011 grad(V)=0.000166\n",
      "I 2016-12-18 00:09:17 downhill.base:232 Adam 30 loss=0.384556 error=0.278784 grad(U)=0.000011 grad(V)=0.000165\n",
      "I 2016-12-18 00:09:17 downhill.base:232 validation 3 loss=0.383182 error=0.277708 grad(U)=0.000011 grad(V)=0.000163 *\n",
      "I 2016-12-18 00:09:17 downhill.base:232 Adam 31 loss=0.383182 error=0.277708 grad(U)=0.000011 grad(V)=0.000163\n",
      "I 2016-12-18 00:09:18 downhill.base:232 Adam 32 loss=0.381828 error=0.276649 grad(U)=0.000011 grad(V)=0.000162\n",
      "I 2016-12-18 00:09:18 downhill.base:232 Adam 33 loss=0.380495 error=0.275607 grad(U)=0.000011 grad(V)=0.000161\n",
      "I 2016-12-18 00:09:18 downhill.base:232 Adam 34 loss=0.379183 error=0.274581 grad(U)=0.000011 grad(V)=0.000160\n",
      "I 2016-12-18 00:09:18 downhill.base:232 Adam 35 loss=0.377889 error=0.273572 grad(U)=0.000011 grad(V)=0.000158\n",
      "I 2016-12-18 00:09:18 downhill.base:232 Adam 36 loss=0.376616 error=0.272578 grad(U)=0.000010 grad(V)=0.000157\n",
      "I 2016-12-18 00:09:19 downhill.base:232 Adam 37 loss=0.375361 error=0.271600 grad(U)=0.000010 grad(V)=0.000156\n",
      "I 2016-12-18 00:09:19 downhill.base:232 Adam 38 loss=0.374124 error=0.270637 grad(U)=0.000010 grad(V)=0.000155\n",
      "I 2016-12-18 00:09:19 downhill.base:232 Adam 39 loss=0.372905 error=0.269689 grad(U)=0.000010 grad(V)=0.000154\n",
      "I 2016-12-18 00:09:19 downhill.base:232 Adam 40 loss=0.371704 error=0.268755 grad(U)=0.000010 grad(V)=0.000152\n",
      "I 2016-12-18 00:09:19 downhill.base:232 validation 4 loss=0.370521 error=0.267835 grad(U)=0.000010 grad(V)=0.000151 *\n",
      "I 2016-12-18 00:09:20 downhill.base:232 Adam 41 loss=0.370521 error=0.267835 grad(U)=0.000010 grad(V)=0.000151\n",
      "I 2016-12-18 00:09:20 downhill.base:232 Adam 42 loss=0.369353 error=0.266928 grad(U)=0.000010 grad(V)=0.000150\n",
      "I 2016-12-18 00:09:20 downhill.base:232 Adam 43 loss=0.368202 error=0.266035 grad(U)=0.000010 grad(V)=0.000149\n",
      "I 2016-12-18 00:09:20 downhill.base:232 Adam 44 loss=0.367067 error=0.265155 grad(U)=0.000010 grad(V)=0.000148\n",
      "I 2016-12-18 00:09:20 downhill.base:232 Adam 45 loss=0.365948 error=0.264287 grad(U)=0.000010 grad(V)=0.000147\n",
      "I 2016-12-18 00:09:21 downhill.base:232 Adam 46 loss=0.364843 error=0.263431 grad(U)=0.000010 grad(V)=0.000146\n",
      "I 2016-12-18 00:09:21 downhill.base:232 Adam 47 loss=0.363752 error=0.262588 grad(U)=0.000010 grad(V)=0.000145\n",
      "I 2016-12-18 00:09:21 downhill.base:232 Adam 48 loss=0.362676 error=0.261755 grad(U)=0.000010 grad(V)=0.000144\n",
      "I 2016-12-18 00:09:21 downhill.base:232 Adam 49 loss=0.361614 error=0.260935 grad(U)=0.000009 grad(V)=0.000143\n",
      "I 2016-12-18 00:09:21 downhill.base:232 Adam 50 loss=0.360565 error=0.260125 grad(U)=0.000009 grad(V)=0.000142\n",
      "I 2016-12-18 00:09:22 downhill.base:232 validation 5 loss=0.359530 error=0.259325 grad(U)=0.000009 grad(V)=0.000141 *\n",
      "I 2016-12-18 00:09:22 downhill.base:232 Adam 51 loss=0.359530 error=0.259325 grad(U)=0.000009 grad(V)=0.000141\n",
      "I 2016-12-18 00:09:22 downhill.base:232 Adam 52 loss=0.358507 error=0.258537 grad(U)=0.000009 grad(V)=0.000140\n",
      "I 2016-12-18 00:09:22 downhill.base:232 Adam 53 loss=0.357496 error=0.257758 grad(U)=0.000009 grad(V)=0.000139\n",
      "I 2016-12-18 00:09:22 downhill.base:232 Adam 54 loss=0.356498 error=0.256989 grad(U)=0.000009 grad(V)=0.000138\n",
      "I 2016-12-18 00:09:23 downhill.base:232 Adam 55 loss=0.355511 error=0.256230 grad(U)=0.000009 grad(V)=0.000137\n",
      "I 2016-12-18 00:09:23 downhill.base:232 Adam 56 loss=0.354536 error=0.255480 grad(U)=0.000009 grad(V)=0.000136\n",
      "I 2016-12-18 00:09:23 downhill.base:232 Adam 57 loss=0.353572 error=0.254739 grad(U)=0.000009 grad(V)=0.000136\n",
      "I 2016-12-18 00:09:23 downhill.base:232 Adam 58 loss=0.352619 error=0.254007 grad(U)=0.000009 grad(V)=0.000135\n",
      "I 2016-12-18 00:09:23 downhill.base:232 Adam 59 loss=0.351676 error=0.253284 grad(U)=0.000009 grad(V)=0.000134\n",
      "I 2016-12-18 00:09:24 downhill.base:232 Adam 60 loss=0.350744 error=0.252569 grad(U)=0.000009 grad(V)=0.000133\n",
      "I 2016-12-18 00:09:24 downhill.base:232 validation 6 loss=0.349823 error=0.251862 grad(U)=0.000009 grad(V)=0.000132 *\n",
      "I 2016-12-18 00:09:24 downhill.base:232 Adam 61 loss=0.349823 error=0.251862 grad(U)=0.000009 grad(V)=0.000132\n",
      "I 2016-12-18 00:09:24 downhill.base:232 Adam 62 loss=0.348911 error=0.251164 grad(U)=0.000009 grad(V)=0.000131\n",
      "I 2016-12-18 00:09:24 downhill.base:232 Adam 63 loss=0.348008 error=0.250473 grad(U)=0.000009 grad(V)=0.000130\n",
      "I 2016-12-18 00:09:25 downhill.base:232 Adam 64 loss=0.347115 error=0.249789 grad(U)=0.000009 grad(V)=0.000130\n",
      "I 2016-12-18 00:09:25 downhill.base:232 Adam 65 loss=0.346232 error=0.249114 grad(U)=0.000009 grad(V)=0.000129\n",
      "I 2016-12-18 00:09:25 downhill.base:232 Adam 66 loss=0.345357 error=0.248445 grad(U)=0.000008 grad(V)=0.000128\n",
      "I 2016-12-18 00:09:25 downhill.base:232 Adam 67 loss=0.344491 error=0.247784 grad(U)=0.000008 grad(V)=0.000127\n",
      "I 2016-12-18 00:09:25 downhill.base:232 Adam 68 loss=0.343633 error=0.247129 grad(U)=0.000008 grad(V)=0.000127\n",
      "I 2016-12-18 00:09:26 downhill.base:232 Adam 69 loss=0.342784 error=0.246482 grad(U)=0.000008 grad(V)=0.000126\n",
      "I 2016-12-18 00:09:26 downhill.base:232 Adam 70 loss=0.341943 error=0.245840 grad(U)=0.000008 grad(V)=0.000125\n",
      "I 2016-12-18 00:09:26 downhill.base:232 validation 7 loss=0.341109 error=0.245206 grad(U)=0.000008 grad(V)=0.000124 *\n",
      "I 2016-12-18 00:09:26 downhill.base:232 Adam 71 loss=0.341109 error=0.245206 grad(U)=0.000008 grad(V)=0.000124\n",
      "I 2016-12-18 00:09:26 downhill.base:232 Adam 72 loss=0.340284 error=0.244578 grad(U)=0.000008 grad(V)=0.000124\n",
      "I 2016-12-18 00:09:27 downhill.base:232 Adam 73 loss=0.339466 error=0.243955 grad(U)=0.000008 grad(V)=0.000123\n",
      "I 2016-12-18 00:09:27 downhill.base:232 Adam 74 loss=0.338656 error=0.243339 grad(U)=0.000008 grad(V)=0.000122\n",
      "I 2016-12-18 00:09:27 downhill.base:232 Adam 75 loss=0.337853 error=0.242729 grad(U)=0.000008 grad(V)=0.000122\n",
      "I 2016-12-18 00:09:27 downhill.base:232 Adam 76 loss=0.337057 error=0.242125 grad(U)=0.000008 grad(V)=0.000121\n",
      "I 2016-12-18 00:09:27 downhill.base:232 Adam 77 loss=0.336268 error=0.241526 grad(U)=0.000008 grad(V)=0.000120\n",
      "I 2016-12-18 00:09:28 downhill.base:232 Adam 78 loss=0.335486 error=0.240933 grad(U)=0.000008 grad(V)=0.000119\n",
      "I 2016-12-18 00:09:28 downhill.base:232 Adam 79 loss=0.334710 error=0.240345 grad(U)=0.000008 grad(V)=0.000119\n",
      "I 2016-12-18 00:09:28 downhill.base:232 Adam 80 loss=0.333942 error=0.239763 grad(U)=0.000008 grad(V)=0.000118\n",
      "I 2016-12-18 00:09:28 downhill.base:232 validation 8 loss=0.333179 error=0.239186 grad(U)=0.000008 grad(V)=0.000117 *\n",
      "I 2016-12-18 00:09:28 downhill.base:232 Adam 81 loss=0.333179 error=0.239186 grad(U)=0.000008 grad(V)=0.000117\n",
      "I 2016-12-18 00:09:29 downhill.base:232 Adam 82 loss=0.332423 error=0.238614 grad(U)=0.000008 grad(V)=0.000117\n",
      "I 2016-12-18 00:09:29 downhill.base:232 Adam 83 loss=0.331673 error=0.238047 grad(U)=0.000008 grad(V)=0.000116\n",
      "I 2016-12-18 00:09:29 downhill.base:232 Adam 84 loss=0.330929 error=0.237485 grad(U)=0.000008 grad(V)=0.000116\n",
      "I 2016-12-18 00:09:29 downhill.base:232 Adam 85 loss=0.330191 error=0.236927 grad(U)=0.000008 grad(V)=0.000115\n",
      "I 2016-12-18 00:09:29 downhill.base:232 Adam 86 loss=0.329459 error=0.236375 grad(U)=0.000008 grad(V)=0.000114\n",
      "I 2016-12-18 00:09:30 downhill.base:232 Adam 87 loss=0.328732 error=0.235827 grad(U)=0.000007 grad(V)=0.000114\n",
      "I 2016-12-18 00:09:30 downhill.base:232 Adam 88 loss=0.328011 error=0.235283 grad(U)=0.000007 grad(V)=0.000113\n",
      "I 2016-12-18 00:09:30 downhill.base:232 Adam 89 loss=0.327295 error=0.234744 grad(U)=0.000007 grad(V)=0.000112\n",
      "I 2016-12-18 00:09:30 downhill.base:232 Adam 90 loss=0.326585 error=0.234209 grad(U)=0.000007 grad(V)=0.000112\n",
      "I 2016-12-18 00:09:30 downhill.base:232 validation 9 loss=0.325880 error=0.233679 grad(U)=0.000007 grad(V)=0.000111 *\n",
      "I 2016-12-18 00:09:31 downhill.base:232 Adam 91 loss=0.325880 error=0.233679 grad(U)=0.000007 grad(V)=0.000111\n",
      "I 2016-12-18 00:09:31 downhill.base:232 Adam 92 loss=0.325181 error=0.233153 grad(U)=0.000007 grad(V)=0.000111\n",
      "I 2016-12-18 00:09:31 downhill.base:232 Adam 93 loss=0.324486 error=0.232631 grad(U)=0.000007 grad(V)=0.000110\n",
      "I 2016-12-18 00:09:31 downhill.base:232 Adam 94 loss=0.323797 error=0.232113 grad(U)=0.000007 grad(V)=0.000109\n",
      "I 2016-12-18 00:09:31 downhill.base:232 Adam 95 loss=0.323112 error=0.231599 grad(U)=0.000007 grad(V)=0.000109\n",
      "I 2016-12-18 00:09:32 downhill.base:232 Adam 96 loss=0.322432 error=0.231089 grad(U)=0.000007 grad(V)=0.000108\n",
      "I 2016-12-18 00:09:32 downhill.base:232 Adam 97 loss=0.321757 error=0.230583 grad(U)=0.000007 grad(V)=0.000108\n",
      "I 2016-12-18 00:09:32 downhill.base:232 Adam 98 loss=0.321086 error=0.230080 grad(U)=0.000007 grad(V)=0.000107\n",
      "I 2016-12-18 00:09:32 downhill.base:232 Adam 99 loss=0.320420 error=0.229581 grad(U)=0.000007 grad(V)=0.000107\n",
      "I 2016-12-18 00:09:32 downhill.base:232 Adam 100 loss=0.319759 error=0.229087 grad(U)=0.000007 grad(V)=0.000106\n",
      "I 2016-12-18 00:09:33 downhill.base:232 validation 10 loss=0.319102 error=0.228595 grad(U)=0.000007 grad(V)=0.000106 *\n",
      "I 2016-12-18 00:09:33 downhill.base:232 Adam 101 loss=0.319102 error=0.228595 grad(U)=0.000007 grad(V)=0.000106\n",
      "I 2016-12-18 00:09:33 downhill.base:232 Adam 102 loss=0.318449 error=0.228107 grad(U)=0.000007 grad(V)=0.000105\n",
      "I 2016-12-18 00:09:33 downhill.base:232 Adam 103 loss=0.317801 error=0.227623 grad(U)=0.000007 grad(V)=0.000105\n",
      "I 2016-12-18 00:09:33 downhill.base:232 Adam 104 loss=0.317157 error=0.227142 grad(U)=0.000007 grad(V)=0.000104\n",
      "I 2016-12-18 00:09:34 downhill.base:232 Adam 105 loss=0.316517 error=0.226665 grad(U)=0.000007 grad(V)=0.000103\n",
      "I 2016-12-18 00:09:34 downhill.base:232 Adam 106 loss=0.315881 error=0.226190 grad(U)=0.000007 grad(V)=0.000103\n",
      "I 2016-12-18 00:09:34 downhill.base:232 Adam 107 loss=0.315250 error=0.225719 grad(U)=0.000007 grad(V)=0.000102\n",
      "I 2016-12-18 00:09:34 downhill.base:232 Adam 108 loss=0.314622 error=0.225252 grad(U)=0.000007 grad(V)=0.000102\n",
      "I 2016-12-18 00:09:34 downhill.base:232 Adam 109 loss=0.313998 error=0.224787 grad(U)=0.000007 grad(V)=0.000101\n",
      "I 2016-12-18 00:09:35 downhill.base:232 Adam 110 loss=0.313378 error=0.224326 grad(U)=0.000007 grad(V)=0.000101\n",
      "I 2016-12-18 00:09:35 downhill.base:232 validation 11 loss=0.312762 error=0.223867 grad(U)=0.000007 grad(V)=0.000100 *\n",
      "I 2016-12-18 00:09:35 downhill.base:232 Adam 111 loss=0.312762 error=0.223867 grad(U)=0.000007 grad(V)=0.000100\n",
      "I 2016-12-18 00:09:35 downhill.base:232 Adam 112 loss=0.312149 error=0.223412 grad(U)=0.000007 grad(V)=0.000100\n",
      "I 2016-12-18 00:09:35 downhill.base:232 Adam 113 loss=0.311541 error=0.222960 grad(U)=0.000007 grad(V)=0.000099\n",
      "I 2016-12-18 00:09:36 downhill.base:232 Adam 114 loss=0.310935 error=0.222510 grad(U)=0.000006 grad(V)=0.000099\n",
      "I 2016-12-18 00:09:36 downhill.base:232 Adam 115 loss=0.310334 error=0.222064 grad(U)=0.000006 grad(V)=0.000098\n",
      "I 2016-12-18 00:09:36 downhill.base:232 Adam 116 loss=0.309736 error=0.221620 grad(U)=0.000006 grad(V)=0.000098\n",
      "I 2016-12-18 00:09:36 downhill.base:232 Adam 117 loss=0.309141 error=0.221180 grad(U)=0.000006 grad(V)=0.000097\n",
      "I 2016-12-18 00:09:36 downhill.base:232 Adam 118 loss=0.308550 error=0.220742 grad(U)=0.000006 grad(V)=0.000097\n",
      "I 2016-12-18 00:09:37 downhill.base:232 Adam 119 loss=0.307962 error=0.220307 grad(U)=0.000006 grad(V)=0.000097\n",
      "I 2016-12-18 00:09:37 downhill.base:232 Adam 120 loss=0.307378 error=0.219874 grad(U)=0.000006 grad(V)=0.000096\n",
      "I 2016-12-18 00:09:37 downhill.base:232 validation 12 loss=0.306797 error=0.219444 grad(U)=0.000006 grad(V)=0.000096 *\n",
      "I 2016-12-18 00:09:37 downhill.base:232 Adam 121 loss=0.306797 error=0.219444 grad(U)=0.000006 grad(V)=0.000096\n",
      "I 2016-12-18 00:09:37 downhill.base:232 Adam 122 loss=0.306219 error=0.219017 grad(U)=0.000006 grad(V)=0.000095\n",
      "I 2016-12-18 00:09:38 downhill.base:232 Adam 123 loss=0.305644 error=0.218592 grad(U)=0.000006 grad(V)=0.000095\n",
      "I 2016-12-18 00:09:38 downhill.base:232 Adam 124 loss=0.305072 error=0.218170 grad(U)=0.000006 grad(V)=0.000094\n",
      "I 2016-12-18 00:09:38 downhill.base:232 Adam 125 loss=0.304504 error=0.217751 grad(U)=0.000006 grad(V)=0.000094\n",
      "I 2016-12-18 00:09:38 downhill.base:232 Adam 126 loss=0.303939 error=0.217334 grad(U)=0.000006 grad(V)=0.000093\n",
      "I 2016-12-18 00:09:39 downhill.base:232 Adam 127 loss=0.303376 error=0.216919 grad(U)=0.000006 grad(V)=0.000093\n",
      "I 2016-12-18 00:09:39 downhill.base:232 Adam 128 loss=0.302817 error=0.216507 grad(U)=0.000006 grad(V)=0.000092\n",
      "I 2016-12-18 00:09:39 downhill.base:232 Adam 129 loss=0.302261 error=0.216098 grad(U)=0.000006 grad(V)=0.000092\n",
      "I 2016-12-18 00:09:39 downhill.base:232 Adam 130 loss=0.301707 error=0.215690 grad(U)=0.000006 grad(V)=0.000092\n",
      "I 2016-12-18 00:09:39 downhill.base:232 validation 13 loss=0.301157 error=0.215285 grad(U)=0.000006 grad(V)=0.000091 *\n",
      "I 2016-12-18 00:09:40 downhill.base:232 Adam 131 loss=0.301157 error=0.215285 grad(U)=0.000006 grad(V)=0.000091\n",
      "I 2016-12-18 00:09:40 downhill.base:232 Adam 132 loss=0.300609 error=0.214883 grad(U)=0.000006 grad(V)=0.000091\n",
      "I 2016-12-18 00:09:40 downhill.base:232 Adam 133 loss=0.300064 error=0.214482 grad(U)=0.000006 grad(V)=0.000090\n",
      "I 2016-12-18 00:09:40 downhill.base:232 Adam 134 loss=0.299522 error=0.214084 grad(U)=0.000006 grad(V)=0.000090\n",
      "I 2016-12-18 00:09:41 downhill.base:232 Adam 135 loss=0.298982 error=0.213689 grad(U)=0.000006 grad(V)=0.000089\n",
      "I 2016-12-18 00:09:41 downhill.base:232 Adam 136 loss=0.298446 error=0.213295 grad(U)=0.000006 grad(V)=0.000089\n",
      "I 2016-12-18 00:09:41 downhill.base:232 Adam 137 loss=0.297912 error=0.212904 grad(U)=0.000006 grad(V)=0.000089\n",
      "I 2016-12-18 00:09:41 downhill.base:232 Adam 138 loss=0.297381 error=0.212514 grad(U)=0.000006 grad(V)=0.000088\n",
      "I 2016-12-18 00:09:41 downhill.base:232 Adam 139 loss=0.296852 error=0.212127 grad(U)=0.000006 grad(V)=0.000088\n",
      "I 2016-12-18 00:09:42 downhill.base:232 Adam 140 loss=0.296326 error=0.211742 grad(U)=0.000006 grad(V)=0.000087\n",
      "I 2016-12-18 00:09:42 downhill.base:232 validation 14 loss=0.295802 error=0.211359 grad(U)=0.000006 grad(V)=0.000087 *\n",
      "I 2016-12-18 00:09:42 downhill.base:232 Adam 141 loss=0.295802 error=0.211359 grad(U)=0.000006 grad(V)=0.000087\n",
      "I 2016-12-18 00:09:42 downhill.base:232 Adam 142 loss=0.295281 error=0.210978 grad(U)=0.000006 grad(V)=0.000087\n",
      "I 2016-12-18 00:09:42 downhill.base:232 Adam 143 loss=0.294763 error=0.210599 grad(U)=0.000006 grad(V)=0.000086\n",
      "I 2016-12-18 00:09:43 downhill.base:232 Adam 144 loss=0.294247 error=0.210223 grad(U)=0.000006 grad(V)=0.000086\n",
      "I 2016-12-18 00:09:43 downhill.base:232 Adam 145 loss=0.293733 error=0.209848 grad(U)=0.000006 grad(V)=0.000085\n",
      "I 2016-12-18 00:09:43 downhill.base:232 Adam 146 loss=0.293222 error=0.209475 grad(U)=0.000006 grad(V)=0.000085\n",
      "I 2016-12-18 00:09:43 downhill.base:232 Adam 147 loss=0.292713 error=0.209104 grad(U)=0.000006 grad(V)=0.000085\n",
      "I 2016-12-18 00:09:43 downhill.base:232 Adam 148 loss=0.292207 error=0.208735 grad(U)=0.000005 grad(V)=0.000084\n",
      "I 2016-12-18 00:09:44 downhill.base:232 Adam 149 loss=0.291703 error=0.208368 grad(U)=0.000005 grad(V)=0.000084\n",
      "I 2016-12-18 00:09:44 downhill.base:232 Adam 150 loss=0.291201 error=0.208003 grad(U)=0.000005 grad(V)=0.000084\n",
      "I 2016-12-18 00:09:44 downhill.base:232 validation 15 loss=0.290702 error=0.207640 grad(U)=0.000005 grad(V)=0.000083 *\n",
      "I 2016-12-18 00:09:44 downhill.base:232 Adam 151 loss=0.290702 error=0.207640 grad(U)=0.000005 grad(V)=0.000083\n",
      "I 2016-12-18 00:09:44 downhill.base:232 Adam 152 loss=0.290205 error=0.207278 grad(U)=0.000005 grad(V)=0.000083\n",
      "I 2016-12-18 00:09:44 downhill.base:232 Adam 153 loss=0.289710 error=0.206919 grad(U)=0.000005 grad(V)=0.000082\n",
      "I 2016-12-18 00:09:45 downhill.base:232 Adam 154 loss=0.289217 error=0.206561 grad(U)=0.000005 grad(V)=0.000082\n",
      "I 2016-12-18 00:09:45 downhill.base:232 Adam 155 loss=0.288727 error=0.206205 grad(U)=0.000005 grad(V)=0.000082\n",
      "I 2016-12-18 00:09:45 downhill.base:232 Adam 156 loss=0.288239 error=0.205851 grad(U)=0.000005 grad(V)=0.000081\n",
      "I 2016-12-18 00:09:45 downhill.base:232 Adam 157 loss=0.287752 error=0.205498 grad(U)=0.000005 grad(V)=0.000081\n",
      "I 2016-12-18 00:09:45 downhill.base:232 Adam 158 loss=0.287268 error=0.205147 grad(U)=0.000005 grad(V)=0.000081\n",
      "I 2016-12-18 00:09:46 downhill.base:232 Adam 159 loss=0.286787 error=0.204798 grad(U)=0.000005 grad(V)=0.000080\n",
      "I 2016-12-18 00:09:46 downhill.base:232 Adam 160 loss=0.286307 error=0.204451 grad(U)=0.000005 grad(V)=0.000080\n",
      "I 2016-12-18 00:09:46 downhill.base:232 validation 16 loss=0.285829 error=0.204106 grad(U)=0.000005 grad(V)=0.000080 *\n",
      "I 2016-12-18 00:09:46 downhill.base:232 Adam 161 loss=0.285829 error=0.204106 grad(U)=0.000005 grad(V)=0.000080\n",
      "I 2016-12-18 00:09:46 downhill.base:232 Adam 162 loss=0.285354 error=0.203762 grad(U)=0.000005 grad(V)=0.000079\n",
      "I 2016-12-18 00:09:47 downhill.base:232 Adam 163 loss=0.284880 error=0.203419 grad(U)=0.000005 grad(V)=0.000079\n",
      "I 2016-12-18 00:09:47 downhill.base:232 Adam 164 loss=0.284408 error=0.203079 grad(U)=0.000005 grad(V)=0.000078\n",
      "I 2016-12-18 00:09:47 downhill.base:232 Adam 165 loss=0.283939 error=0.202740 grad(U)=0.000005 grad(V)=0.000078\n",
      "I 2016-12-18 00:09:47 downhill.base:232 Adam 166 loss=0.283471 error=0.202402 grad(U)=0.000005 grad(V)=0.000078\n",
      "I 2016-12-18 00:09:47 downhill.base:232 Adam 167 loss=0.283006 error=0.202067 grad(U)=0.000005 grad(V)=0.000077\n",
      "I 2016-12-18 00:09:48 downhill.base:232 Adam 168 loss=0.282542 error=0.201732 grad(U)=0.000005 grad(V)=0.000077\n",
      "I 2016-12-18 00:09:48 downhill.base:232 Adam 169 loss=0.282080 error=0.201400 grad(U)=0.000005 grad(V)=0.000077\n",
      "I 2016-12-18 00:09:48 downhill.base:232 Adam 170 loss=0.281621 error=0.201069 grad(U)=0.000005 grad(V)=0.000076\n",
      "I 2016-12-18 00:09:48 downhill.base:232 validation 17 loss=0.281163 error=0.200739 grad(U)=0.000005 grad(V)=0.000076 *\n",
      "I 2016-12-18 00:09:48 downhill.base:232 Adam 171 loss=0.281163 error=0.200739 grad(U)=0.000005 grad(V)=0.000076\n",
      "I 2016-12-18 00:09:49 downhill.base:232 Adam 172 loss=0.280707 error=0.200411 grad(U)=0.000005 grad(V)=0.000076\n",
      "I 2016-12-18 00:09:49 downhill.base:232 Adam 173 loss=0.280252 error=0.200085 grad(U)=0.000005 grad(V)=0.000075\n",
      "I 2016-12-18 00:09:49 downhill.base:232 Adam 174 loss=0.279800 error=0.199760 grad(U)=0.000005 grad(V)=0.000075\n",
      "I 2016-12-18 00:09:49 downhill.base:232 Adam 175 loss=0.279349 error=0.199436 grad(U)=0.000005 grad(V)=0.000075\n",
      "I 2016-12-18 00:09:49 downhill.base:232 Adam 176 loss=0.278900 error=0.199114 grad(U)=0.000005 grad(V)=0.000074\n",
      "I 2016-12-18 00:09:50 downhill.base:232 Adam 177 loss=0.278453 error=0.198794 grad(U)=0.000005 grad(V)=0.000074\n",
      "I 2016-12-18 00:09:50 downhill.base:232 Adam 178 loss=0.278008 error=0.198475 grad(U)=0.000005 grad(V)=0.000074\n",
      "I 2016-12-18 00:09:50 downhill.base:232 Adam 179 loss=0.277565 error=0.198157 grad(U)=0.000005 grad(V)=0.000074\n",
      "I 2016-12-18 00:09:50 downhill.base:232 Adam 180 loss=0.277123 error=0.197841 grad(U)=0.000005 grad(V)=0.000073\n",
      "I 2016-12-18 00:09:50 downhill.base:232 validation 18 loss=0.276683 error=0.197526 grad(U)=0.000005 grad(V)=0.000073 *\n",
      "I 2016-12-18 00:09:51 downhill.base:232 Adam 181 loss=0.276683 error=0.197526 grad(U)=0.000005 grad(V)=0.000073\n",
      "I 2016-12-18 00:09:51 downhill.base:232 Adam 182 loss=0.276245 error=0.197212 grad(U)=0.000005 grad(V)=0.000073\n",
      "I 2016-12-18 00:09:51 downhill.base:232 Adam 183 loss=0.275808 error=0.196900 grad(U)=0.000005 grad(V)=0.000072\n",
      "I 2016-12-18 00:09:51 downhill.base:232 Adam 184 loss=0.275373 error=0.196589 grad(U)=0.000005 grad(V)=0.000072\n",
      "I 2016-12-18 00:09:51 downhill.base:232 Adam 185 loss=0.274940 error=0.196280 grad(U)=0.000005 grad(V)=0.000072\n",
      "I 2016-12-18 00:09:52 downhill.base:232 Adam 186 loss=0.274509 error=0.195972 grad(U)=0.000005 grad(V)=0.000071\n",
      "I 2016-12-18 00:09:52 downhill.base:232 Adam 187 loss=0.274079 error=0.195665 grad(U)=0.000005 grad(V)=0.000071\n",
      "I 2016-12-18 00:09:52 downhill.base:232 Adam 188 loss=0.273650 error=0.195360 grad(U)=0.000005 grad(V)=0.000071\n",
      "I 2016-12-18 00:09:52 downhill.base:232 Adam 189 loss=0.273224 error=0.195056 grad(U)=0.000005 grad(V)=0.000070\n",
      "I 2016-12-18 00:09:52 downhill.base:232 Adam 190 loss=0.272799 error=0.194753 grad(U)=0.000005 grad(V)=0.000070\n",
      "I 2016-12-18 00:09:53 downhill.base:232 validation 19 loss=0.272375 error=0.194452 grad(U)=0.000005 grad(V)=0.000070 *\n",
      "I 2016-12-18 00:09:53 downhill.base:232 Adam 191 loss=0.272375 error=0.194452 grad(U)=0.000005 grad(V)=0.000070\n",
      "I 2016-12-18 00:09:53 downhill.base:232 Adam 192 loss=0.271953 error=0.194152 grad(U)=0.000004 grad(V)=0.000070\n",
      "I 2016-12-18 00:09:53 downhill.base:232 Adam 193 loss=0.271533 error=0.193853 grad(U)=0.000004 grad(V)=0.000069\n",
      "I 2016-12-18 00:09:53 downhill.base:232 Adam 194 loss=0.271114 error=0.193555 grad(U)=0.000004 grad(V)=0.000069\n",
      "I 2016-12-18 00:09:54 downhill.base:232 Adam 195 loss=0.270697 error=0.193259 grad(U)=0.000004 grad(V)=0.000069\n",
      "I 2016-12-18 00:09:54 downhill.base:232 Adam 196 loss=0.270281 error=0.192964 grad(U)=0.000004 grad(V)=0.000068\n",
      "I 2016-12-18 00:09:54 downhill.base:232 Adam 197 loss=0.269867 error=0.192670 grad(U)=0.000004 grad(V)=0.000068\n",
      "I 2016-12-18 00:09:54 downhill.base:232 Adam 198 loss=0.269455 error=0.192377 grad(U)=0.000004 grad(V)=0.000068\n",
      "I 2016-12-18 00:09:54 downhill.base:232 Adam 199 loss=0.269043 error=0.192086 grad(U)=0.000004 grad(V)=0.000068\n",
      "I 2016-12-18 00:09:55 downhill.base:232 Adam 200 loss=0.268634 error=0.191796 grad(U)=0.000004 grad(V)=0.000067\n",
      "I 2016-12-18 00:09:55 downhill.base:232 validation 20 loss=0.268226 error=0.191507 grad(U)=0.000004 grad(V)=0.000067 *\n",
      "I 2016-12-18 00:09:55 downhill.base:232 Adam 201 loss=0.268226 error=0.191507 grad(U)=0.000004 grad(V)=0.000067\n",
      "I 2016-12-18 00:09:55 downhill.base:232 Adam 202 loss=0.267819 error=0.191219 grad(U)=0.000004 grad(V)=0.000067\n",
      "I 2016-12-18 00:09:55 downhill.base:232 Adam 203 loss=0.267414 error=0.190933 grad(U)=0.000004 grad(V)=0.000066\n",
      "I 2016-12-18 00:09:56 downhill.base:232 Adam 204 loss=0.267010 error=0.190647 grad(U)=0.000004 grad(V)=0.000066\n",
      "I 2016-12-18 00:09:56 downhill.base:232 Adam 205 loss=0.266607 error=0.190363 grad(U)=0.000004 grad(V)=0.000066\n",
      "I 2016-12-18 00:09:56 downhill.base:232 Adam 206 loss=0.266207 error=0.190080 grad(U)=0.000004 grad(V)=0.000066\n",
      "I 2016-12-18 00:09:56 downhill.base:232 Adam 207 loss=0.265807 error=0.189798 grad(U)=0.000004 grad(V)=0.000065\n",
      "I 2016-12-18 00:09:56 downhill.base:232 Adam 208 loss=0.265409 error=0.189517 grad(U)=0.000004 grad(V)=0.000065\n",
      "I 2016-12-18 00:09:56 downhill.base:232 Adam 209 loss=0.265012 error=0.189238 grad(U)=0.000004 grad(V)=0.000065\n",
      "I 2016-12-18 00:09:57 downhill.base:232 Adam 210 loss=0.264617 error=0.188959 grad(U)=0.000004 grad(V)=0.000064\n",
      "I 2016-12-18 00:09:57 downhill.base:232 validation 21 loss=0.264222 error=0.188682 grad(U)=0.000004 grad(V)=0.000064 *\n",
      "I 2016-12-18 00:09:57 downhill.base:232 Adam 211 loss=0.264222 error=0.188682 grad(U)=0.000004 grad(V)=0.000064\n",
      "I 2016-12-18 00:09:57 downhill.base:232 Adam 212 loss=0.263830 error=0.188405 grad(U)=0.000004 grad(V)=0.000064\n",
      "I 2016-12-18 00:09:57 downhill.base:232 Adam 213 loss=0.263438 error=0.188130 grad(U)=0.000004 grad(V)=0.000064\n",
      "I 2016-12-18 00:09:58 downhill.base:232 Adam 214 loss=0.263048 error=0.187856 grad(U)=0.000004 grad(V)=0.000063\n",
      "I 2016-12-18 00:09:58 downhill.base:232 Adam 215 loss=0.262660 error=0.187583 grad(U)=0.000004 grad(V)=0.000063\n",
      "I 2016-12-18 00:09:58 downhill.base:232 Adam 216 loss=0.262272 error=0.187311 grad(U)=0.000004 grad(V)=0.000063\n",
      "I 2016-12-18 00:09:58 downhill.base:232 Adam 217 loss=0.261886 error=0.187040 grad(U)=0.000004 grad(V)=0.000063\n",
      "I 2016-12-18 00:09:58 downhill.base:232 Adam 218 loss=0.261501 error=0.186770 grad(U)=0.000004 grad(V)=0.000062\n",
      "I 2016-12-18 00:09:59 downhill.base:232 Adam 219 loss=0.261118 error=0.186501 grad(U)=0.000004 grad(V)=0.000062\n",
      "I 2016-12-18 00:09:59 downhill.base:232 Adam 220 loss=0.260736 error=0.186234 grad(U)=0.000004 grad(V)=0.000062\n",
      "I 2016-12-18 00:09:59 downhill.base:232 validation 22 loss=0.260355 error=0.185967 grad(U)=0.000004 grad(V)=0.000062 *\n",
      "I 2016-12-18 00:09:59 downhill.base:232 Adam 221 loss=0.260355 error=0.185967 grad(U)=0.000004 grad(V)=0.000062\n",
      "I 2016-12-18 00:09:59 downhill.base:232 Adam 222 loss=0.259975 error=0.185701 grad(U)=0.000004 grad(V)=0.000061\n",
      "I 2016-12-18 00:10:00 downhill.base:232 Adam 223 loss=0.259597 error=0.185437 grad(U)=0.000004 grad(V)=0.000061\n",
      "I 2016-12-18 00:10:00 downhill.base:232 Adam 224 loss=0.259220 error=0.185173 grad(U)=0.000004 grad(V)=0.000061\n",
      "I 2016-12-18 00:10:00 downhill.base:232 Adam 225 loss=0.258844 error=0.184911 grad(U)=0.000004 grad(V)=0.000061\n",
      "I 2016-12-18 00:10:00 downhill.base:232 Adam 226 loss=0.258469 error=0.184649 grad(U)=0.000004 grad(V)=0.000060\n",
      "I 2016-12-18 00:10:00 downhill.base:232 Adam 227 loss=0.258096 error=0.184388 grad(U)=0.000004 grad(V)=0.000060\n",
      "I 2016-12-18 00:10:01 downhill.base:232 Adam 228 loss=0.257724 error=0.184129 grad(U)=0.000004 grad(V)=0.000060\n",
      "I 2016-12-18 00:10:01 downhill.base:232 Adam 229 loss=0.257353 error=0.183870 grad(U)=0.000004 grad(V)=0.000060\n",
      "I 2016-12-18 00:10:01 downhill.base:232 Adam 230 loss=0.256983 error=0.183613 grad(U)=0.000004 grad(V)=0.000059\n",
      "I 2016-12-18 00:10:01 downhill.base:232 validation 23 loss=0.256614 error=0.183356 grad(U)=0.000004 grad(V)=0.000059 *\n",
      "I 2016-12-18 00:10:01 downhill.base:232 Adam 231 loss=0.256614 error=0.183356 grad(U)=0.000004 grad(V)=0.000059\n",
      "I 2016-12-18 00:10:02 downhill.base:232 Adam 232 loss=0.256247 error=0.183100 grad(U)=0.000004 grad(V)=0.000059\n",
      "I 2016-12-18 00:10:02 downhill.base:232 Adam 233 loss=0.255881 error=0.182846 grad(U)=0.000004 grad(V)=0.000059\n",
      "I 2016-12-18 00:10:02 downhill.base:232 Adam 234 loss=0.255516 error=0.182592 grad(U)=0.000004 grad(V)=0.000058\n",
      "I 2016-12-18 00:10:02 downhill.base:232 Adam 235 loss=0.255152 error=0.182339 grad(U)=0.000004 grad(V)=0.000058\n",
      "I 2016-12-18 00:10:02 downhill.base:232 Adam 236 loss=0.254789 error=0.182087 grad(U)=0.000004 grad(V)=0.000058\n",
      "I 2016-12-18 00:10:03 downhill.base:232 Adam 237 loss=0.254428 error=0.181837 grad(U)=0.000004 grad(V)=0.000058\n",
      "I 2016-12-18 00:10:03 downhill.base:232 Adam 238 loss=0.254067 error=0.181587 grad(U)=0.000004 grad(V)=0.000057\n",
      "I 2016-12-18 00:10:03 downhill.base:232 Adam 239 loss=0.253708 error=0.181338 grad(U)=0.000004 grad(V)=0.000057\n",
      "I 2016-12-18 00:10:03 downhill.base:232 Adam 240 loss=0.253350 error=0.181090 grad(U)=0.000004 grad(V)=0.000057\n",
      "I 2016-12-18 00:10:03 downhill.base:232 validation 24 loss=0.252993 error=0.180842 grad(U)=0.000004 grad(V)=0.000057 *\n",
      "I 2016-12-18 00:10:04 downhill.base:232 Adam 241 loss=0.252993 error=0.180842 grad(U)=0.000004 grad(V)=0.000057\n",
      "I 2016-12-18 00:10:04 downhill.base:232 Adam 242 loss=0.252637 error=0.180596 grad(U)=0.000004 grad(V)=0.000057\n",
      "I 2016-12-18 00:10:04 downhill.base:232 Adam 243 loss=0.252282 error=0.180351 grad(U)=0.000004 grad(V)=0.000056\n",
      "I 2016-12-18 00:10:04 downhill.base:232 Adam 244 loss=0.251929 error=0.180106 grad(U)=0.000004 grad(V)=0.000056\n",
      "I 2016-12-18 00:10:04 downhill.base:232 Adam 245 loss=0.251576 error=0.179863 grad(U)=0.000004 grad(V)=0.000056\n",
      "I 2016-12-18 00:10:05 downhill.base:232 Adam 246 loss=0.251225 error=0.179620 grad(U)=0.000004 grad(V)=0.000056\n",
      "I 2016-12-18 00:10:05 downhill.base:232 Adam 247 loss=0.250874 error=0.179378 grad(U)=0.000004 grad(V)=0.000055\n",
      "I 2016-12-18 00:10:05 downhill.base:232 Adam 248 loss=0.250525 error=0.179137 grad(U)=0.000004 grad(V)=0.000055\n",
      "I 2016-12-18 00:10:05 downhill.base:232 Adam 249 loss=0.250177 error=0.178897 grad(U)=0.000004 grad(V)=0.000055\n",
      "I 2016-12-18 00:10:05 downhill.base:232 Adam 250 loss=0.249829 error=0.178658 grad(U)=0.000004 grad(V)=0.000055\n",
      "I 2016-12-18 00:10:06 downhill.base:232 validation 25 loss=0.249483 error=0.178420 grad(U)=0.000004 grad(V)=0.000054 *\n",
      "I 2016-12-18 00:10:06 downhill.base:232 Adam 251 loss=0.249483 error=0.178420 grad(U)=0.000004 grad(V)=0.000054\n",
      "I 2016-12-18 00:10:06 downhill.base:232 Adam 252 loss=0.249138 error=0.178183 grad(U)=0.000003 grad(V)=0.000054\n",
      "I 2016-12-18 00:10:06 downhill.base:232 Adam 253 loss=0.248794 error=0.177946 grad(U)=0.000003 grad(V)=0.000054\n",
      "I 2016-12-18 00:10:06 downhill.base:232 Adam 254 loss=0.248451 error=0.177710 grad(U)=0.000003 grad(V)=0.000054\n",
      "I 2016-12-18 00:10:07 downhill.base:232 Adam 255 loss=0.248109 error=0.177476 grad(U)=0.000003 grad(V)=0.000054\n",
      "I 2016-12-18 00:10:07 downhill.base:232 Adam 256 loss=0.247769 error=0.177242 grad(U)=0.000003 grad(V)=0.000053\n",
      "I 2016-12-18 00:10:07 downhill.base:232 Adam 257 loss=0.247429 error=0.177008 grad(U)=0.000003 grad(V)=0.000053\n",
      "I 2016-12-18 00:10:07 downhill.base:232 Adam 258 loss=0.247090 error=0.176776 grad(U)=0.000003 grad(V)=0.000053\n",
      "I 2016-12-18 00:10:07 downhill.base:232 Adam 259 loss=0.246752 error=0.176545 grad(U)=0.000003 grad(V)=0.000053\n",
      "I 2016-12-18 00:10:08 downhill.base:232 Adam 260 loss=0.246415 error=0.176314 grad(U)=0.000003 grad(V)=0.000053\n",
      "I 2016-12-18 00:10:08 downhill.base:232 validation 26 loss=0.246079 error=0.176084 grad(U)=0.000003 grad(V)=0.000052 *\n",
      "I 2016-12-18 00:10:08 downhill.base:232 Adam 261 loss=0.246079 error=0.176084 grad(U)=0.000003 grad(V)=0.000052\n",
      "I 2016-12-18 00:10:08 downhill.base:232 Adam 262 loss=0.245745 error=0.175855 grad(U)=0.000003 grad(V)=0.000052\n",
      "I 2016-12-18 00:10:08 downhill.base:232 Adam 263 loss=0.245411 error=0.175627 grad(U)=0.000003 grad(V)=0.000052\n",
      "I 2016-12-18 00:10:09 downhill.base:232 Adam 264 loss=0.245078 error=0.175399 grad(U)=0.000003 grad(V)=0.000052\n",
      "I 2016-12-18 00:10:09 downhill.base:232 Adam 265 loss=0.244746 error=0.175173 grad(U)=0.000003 grad(V)=0.000052\n",
      "I 2016-12-18 00:10:09 downhill.base:232 Adam 266 loss=0.244415 error=0.174947 grad(U)=0.000003 grad(V)=0.000051\n",
      "I 2016-12-18 00:10:09 downhill.base:232 Adam 267 loss=0.244085 error=0.174722 grad(U)=0.000003 grad(V)=0.000051\n",
      "I 2016-12-18 00:10:09 downhill.base:232 Adam 268 loss=0.243757 error=0.174498 grad(U)=0.000003 grad(V)=0.000051\n",
      "I 2016-12-18 00:10:10 downhill.base:232 Adam 269 loss=0.243429 error=0.174274 grad(U)=0.000003 grad(V)=0.000051\n",
      "I 2016-12-18 00:10:10 downhill.base:232 Adam 270 loss=0.243102 error=0.174051 grad(U)=0.000003 grad(V)=0.000050\n",
      "I 2016-12-18 00:10:10 downhill.base:232 validation 27 loss=0.242776 error=0.173829 grad(U)=0.000003 grad(V)=0.000050 *\n",
      "I 2016-12-18 00:10:10 downhill.base:232 Adam 271 loss=0.242776 error=0.173829 grad(U)=0.000003 grad(V)=0.000050\n",
      "I 2016-12-18 00:10:10 downhill.base:232 Adam 272 loss=0.242450 error=0.173608 grad(U)=0.000003 grad(V)=0.000050\n",
      "I 2016-12-18 00:10:11 downhill.base:232 Adam 273 loss=0.242126 error=0.173388 grad(U)=0.000003 grad(V)=0.000050\n",
      "I 2016-12-18 00:10:11 downhill.base:232 Adam 274 loss=0.241803 error=0.173168 grad(U)=0.000003 grad(V)=0.000050\n",
      "I 2016-12-18 00:10:11 downhill.base:232 Adam 275 loss=0.241481 error=0.172950 grad(U)=0.000003 grad(V)=0.000049\n",
      "I 2016-12-18 00:10:11 downhill.base:232 Adam 276 loss=0.241159 error=0.172732 grad(U)=0.000003 grad(V)=0.000049\n",
      "I 2016-12-18 00:10:12 downhill.base:232 Adam 277 loss=0.240839 error=0.172514 grad(U)=0.000003 grad(V)=0.000049\n",
      "I 2016-12-18 00:10:12 downhill.base:232 Adam 278 loss=0.240519 error=0.172298 grad(U)=0.000003 grad(V)=0.000049\n",
      "I 2016-12-18 00:10:12 downhill.base:232 Adam 279 loss=0.240200 error=0.172082 grad(U)=0.000003 grad(V)=0.000049\n",
      "I 2016-12-18 00:10:12 downhill.base:232 Adam 280 loss=0.239883 error=0.171867 grad(U)=0.000003 grad(V)=0.000049\n",
      "I 2016-12-18 00:10:12 downhill.base:232 validation 28 loss=0.239566 error=0.171653 grad(U)=0.000003 grad(V)=0.000048 *\n",
      "I 2016-12-18 00:10:13 downhill.base:232 Adam 281 loss=0.239566 error=0.171653 grad(U)=0.000003 grad(V)=0.000048\n",
      "I 2016-12-18 00:10:13 downhill.base:232 Adam 282 loss=0.239250 error=0.171439 grad(U)=0.000003 grad(V)=0.000048\n",
      "I 2016-12-18 00:10:13 downhill.base:232 Adam 283 loss=0.238935 error=0.171226 grad(U)=0.000003 grad(V)=0.000048\n",
      "I 2016-12-18 00:10:13 downhill.base:232 Adam 284 loss=0.238620 error=0.171014 grad(U)=0.000003 grad(V)=0.000048\n",
      "I 2016-12-18 00:10:14 downhill.base:232 Adam 285 loss=0.238307 error=0.170803 grad(U)=0.000003 grad(V)=0.000048\n",
      "I 2016-12-18 00:10:14 downhill.base:232 Adam 286 loss=0.237995 error=0.170592 grad(U)=0.000003 grad(V)=0.000047\n",
      "I 2016-12-18 00:10:14 downhill.base:232 Adam 287 loss=0.237683 error=0.170382 grad(U)=0.000003 grad(V)=0.000047\n",
      "I 2016-12-18 00:10:14 downhill.base:232 Adam 288 loss=0.237372 error=0.170173 grad(U)=0.000003 grad(V)=0.000047\n",
      "I 2016-12-18 00:10:14 downhill.base:232 Adam 289 loss=0.237063 error=0.169964 grad(U)=0.000003 grad(V)=0.000047\n",
      "I 2016-12-18 00:10:15 downhill.base:232 Adam 290 loss=0.236754 error=0.169756 grad(U)=0.000003 grad(V)=0.000047\n",
      "I 2016-12-18 00:10:15 downhill.base:232 validation 29 loss=0.236446 error=0.169549 grad(U)=0.000003 grad(V)=0.000046 *\n",
      "I 2016-12-18 00:10:15 downhill.base:232 Adam 291 loss=0.236446 error=0.169549 grad(U)=0.000003 grad(V)=0.000046\n",
      "I 2016-12-18 00:10:15 downhill.base:232 Adam 292 loss=0.236138 error=0.169343 grad(U)=0.000003 grad(V)=0.000046\n",
      "I 2016-12-18 00:10:16 downhill.base:232 Adam 293 loss=0.235832 error=0.169137 grad(U)=0.000003 grad(V)=0.000046\n",
      "I 2016-12-18 00:10:16 downhill.base:232 Adam 294 loss=0.235526 error=0.168932 grad(U)=0.000003 grad(V)=0.000046\n",
      "I 2016-12-18 00:10:16 downhill.base:232 Adam 295 loss=0.235222 error=0.168727 grad(U)=0.000003 grad(V)=0.000046\n",
      "I 2016-12-18 00:10:16 downhill.base:232 Adam 296 loss=0.234918 error=0.168524 grad(U)=0.000003 grad(V)=0.000046\n",
      "I 2016-12-18 00:10:17 downhill.base:232 Adam 297 loss=0.234615 error=0.168321 grad(U)=0.000003 grad(V)=0.000045\n",
      "I 2016-12-18 00:10:17 downhill.base:232 Adam 298 loss=0.234313 error=0.168118 grad(U)=0.000003 grad(V)=0.000045\n",
      "I 2016-12-18 00:10:17 downhill.base:232 Adam 299 loss=0.234011 error=0.167917 grad(U)=0.000003 grad(V)=0.000045\n",
      "I 2016-12-18 00:10:17 downhill.base:232 Adam 300 loss=0.233711 error=0.167716 grad(U)=0.000003 grad(V)=0.000045\n",
      "I 2016-12-18 00:10:17 downhill.base:232 validation 30 loss=0.233411 error=0.167516 grad(U)=0.000003 grad(V)=0.000045 *\n",
      "I 2016-12-18 00:10:18 downhill.base:232 Adam 301 loss=0.233411 error=0.167516 grad(U)=0.000003 grad(V)=0.000045\n",
      "I 2016-12-18 00:10:18 downhill.base:232 Adam 302 loss=0.233112 error=0.167316 grad(U)=0.000003 grad(V)=0.000044\n",
      "I 2016-12-18 00:10:18 downhill.base:232 Adam 303 loss=0.232814 error=0.167117 grad(U)=0.000003 grad(V)=0.000044\n",
      "I 2016-12-18 00:10:18 downhill.base:232 Adam 304 loss=0.232517 error=0.166919 grad(U)=0.000003 grad(V)=0.000044\n",
      "I 2016-12-18 00:10:18 downhill.base:232 Adam 305 loss=0.232220 error=0.166721 grad(U)=0.000003 grad(V)=0.000044\n",
      "I 2016-12-18 00:10:19 downhill.base:232 Adam 306 loss=0.231925 error=0.166524 grad(U)=0.000003 grad(V)=0.000044\n",
      "I 2016-12-18 00:10:19 downhill.base:232 Adam 307 loss=0.231630 error=0.166328 grad(U)=0.000003 grad(V)=0.000044\n",
      "I 2016-12-18 00:10:19 downhill.base:232 Adam 308 loss=0.231336 error=0.166132 grad(U)=0.000003 grad(V)=0.000043\n",
      "I 2016-12-18 00:10:19 downhill.base:232 Adam 309 loss=0.231043 error=0.165937 grad(U)=0.000003 grad(V)=0.000043\n",
      "I 2016-12-18 00:10:19 downhill.base:232 Adam 310 loss=0.230750 error=0.165743 grad(U)=0.000003 grad(V)=0.000043\n",
      "I 2016-12-18 00:10:20 downhill.base:232 validation 31 loss=0.230458 error=0.165549 grad(U)=0.000003 grad(V)=0.000043 *\n",
      "I 2016-12-18 00:10:20 downhill.base:232 Adam 311 loss=0.230458 error=0.165549 grad(U)=0.000003 grad(V)=0.000043\n",
      "I 2016-12-18 00:10:20 downhill.base:232 Adam 312 loss=0.230167 error=0.165356 grad(U)=0.000003 grad(V)=0.000043\n",
      "I 2016-12-18 00:10:20 downhill.base:232 Adam 313 loss=0.229877 error=0.165163 grad(U)=0.000003 grad(V)=0.000043\n",
      "I 2016-12-18 00:10:20 downhill.base:232 Adam 314 loss=0.229588 error=0.164972 grad(U)=0.000003 grad(V)=0.000042\n",
      "I 2016-12-18 00:10:21 downhill.base:232 Adam 315 loss=0.229299 error=0.164780 grad(U)=0.000003 grad(V)=0.000042\n",
      "I 2016-12-18 00:10:21 downhill.base:232 Adam 316 loss=0.229011 error=0.164590 grad(U)=0.000003 grad(V)=0.000042\n",
      "I 2016-12-18 00:10:21 downhill.base:232 Adam 317 loss=0.228724 error=0.164400 grad(U)=0.000003 grad(V)=0.000042\n",
      "I 2016-12-18 00:10:21 downhill.base:232 Adam 318 loss=0.228438 error=0.164210 grad(U)=0.000003 grad(V)=0.000042\n",
      "I 2016-12-18 00:10:21 downhill.base:232 Adam 319 loss=0.228152 error=0.164022 grad(U)=0.000003 grad(V)=0.000042\n",
      "I 2016-12-18 00:10:22 downhill.base:232 Adam 320 loss=0.227867 error=0.163834 grad(U)=0.000003 grad(V)=0.000041\n",
      "I 2016-12-18 00:10:22 downhill.base:232 validation 32 loss=0.227583 error=0.163646 grad(U)=0.000003 grad(V)=0.000041 *\n",
      "I 2016-12-18 00:10:22 downhill.base:232 Adam 321 loss=0.227583 error=0.163646 grad(U)=0.000003 grad(V)=0.000041\n",
      "I 2016-12-18 00:10:22 downhill.base:232 Adam 322 loss=0.227300 error=0.163459 grad(U)=0.000003 grad(V)=0.000041\n",
      "I 2016-12-18 00:10:22 downhill.base:232 Adam 323 loss=0.227017 error=0.163273 grad(U)=0.000003 grad(V)=0.000041\n",
      "I 2016-12-18 00:10:23 downhill.base:232 Adam 324 loss=0.226736 error=0.163087 grad(U)=0.000003 grad(V)=0.000041\n",
      "I 2016-12-18 00:10:23 downhill.base:232 Adam 325 loss=0.226454 error=0.162902 grad(U)=0.000003 grad(V)=0.000041\n",
      "I 2016-12-18 00:10:23 downhill.base:232 Adam 326 loss=0.226174 error=0.162718 grad(U)=0.000003 grad(V)=0.000040\n",
      "I 2016-12-18 00:10:23 downhill.base:232 Adam 327 loss=0.225895 error=0.162534 grad(U)=0.000003 grad(V)=0.000040\n",
      "I 2016-12-18 00:10:23 downhill.base:232 Adam 328 loss=0.225616 error=0.162351 grad(U)=0.000003 grad(V)=0.000040\n",
      "I 2016-12-18 00:10:24 downhill.base:232 Adam 329 loss=0.225338 error=0.162168 grad(U)=0.000003 grad(V)=0.000040\n",
      "I 2016-12-18 00:10:24 downhill.base:232 Adam 330 loss=0.225060 error=0.161986 grad(U)=0.000003 grad(V)=0.000040\n",
      "I 2016-12-18 00:10:24 downhill.base:232 validation 33 loss=0.224783 error=0.161804 grad(U)=0.000003 grad(V)=0.000040 *\n",
      "I 2016-12-18 00:10:24 downhill.base:232 Adam 331 loss=0.224783 error=0.161804 grad(U)=0.000003 grad(V)=0.000040\n",
      "I 2016-12-18 00:10:24 downhill.base:232 Adam 332 loss=0.224508 error=0.161624 grad(U)=0.000003 grad(V)=0.000040\n",
      "I 2016-12-18 00:10:25 downhill.base:232 Adam 333 loss=0.224232 error=0.161443 grad(U)=0.000003 grad(V)=0.000039\n",
      "I 2016-12-18 00:10:25 downhill.base:232 Adam 334 loss=0.223958 error=0.161263 grad(U)=0.000003 grad(V)=0.000039\n",
      "I 2016-12-18 00:10:25 downhill.base:232 Adam 335 loss=0.223684 error=0.161084 grad(U)=0.000002 grad(V)=0.000039\n",
      "I 2016-12-18 00:10:25 downhill.base:232 Adam 336 loss=0.223411 error=0.160906 grad(U)=0.000002 grad(V)=0.000039\n",
      "I 2016-12-18 00:10:25 downhill.base:232 Adam 337 loss=0.223138 error=0.160728 grad(U)=0.000002 grad(V)=0.000039\n",
      "I 2016-12-18 00:10:26 downhill.base:232 Adam 338 loss=0.222867 error=0.160550 grad(U)=0.000002 grad(V)=0.000039\n",
      "I 2016-12-18 00:10:26 downhill.base:232 Adam 339 loss=0.222596 error=0.160373 grad(U)=0.000002 grad(V)=0.000038\n",
      "I 2016-12-18 00:10:26 downhill.base:232 Adam 340 loss=0.222325 error=0.160197 grad(U)=0.000002 grad(V)=0.000038\n",
      "I 2016-12-18 00:10:26 downhill.base:232 validation 34 loss=0.222056 error=0.160021 grad(U)=0.000002 grad(V)=0.000038 *\n",
      "I 2016-12-18 00:10:26 downhill.base:232 Adam 341 loss=0.222056 error=0.160021 grad(U)=0.000002 grad(V)=0.000038\n",
      "I 2016-12-18 00:10:27 downhill.base:232 Adam 342 loss=0.221787 error=0.159846 grad(U)=0.000002 grad(V)=0.000038\n",
      "I 2016-12-18 00:10:27 downhill.base:232 Adam 343 loss=0.221518 error=0.159672 grad(U)=0.000002 grad(V)=0.000038\n",
      "I 2016-12-18 00:10:27 downhill.base:232 Adam 344 loss=0.221251 error=0.159498 grad(U)=0.000002 grad(V)=0.000038\n",
      "I 2016-12-18 00:10:27 downhill.base:232 Adam 345 loss=0.220984 error=0.159324 grad(U)=0.000002 grad(V)=0.000038\n",
      "I 2016-12-18 00:10:27 downhill.base:232 Adam 346 loss=0.220718 error=0.159151 grad(U)=0.000002 grad(V)=0.000037\n",
      "I 2016-12-18 00:10:28 downhill.base:232 Adam 347 loss=0.220452 error=0.158979 grad(U)=0.000002 grad(V)=0.000037\n",
      "I 2016-12-18 00:10:28 downhill.base:232 Adam 348 loss=0.220187 error=0.158807 grad(U)=0.000002 grad(V)=0.000037\n",
      "I 2016-12-18 00:10:28 downhill.base:232 Adam 349 loss=0.219923 error=0.158635 grad(U)=0.000002 grad(V)=0.000037\n",
      "I 2016-12-18 00:10:28 downhill.base:232 Adam 350 loss=0.219660 error=0.158465 grad(U)=0.000002 grad(V)=0.000037\n",
      "I 2016-12-18 00:10:28 downhill.base:232 validation 35 loss=0.219397 error=0.158294 grad(U)=0.000002 grad(V)=0.000037 *\n",
      "I 2016-12-18 00:10:28 downhill.base:232 Adam 351 loss=0.219397 error=0.158294 grad(U)=0.000002 grad(V)=0.000037\n",
      "I 2016-12-18 00:10:29 downhill.base:232 Adam 352 loss=0.219135 error=0.158125 grad(U)=0.000002 grad(V)=0.000037\n",
      "I 2016-12-18 00:10:29 downhill.base:232 Adam 353 loss=0.218873 error=0.157956 grad(U)=0.000002 grad(V)=0.000036\n",
      "I 2016-12-18 00:10:29 downhill.base:232 Adam 354 loss=0.218613 error=0.157787 grad(U)=0.000002 grad(V)=0.000036\n",
      "I 2016-12-18 00:10:29 downhill.base:232 Adam 355 loss=0.218352 error=0.157619 grad(U)=0.000002 grad(V)=0.000036\n",
      "I 2016-12-18 00:10:29 downhill.base:232 Adam 356 loss=0.218093 error=0.157451 grad(U)=0.000002 grad(V)=0.000036\n",
      "I 2016-12-18 00:10:30 downhill.base:232 Adam 357 loss=0.217834 error=0.157284 grad(U)=0.000002 grad(V)=0.000036\n",
      "I 2016-12-18 00:10:30 downhill.base:232 Adam 358 loss=0.217576 error=0.157118 grad(U)=0.000002 grad(V)=0.000036\n",
      "I 2016-12-18 00:10:30 downhill.base:232 Adam 359 loss=0.217319 error=0.156952 grad(U)=0.000002 grad(V)=0.000036\n",
      "I 2016-12-18 00:10:30 downhill.base:232 Adam 360 loss=0.217062 error=0.156787 grad(U)=0.000002 grad(V)=0.000035\n",
      "I 2016-12-18 00:10:30 downhill.base:232 validation 36 loss=0.216806 error=0.156622 grad(U)=0.000002 grad(V)=0.000035 *\n",
      "I 2016-12-18 00:10:31 downhill.base:232 Adam 361 loss=0.216806 error=0.156622 grad(U)=0.000002 grad(V)=0.000035\n",
      "I 2016-12-18 00:10:31 downhill.base:232 Adam 362 loss=0.216550 error=0.156457 grad(U)=0.000002 grad(V)=0.000035\n",
      "I 2016-12-18 00:10:31 downhill.base:232 Adam 363 loss=0.216295 error=0.156294 grad(U)=0.000002 grad(V)=0.000035\n",
      "I 2016-12-18 00:10:31 downhill.base:232 Adam 364 loss=0.216041 error=0.156130 grad(U)=0.000002 grad(V)=0.000035\n",
      "I 2016-12-18 00:10:31 downhill.base:232 Adam 365 loss=0.215787 error=0.155967 grad(U)=0.000002 grad(V)=0.000035\n",
      "I 2016-12-18 00:10:32 downhill.base:232 Adam 366 loss=0.215534 error=0.155805 grad(U)=0.000002 grad(V)=0.000035\n",
      "I 2016-12-18 00:10:32 downhill.base:232 Adam 367 loss=0.215282 error=0.155643 grad(U)=0.000002 grad(V)=0.000035\n",
      "I 2016-12-18 00:10:32 downhill.base:232 Adam 368 loss=0.215030 error=0.155482 grad(U)=0.000002 grad(V)=0.000034\n",
      "I 2016-12-18 00:10:32 downhill.base:232 Adam 369 loss=0.214779 error=0.155321 grad(U)=0.000002 grad(V)=0.000034\n",
      "I 2016-12-18 00:10:32 downhill.base:232 Adam 370 loss=0.214529 error=0.155161 grad(U)=0.000002 grad(V)=0.000034\n",
      "I 2016-12-18 00:10:33 downhill.base:232 validation 37 loss=0.214279 error=0.155001 grad(U)=0.000002 grad(V)=0.000034 *\n",
      "I 2016-12-18 00:10:33 downhill.base:232 Adam 371 loss=0.214279 error=0.155001 grad(U)=0.000002 grad(V)=0.000034\n",
      "I 2016-12-18 00:10:33 downhill.base:232 Adam 372 loss=0.214030 error=0.154842 grad(U)=0.000002 grad(V)=0.000034\n",
      "I 2016-12-18 00:10:33 downhill.base:232 Adam 373 loss=0.213781 error=0.154683 grad(U)=0.000002 grad(V)=0.000034\n",
      "I 2016-12-18 00:10:33 downhill.base:232 Adam 374 loss=0.213533 error=0.154525 grad(U)=0.000002 grad(V)=0.000034\n",
      "I 2016-12-18 00:10:34 downhill.base:232 Adam 375 loss=0.213286 error=0.154367 grad(U)=0.000002 grad(V)=0.000033\n",
      "I 2016-12-18 00:10:34 downhill.base:232 Adam 376 loss=0.213039 error=0.154210 grad(U)=0.000002 grad(V)=0.000033\n",
      "I 2016-12-18 00:10:34 downhill.base:232 Adam 377 loss=0.212793 error=0.154053 grad(U)=0.000002 grad(V)=0.000033\n",
      "I 2016-12-18 00:10:34 downhill.base:232 Adam 378 loss=0.212548 error=0.153897 grad(U)=0.000002 grad(V)=0.000033\n",
      "I 2016-12-18 00:10:34 downhill.base:232 Adam 379 loss=0.212303 error=0.153741 grad(U)=0.000002 grad(V)=0.000033\n",
      "I 2016-12-18 00:10:35 downhill.base:232 Adam 380 loss=0.212058 error=0.153586 grad(U)=0.000002 grad(V)=0.000033\n",
      "I 2016-12-18 00:10:35 downhill.base:232 validation 38 loss=0.211815 error=0.153431 grad(U)=0.000002 grad(V)=0.000033 *\n",
      "I 2016-12-18 00:10:35 downhill.base:232 Adam 381 loss=0.211815 error=0.153431 grad(U)=0.000002 grad(V)=0.000033\n",
      "I 2016-12-18 00:10:35 downhill.base:232 Adam 382 loss=0.211572 error=0.153277 grad(U)=0.000002 grad(V)=0.000033\n",
      "I 2016-12-18 00:10:35 downhill.base:232 Adam 383 loss=0.211329 error=0.153123 grad(U)=0.000002 grad(V)=0.000032\n",
      "I 2016-12-18 00:10:36 downhill.base:232 Adam 384 loss=0.211087 error=0.152970 grad(U)=0.000002 grad(V)=0.000032\n",
      "I 2016-12-18 00:10:36 downhill.base:232 Adam 385 loss=0.210846 error=0.152817 grad(U)=0.000002 grad(V)=0.000032\n",
      "I 2016-12-18 00:10:36 downhill.base:232 Adam 386 loss=0.210606 error=0.152665 grad(U)=0.000002 grad(V)=0.000032\n",
      "I 2016-12-18 00:10:36 downhill.base:232 Adam 387 loss=0.210365 error=0.152513 grad(U)=0.000002 grad(V)=0.000032\n",
      "I 2016-12-18 00:10:36 downhill.base:232 Adam 388 loss=0.210126 error=0.152361 grad(U)=0.000002 grad(V)=0.000032\n",
      "I 2016-12-18 00:10:37 downhill.base:232 Adam 389 loss=0.209887 error=0.152210 grad(U)=0.000002 grad(V)=0.000032\n",
      "I 2016-12-18 00:10:37 downhill.base:232 Adam 390 loss=0.209649 error=0.152060 grad(U)=0.000002 grad(V)=0.000032\n",
      "I 2016-12-18 00:10:37 downhill.base:232 validation 39 loss=0.209411 error=0.151910 grad(U)=0.000002 grad(V)=0.000031 *\n",
      "I 2016-12-18 00:10:37 downhill.base:232 Adam 391 loss=0.209411 error=0.151910 grad(U)=0.000002 grad(V)=0.000031\n",
      "I 2016-12-18 00:10:37 downhill.base:232 Adam 392 loss=0.209174 error=0.151760 grad(U)=0.000002 grad(V)=0.000031\n",
      "I 2016-12-18 00:10:38 downhill.base:232 Adam 393 loss=0.208938 error=0.151611 grad(U)=0.000002 grad(V)=0.000031\n",
      "I 2016-12-18 00:10:38 downhill.base:232 Adam 394 loss=0.208702 error=0.151463 grad(U)=0.000002 grad(V)=0.000031\n",
      "I 2016-12-18 00:10:38 downhill.base:232 Adam 395 loss=0.208467 error=0.151315 grad(U)=0.000002 grad(V)=0.000031\n",
      "I 2016-12-18 00:10:38 downhill.base:232 Adam 396 loss=0.208232 error=0.151167 grad(U)=0.000002 grad(V)=0.000031\n",
      "I 2016-12-18 00:10:38 downhill.base:232 Adam 397 loss=0.207998 error=0.151020 grad(U)=0.000002 grad(V)=0.000031\n",
      "I 2016-12-18 00:10:39 downhill.base:232 Adam 398 loss=0.207764 error=0.150873 grad(U)=0.000002 grad(V)=0.000031\n",
      "I 2016-12-18 00:10:39 downhill.base:232 Adam 399 loss=0.207531 error=0.150727 grad(U)=0.000002 grad(V)=0.000031\n",
      "I 2016-12-18 00:10:39 downhill.base:232 Adam 400 loss=0.207298 error=0.150581 grad(U)=0.000002 grad(V)=0.000030\n",
      "I 2016-12-18 00:10:39 downhill.base:232 validation 40 loss=0.207066 error=0.150435 grad(U)=0.000002 grad(V)=0.000030 *\n",
      "I 2016-12-18 00:10:39 downhill.base:232 Adam 401 loss=0.207066 error=0.150435 grad(U)=0.000002 grad(V)=0.000030\n",
      "I 2016-12-18 00:10:40 downhill.base:232 Adam 402 loss=0.206835 error=0.150290 grad(U)=0.000002 grad(V)=0.000030\n",
      "I 2016-12-18 00:10:40 downhill.base:232 Adam 403 loss=0.206604 error=0.150146 grad(U)=0.000002 grad(V)=0.000030\n",
      "I 2016-12-18 00:10:40 downhill.base:232 Adam 404 loss=0.206374 error=0.150002 grad(U)=0.000002 grad(V)=0.000030\n",
      "I 2016-12-18 00:10:40 downhill.base:232 Adam 405 loss=0.206144 error=0.149858 grad(U)=0.000002 grad(V)=0.000030\n",
      "I 2016-12-18 00:10:40 downhill.base:232 Adam 406 loss=0.205915 error=0.149715 grad(U)=0.000002 grad(V)=0.000030\n",
      "I 2016-12-18 00:10:41 downhill.base:232 Adam 407 loss=0.205687 error=0.149572 grad(U)=0.000002 grad(V)=0.000030\n",
      "I 2016-12-18 00:10:41 downhill.base:232 Adam 408 loss=0.205459 error=0.149430 grad(U)=0.000002 grad(V)=0.000029\n",
      "I 2016-12-18 00:10:41 downhill.base:232 Adam 409 loss=0.205231 error=0.149288 grad(U)=0.000002 grad(V)=0.000029\n",
      "I 2016-12-18 00:10:41 downhill.base:232 Adam 410 loss=0.205004 error=0.149147 grad(U)=0.000002 grad(V)=0.000029\n",
      "I 2016-12-18 00:10:41 downhill.base:232 validation 41 loss=0.204778 error=0.149006 grad(U)=0.000002 grad(V)=0.000029 *\n",
      "I 2016-12-18 00:10:41 downhill.base:232 Adam 411 loss=0.204778 error=0.149006 grad(U)=0.000002 grad(V)=0.000029\n",
      "I 2016-12-18 00:10:42 downhill.base:232 Adam 412 loss=0.204552 error=0.148866 grad(U)=0.000002 grad(V)=0.000029\n",
      "I 2016-12-18 00:10:42 downhill.base:232 Adam 413 loss=0.204327 error=0.148726 grad(U)=0.000002 grad(V)=0.000029\n",
      "I 2016-12-18 00:10:42 downhill.base:232 Adam 414 loss=0.204102 error=0.148586 grad(U)=0.000002 grad(V)=0.000029\n",
      "I 2016-12-18 00:10:42 downhill.base:232 Adam 415 loss=0.203878 error=0.148447 grad(U)=0.000002 grad(V)=0.000029\n",
      "I 2016-12-18 00:10:42 downhill.base:232 Adam 416 loss=0.203654 error=0.148308 grad(U)=0.000002 grad(V)=0.000029\n",
      "I 2016-12-18 00:10:43 downhill.base:232 Adam 417 loss=0.203431 error=0.148169 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:43 downhill.base:232 Adam 418 loss=0.203209 error=0.148032 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:43 downhill.base:232 Adam 419 loss=0.202987 error=0.147894 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:43 downhill.base:232 Adam 420 loss=0.202765 error=0.147757 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:43 downhill.base:232 validation 42 loss=0.202544 error=0.147620 grad(U)=0.000002 grad(V)=0.000028 *\n",
      "I 2016-12-18 00:10:44 downhill.base:232 Adam 421 loss=0.202544 error=0.147620 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:44 downhill.base:232 Adam 422 loss=0.202324 error=0.147484 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:44 downhill.base:232 Adam 423 loss=0.202104 error=0.147348 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:44 downhill.base:232 Adam 424 loss=0.201885 error=0.147213 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:44 downhill.base:232 Adam 425 loss=0.201666 error=0.147078 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:45 downhill.base:232 Adam 426 loss=0.201447 error=0.146943 grad(U)=0.000002 grad(V)=0.000028\n",
      "I 2016-12-18 00:10:45 downhill.base:232 Adam 427 loss=0.201230 error=0.146809 grad(U)=0.000002 grad(V)=0.000027\n",
      "I 2016-12-18 00:10:45 downhill.base:232 Adam 428 loss=0.201012 error=0.146676 grad(U)=0.000002 grad(V)=0.000027\n",
      "I 2016-12-18 00:10:45 downhill.base:232 Adam 429 loss=0.200795 error=0.146542 grad(U)=0.000002 grad(V)=0.000027\n",
      "I 2016-12-18 00:10:45 downhill.base:232 Adam 430 loss=0.200579 error=0.146409 grad(U)=0.000002 grad(V)=0.000027\n",
      "I 2016-12-18 00:10:46 downhill.base:232 validation 43 loss=0.200363 error=0.146277 grad(U)=0.000002 grad(V)=0.000027 *\n",
      "I 2016-12-18 00:10:46 downhill.base:232 Adam 431 loss=0.200363 error=0.146277 grad(U)=0.000002 grad(V)=0.000027\n",
      "I 2016-12-18 00:10:46 downhill.base:232 Adam 432 loss=0.200148 error=0.146145 grad(U)=0.000002 grad(V)=0.000027\n",
      "I 2016-12-18 00:10:46 downhill.base:232 Adam 433 loss=0.199933 error=0.146013 grad(U)=0.000002 grad(V)=0.000027\n",
      "I 2016-12-18 00:10:46 downhill.base:232 Adam 434 loss=0.199719 error=0.145882 grad(U)=0.000002 grad(V)=0.000027\n",
      "I 2016-12-18 00:10:47 downhill.base:232 Adam 435 loss=0.199505 error=0.145751 grad(U)=0.000002 grad(V)=0.000027\n",
      "I 2016-12-18 00:10:47 downhill.base:232 Adam 436 loss=0.199292 error=0.145620 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:47 downhill.base:232 Adam 437 loss=0.199079 error=0.145490 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:47 downhill.base:232 Adam 438 loss=0.198867 error=0.145361 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:47 downhill.base:232 Adam 439 loss=0.198655 error=0.145231 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:48 downhill.base:232 Adam 440 loss=0.198444 error=0.145103 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:48 downhill.base:232 validation 44 loss=0.198234 error=0.144974 grad(U)=0.000002 grad(V)=0.000026 *\n",
      "I 2016-12-18 00:10:48 downhill.base:232 Adam 441 loss=0.198234 error=0.144974 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:48 downhill.base:232 Adam 442 loss=0.198023 error=0.144846 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:49 downhill.base:232 Adam 443 loss=0.197814 error=0.144718 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:49 downhill.base:232 Adam 444 loss=0.197604 error=0.144591 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:49 downhill.base:232 Adam 445 loss=0.197396 error=0.144464 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:49 downhill.base:232 Adam 446 loss=0.197187 error=0.144338 grad(U)=0.000002 grad(V)=0.000026\n",
      "I 2016-12-18 00:10:50 downhill.base:232 Adam 447 loss=0.196980 error=0.144212 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:50 downhill.base:232 Adam 448 loss=0.196772 error=0.144086 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:50 downhill.base:232 Adam 449 loss=0.196565 error=0.143960 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:50 downhill.base:232 Adam 450 loss=0.196359 error=0.143835 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:50 downhill.base:232 validation 45 loss=0.196153 error=0.143711 grad(U)=0.000002 grad(V)=0.000025 *\n",
      "I 2016-12-18 00:10:51 downhill.base:232 Adam 451 loss=0.196153 error=0.143711 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:51 downhill.base:232 Adam 452 loss=0.195948 error=0.143587 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:51 downhill.base:232 Adam 453 loss=0.195743 error=0.143463 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:51 downhill.base:232 Adam 454 loss=0.195539 error=0.143339 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:51 downhill.base:232 Adam 455 loss=0.195335 error=0.143216 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:52 downhill.base:232 Adam 456 loss=0.195132 error=0.143094 grad(U)=0.000002 grad(V)=0.000025\n",
      "I 2016-12-18 00:10:52 downhill.base:232 Adam 457 loss=0.194929 error=0.142971 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:52 downhill.base:232 Adam 458 loss=0.194726 error=0.142849 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:52 downhill.base:232 Adam 459 loss=0.194524 error=0.142728 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:52 downhill.base:232 Adam 460 loss=0.194323 error=0.142606 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:53 downhill.base:232 validation 46 loss=0.194122 error=0.142486 grad(U)=0.000002 grad(V)=0.000024 *\n",
      "I 2016-12-18 00:10:53 downhill.base:232 Adam 461 loss=0.194122 error=0.142486 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:53 downhill.base:232 Adam 462 loss=0.193921 error=0.142365 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:53 downhill.base:232 Adam 463 loss=0.193721 error=0.142245 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:53 downhill.base:232 Adam 464 loss=0.193522 error=0.142125 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:54 downhill.base:232 Adam 465 loss=0.193323 error=0.142006 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:54 downhill.base:232 Adam 466 loss=0.193124 error=0.141887 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:54 downhill.base:232 Adam 467 loss=0.192926 error=0.141769 grad(U)=0.000002 grad(V)=0.000024\n",
      "I 2016-12-18 00:10:54 downhill.base:232 Adam 468 loss=0.192728 error=0.141650 grad(U)=0.000002 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:54 downhill.base:232 Adam 469 loss=0.192531 error=0.141532 grad(U)=0.000002 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:55 downhill.base:232 Adam 470 loss=0.192334 error=0.141415 grad(U)=0.000001 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:55 downhill.base:232 validation 47 loss=0.192137 error=0.141298 grad(U)=0.000001 grad(V)=0.000023 *\n",
      "I 2016-12-18 00:10:55 downhill.base:232 Adam 471 loss=0.192137 error=0.141298 grad(U)=0.000001 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:55 downhill.base:232 Adam 472 loss=0.191942 error=0.141181 grad(U)=0.000001 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:55 downhill.base:232 Adam 473 loss=0.191746 error=0.141064 grad(U)=0.000001 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:55 downhill.base:232 Adam 474 loss=0.191551 error=0.140948 grad(U)=0.000001 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:56 downhill.base:232 Adam 475 loss=0.191356 error=0.140833 grad(U)=0.000001 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:56 downhill.base:232 Adam 476 loss=0.191162 error=0.140717 grad(U)=0.000001 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:56 downhill.base:232 Adam 477 loss=0.190969 error=0.140602 grad(U)=0.000001 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:56 downhill.base:232 Adam 478 loss=0.190776 error=0.140488 grad(U)=0.000001 grad(V)=0.000023\n",
      "I 2016-12-18 00:10:56 downhill.base:232 Adam 479 loss=0.190583 error=0.140373 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:57 downhill.base:232 Adam 480 loss=0.190391 error=0.140259 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:57 downhill.base:232 validation 48 loss=0.190199 error=0.140146 grad(U)=0.000001 grad(V)=0.000022 *\n",
      "I 2016-12-18 00:10:57 downhill.base:232 Adam 481 loss=0.190199 error=0.140146 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:57 downhill.base:232 Adam 482 loss=0.190007 error=0.140032 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:57 downhill.base:232 Adam 483 loss=0.189817 error=0.139919 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:58 downhill.base:232 Adam 484 loss=0.189626 error=0.139807 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:58 downhill.base:232 Adam 485 loss=0.189436 error=0.139695 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:58 downhill.base:232 Adam 486 loss=0.189247 error=0.139583 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:58 downhill.base:232 Adam 487 loss=0.189057 error=0.139471 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:58 downhill.base:232 Adam 488 loss=0.188869 error=0.139360 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:59 downhill.base:232 Adam 489 loss=0.188680 error=0.139249 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:59 downhill.base:232 Adam 490 loss=0.188492 error=0.139139 grad(U)=0.000001 grad(V)=0.000022\n",
      "I 2016-12-18 00:10:59 downhill.base:232 validation 49 loss=0.188305 error=0.139028 grad(U)=0.000001 grad(V)=0.000021 *\n",
      "I 2016-12-18 00:10:59 downhill.base:232 Adam 491 loss=0.188305 error=0.139028 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:10:59 downhill.base:232 Adam 492 loss=0.188118 error=0.138919 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:00 downhill.base:232 Adam 493 loss=0.187931 error=0.138809 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:00 downhill.base:232 Adam 494 loss=0.187745 error=0.138700 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:00 downhill.base:232 Adam 495 loss=0.187560 error=0.138591 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:00 downhill.base:232 Adam 496 loss=0.187374 error=0.138483 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:00 downhill.base:232 Adam 497 loss=0.187189 error=0.138375 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:01 downhill.base:232 Adam 498 loss=0.187005 error=0.138267 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:01 downhill.base:232 Adam 499 loss=0.186821 error=0.138159 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:01 downhill.base:232 Adam 500 loss=0.186637 error=0.138052 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:01 downhill.base:232 validation 50 loss=0.186454 error=0.137945 grad(U)=0.000001 grad(V)=0.000021 *\n",
      "I 2016-12-18 00:11:01 downhill.base:232 Adam 501 loss=0.186454 error=0.137945 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:02 downhill.base:232 Adam 502 loss=0.186272 error=0.137839 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:02 downhill.base:232 Adam 503 loss=0.186089 error=0.137733 grad(U)=0.000001 grad(V)=0.000021\n",
      "I 2016-12-18 00:11:02 downhill.base:232 Adam 504 loss=0.185907 error=0.137627 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:02 downhill.base:232 Adam 505 loss=0.185726 error=0.137521 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:02 downhill.base:232 Adam 506 loss=0.185545 error=0.137416 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:03 downhill.base:232 Adam 507 loss=0.185364 error=0.137311 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:03 downhill.base:232 Adam 508 loss=0.185184 error=0.137206 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:03 downhill.base:232 Adam 509 loss=0.185004 error=0.137102 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:03 downhill.base:232 Adam 510 loss=0.184825 error=0.136998 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:03 downhill.base:232 validation 51 loss=0.184646 error=0.136895 grad(U)=0.000001 grad(V)=0.000020 *\n",
      "I 2016-12-18 00:11:04 downhill.base:232 Adam 511 loss=0.184646 error=0.136895 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:04 downhill.base:232 Adam 512 loss=0.184467 error=0.136791 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:04 downhill.base:232 Adam 513 loss=0.184289 error=0.136688 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:04 downhill.base:232 Adam 514 loss=0.184112 error=0.136586 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:04 downhill.base:232 Adam 515 loss=0.183934 error=0.136483 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:05 downhill.base:232 Adam 516 loss=0.183757 error=0.136381 grad(U)=0.000001 grad(V)=0.000020\n",
      "I 2016-12-18 00:11:05 downhill.base:232 Adam 517 loss=0.183581 error=0.136280 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:05 downhill.base:232 Adam 518 loss=0.183405 error=0.136178 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:05 downhill.base:232 Adam 519 loss=0.183229 error=0.136077 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:05 downhill.base:232 Adam 520 loss=0.183054 error=0.135977 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:05 downhill.base:232 validation 52 loss=0.182879 error=0.135876 grad(U)=0.000001 grad(V)=0.000019 *\n",
      "I 2016-12-18 00:11:06 downhill.base:232 Adam 521 loss=0.182879 error=0.135876 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:06 downhill.base:232 Adam 522 loss=0.182704 error=0.135776 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:06 downhill.base:232 Adam 523 loss=0.182530 error=0.135676 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:06 downhill.base:232 Adam 524 loss=0.182357 error=0.135577 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:06 downhill.base:232 Adam 525 loss=0.182184 error=0.135477 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:07 downhill.base:232 Adam 526 loss=0.182011 error=0.135378 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:07 downhill.base:232 Adam 527 loss=0.181838 error=0.135280 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:07 downhill.base:232 Adam 528 loss=0.181666 error=0.135181 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:07 downhill.base:232 Adam 529 loss=0.181494 error=0.135083 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:08 downhill.base:232 Adam 530 loss=0.181323 error=0.134986 grad(U)=0.000001 grad(V)=0.000019\n",
      "I 2016-12-18 00:11:08 downhill.base:232 validation 53 loss=0.181152 error=0.134888 grad(U)=0.000001 grad(V)=0.000018 *\n",
      "I 2016-12-18 00:11:08 downhill.base:232 Adam 531 loss=0.181152 error=0.134888 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:08 downhill.base:232 Adam 532 loss=0.180982 error=0.134791 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:08 downhill.base:232 Adam 533 loss=0.180812 error=0.134694 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:09 downhill.base:232 Adam 534 loss=0.180642 error=0.134598 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:09 downhill.base:232 Adam 535 loss=0.180473 error=0.134502 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:09 downhill.base:232 Adam 536 loss=0.180304 error=0.134406 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:09 downhill.base:232 Adam 537 loss=0.180135 error=0.134310 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:10 downhill.base:232 Adam 538 loss=0.179967 error=0.134215 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:10 downhill.base:232 Adam 539 loss=0.179799 error=0.134120 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:10 downhill.base:232 Adam 540 loss=0.179632 error=0.134025 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:10 downhill.base:232 validation 54 loss=0.179465 error=0.133931 grad(U)=0.000001 grad(V)=0.000018 *\n",
      "I 2016-12-18 00:11:11 downhill.base:232 Adam 541 loss=0.179465 error=0.133931 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:11 downhill.base:232 Adam 542 loss=0.179299 error=0.133836 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:11 downhill.base:232 Adam 543 loss=0.179132 error=0.133743 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:11 downhill.base:232 Adam 544 loss=0.178966 error=0.133649 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:11 downhill.base:232 Adam 545 loss=0.178801 error=0.133556 grad(U)=0.000001 grad(V)=0.000018\n",
      "I 2016-12-18 00:11:12 downhill.base:232 Adam 546 loss=0.178636 error=0.133463 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:12 downhill.base:232 Adam 547 loss=0.178471 error=0.133370 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:12 downhill.base:232 Adam 548 loss=0.178307 error=0.133277 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:12 downhill.base:232 Adam 549 loss=0.178143 error=0.133185 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:12 downhill.base:232 Adam 550 loss=0.177979 error=0.133094 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:13 downhill.base:232 validation 55 loss=0.177816 error=0.133002 grad(U)=0.000001 grad(V)=0.000017 *\n",
      "I 2016-12-18 00:11:13 downhill.base:232 Adam 551 loss=0.177816 error=0.133002 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:13 downhill.base:232 Adam 552 loss=0.177653 error=0.132911 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:13 downhill.base:232 Adam 553 loss=0.177491 error=0.132820 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:13 downhill.base:232 Adam 554 loss=0.177329 error=0.132729 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:14 downhill.base:232 Adam 555 loss=0.177167 error=0.132638 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:14 downhill.base:232 Adam 556 loss=0.177005 error=0.132548 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:14 downhill.base:232 Adam 557 loss=0.176844 error=0.132458 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:14 downhill.base:232 Adam 558 loss=0.176684 error=0.132369 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:14 downhill.base:232 Adam 559 loss=0.176524 error=0.132279 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:15 downhill.base:232 Adam 560 loss=0.176364 error=0.132190 grad(U)=0.000001 grad(V)=0.000017\n",
      "I 2016-12-18 00:11:15 downhill.base:232 validation 56 loss=0.176204 error=0.132102 grad(U)=0.000001 grad(V)=0.000016 *\n",
      "I 2016-12-18 00:11:15 downhill.base:232 Adam 561 loss=0.176204 error=0.132102 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:15 downhill.base:232 Adam 562 loss=0.176045 error=0.132013 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:15 downhill.base:232 Adam 563 loss=0.175886 error=0.131925 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:16 downhill.base:232 Adam 564 loss=0.175727 error=0.131837 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:16 downhill.base:232 Adam 565 loss=0.175569 error=0.131749 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:16 downhill.base:232 Adam 566 loss=0.175411 error=0.131662 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:16 downhill.base:232 Adam 567 loss=0.175254 error=0.131575 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:16 downhill.base:232 Adam 568 loss=0.175097 error=0.131488 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:17 downhill.base:232 Adam 569 loss=0.174940 error=0.131401 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:17 downhill.base:232 Adam 570 loss=0.174784 error=0.131315 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:17 downhill.base:232 validation 57 loss=0.174628 error=0.131229 grad(U)=0.000001 grad(V)=0.000016 *\n",
      "I 2016-12-18 00:11:17 downhill.base:232 Adam 571 loss=0.174628 error=0.131229 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:17 downhill.base:232 Adam 572 loss=0.174472 error=0.131143 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:18 downhill.base:232 Adam 573 loss=0.174317 error=0.131057 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:18 downhill.base:232 Adam 574 loss=0.174162 error=0.130972 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:18 downhill.base:232 Adam 575 loss=0.174007 error=0.130887 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:18 downhill.base:232 Adam 576 loss=0.173853 error=0.130802 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:18 downhill.base:232 Adam 577 loss=0.173699 error=0.130718 grad(U)=0.000001 grad(V)=0.000016\n",
      "I 2016-12-18 00:11:19 downhill.base:232 Adam 578 loss=0.173546 error=0.130633 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:19 downhill.base:232 Adam 579 loss=0.173393 error=0.130549 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:19 downhill.base:232 Adam 580 loss=0.173240 error=0.130466 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:19 downhill.base:232 validation 58 loss=0.173087 error=0.130382 grad(U)=0.000001 grad(V)=0.000015 *\n",
      "I 2016-12-18 00:11:19 downhill.base:232 Adam 581 loss=0.173087 error=0.130382 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:20 downhill.base:232 Adam 582 loss=0.172935 error=0.130299 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:20 downhill.base:232 Adam 583 loss=0.172784 error=0.130216 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:20 downhill.base:232 Adam 584 loss=0.172632 error=0.130133 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:20 downhill.base:232 Adam 585 loss=0.172481 error=0.130051 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:20 downhill.base:232 Adam 586 loss=0.172330 error=0.129969 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:21 downhill.base:232 Adam 587 loss=0.172180 error=0.129887 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:21 downhill.base:232 Adam 588 loss=0.172030 error=0.129805 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:21 downhill.base:232 Adam 589 loss=0.171880 error=0.129724 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:21 downhill.base:232 Adam 590 loss=0.171731 error=0.129643 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:21 downhill.base:232 validation 59 loss=0.171581 error=0.129562 grad(U)=0.000001 grad(V)=0.000015 *\n",
      "I 2016-12-18 00:11:22 downhill.base:232 Adam 591 loss=0.171581 error=0.129562 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:22 downhill.base:232 Adam 592 loss=0.171433 error=0.129481 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:22 downhill.base:232 Adam 593 loss=0.171284 error=0.129400 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:22 downhill.base:232 Adam 594 loss=0.171136 error=0.129320 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:22 downhill.base:232 Adam 595 loss=0.170989 error=0.129240 grad(U)=0.000001 grad(V)=0.000015\n",
      "I 2016-12-18 00:11:23 downhill.base:232 Adam 596 loss=0.170841 error=0.129161 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:23 downhill.base:232 Adam 597 loss=0.170694 error=0.129081 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:23 downhill.base:232 Adam 598 loss=0.170547 error=0.129002 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:23 downhill.base:232 Adam 599 loss=0.170401 error=0.128923 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:23 downhill.base:232 Adam 600 loss=0.170255 error=0.128844 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:24 downhill.base:232 validation 60 loss=0.170109 error=0.128766 grad(U)=0.000001 grad(V)=0.000014 *\n",
      "I 2016-12-18 00:11:24 downhill.base:232 Adam 601 loss=0.170109 error=0.128766 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:24 downhill.base:232 Adam 602 loss=0.169964 error=0.128688 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:24 downhill.base:232 Adam 603 loss=0.169819 error=0.128610 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:24 downhill.base:232 Adam 604 loss=0.169674 error=0.128532 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:25 downhill.base:232 Adam 605 loss=0.169529 error=0.128455 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:25 downhill.base:232 Adam 606 loss=0.169385 error=0.128377 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:25 downhill.base:232 Adam 607 loss=0.169242 error=0.128300 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:25 downhill.base:232 Adam 608 loss=0.169098 error=0.128224 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:25 downhill.base:232 Adam 609 loss=0.168955 error=0.128147 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:26 downhill.base:232 Adam 610 loss=0.168812 error=0.128071 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:26 downhill.base:232 validation 61 loss=0.168670 error=0.127995 grad(U)=0.000001 grad(V)=0.000014 *\n",
      "I 2016-12-18 00:11:26 downhill.base:232 Adam 611 loss=0.168670 error=0.127995 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:26 downhill.base:232 Adam 612 loss=0.168528 error=0.127919 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:26 downhill.base:232 Adam 613 loss=0.168386 error=0.127843 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:26 downhill.base:232 Adam 614 loss=0.168245 error=0.127768 grad(U)=0.000001 grad(V)=0.000014\n",
      "I 2016-12-18 00:11:27 downhill.base:232 Adam 615 loss=0.168103 error=0.127693 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:27 downhill.base:232 Adam 616 loss=0.167963 error=0.127618 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:27 downhill.base:232 Adam 617 loss=0.167822 error=0.127544 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:27 downhill.base:232 Adam 618 loss=0.167682 error=0.127469 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:28 downhill.base:232 Adam 619 loss=0.167542 error=0.127395 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:28 downhill.base:232 Adam 620 loss=0.167402 error=0.127321 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:28 downhill.base:232 validation 62 loss=0.167263 error=0.127247 grad(U)=0.000001 grad(V)=0.000013 *\n",
      "I 2016-12-18 00:11:28 downhill.base:232 Adam 621 loss=0.167263 error=0.127247 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:28 downhill.base:232 Adam 622 loss=0.167124 error=0.127174 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:29 downhill.base:232 Adam 623 loss=0.166986 error=0.127100 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:29 downhill.base:232 Adam 624 loss=0.166848 error=0.127027 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:29 downhill.base:232 Adam 625 loss=0.166710 error=0.126955 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:29 downhill.base:232 Adam 626 loss=0.166572 error=0.126882 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:30 downhill.base:232 Adam 627 loss=0.166435 error=0.126810 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:30 downhill.base:232 Adam 628 loss=0.166298 error=0.126738 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:30 downhill.base:232 Adam 629 loss=0.166161 error=0.126666 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:30 downhill.base:232 Adam 630 loss=0.166024 error=0.126594 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:30 downhill.base:232 validation 63 loss=0.165888 error=0.126523 grad(U)=0.000001 grad(V)=0.000013 *\n",
      "I 2016-12-18 00:11:31 downhill.base:232 Adam 631 loss=0.165888 error=0.126523 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:31 downhill.base:232 Adam 632 loss=0.165753 error=0.126452 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:31 downhill.base:232 Adam 633 loss=0.165617 error=0.126381 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:31 downhill.base:232 Adam 634 loss=0.165482 error=0.126310 grad(U)=0.000001 grad(V)=0.000013\n",
      "I 2016-12-18 00:11:32 downhill.base:232 Adam 635 loss=0.165347 error=0.126239 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:32 downhill.base:232 Adam 636 loss=0.165212 error=0.126169 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:32 downhill.base:232 Adam 637 loss=0.165078 error=0.126099 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:32 downhill.base:232 Adam 638 loss=0.164944 error=0.126029 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:33 downhill.base:232 Adam 639 loss=0.164810 error=0.125959 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:33 downhill.base:232 Adam 640 loss=0.164677 error=0.125890 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:33 downhill.base:232 validation 64 loss=0.164544 error=0.125821 grad(U)=0.000001 grad(V)=0.000012 *\n",
      "I 2016-12-18 00:11:33 downhill.base:232 Adam 641 loss=0.164544 error=0.125821 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:33 downhill.base:232 Adam 642 loss=0.164411 error=0.125752 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:34 downhill.base:232 Adam 643 loss=0.164279 error=0.125683 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:34 downhill.base:232 Adam 644 loss=0.164146 error=0.125614 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:34 downhill.base:232 Adam 645 loss=0.164015 error=0.125546 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:34 downhill.base:232 Adam 646 loss=0.163883 error=0.125478 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:34 downhill.base:232 Adam 647 loss=0.163752 error=0.125410 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:35 downhill.base:232 Adam 648 loss=0.163621 error=0.125342 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:35 downhill.base:232 Adam 649 loss=0.163490 error=0.125275 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:35 downhill.base:232 Adam 650 loss=0.163360 error=0.125207 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:35 downhill.base:232 validation 65 loss=0.163230 error=0.125140 grad(U)=0.000001 grad(V)=0.000012 *\n",
      "I 2016-12-18 00:11:35 downhill.base:232 Adam 651 loss=0.163230 error=0.125140 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:35 downhill.base:232 Adam 652 loss=0.163100 error=0.125073 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:36 downhill.base:232 Adam 653 loss=0.162970 error=0.125007 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:36 downhill.base:232 Adam 654 loss=0.162841 error=0.124940 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:36 downhill.base:232 Adam 655 loss=0.162712 error=0.124874 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:36 downhill.base:232 Adam 656 loss=0.162584 error=0.124808 grad(U)=0.000001 grad(V)=0.000012\n",
      "I 2016-12-18 00:11:36 downhill.base:232 Adam 657 loss=0.162455 error=0.124742 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:37 downhill.base:232 Adam 658 loss=0.162327 error=0.124677 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:37 downhill.base:232 Adam 659 loss=0.162200 error=0.124611 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:37 downhill.base:232 Adam 660 loss=0.162072 error=0.124546 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:37 downhill.base:232 validation 66 loss=0.161945 error=0.124481 grad(U)=0.000001 grad(V)=0.000011 *\n",
      "I 2016-12-18 00:11:37 downhill.base:232 Adam 661 loss=0.161945 error=0.124481 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:38 downhill.base:232 Adam 662 loss=0.161818 error=0.124416 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:38 downhill.base:232 Adam 663 loss=0.161691 error=0.124352 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:38 downhill.base:232 Adam 664 loss=0.161565 error=0.124287 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:38 downhill.base:232 Adam 665 loss=0.161439 error=0.124223 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:38 downhill.base:232 Adam 666 loss=0.161313 error=0.124159 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:39 downhill.base:232 Adam 667 loss=0.161188 error=0.124095 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:39 downhill.base:232 Adam 668 loss=0.161063 error=0.124032 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:39 downhill.base:232 Adam 669 loss=0.160938 error=0.123968 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:39 downhill.base:232 Adam 670 loss=0.160813 error=0.123905 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:39 downhill.base:232 validation 67 loss=0.160689 error=0.123842 grad(U)=0.000001 grad(V)=0.000011 *\n",
      "I 2016-12-18 00:11:40 downhill.base:232 Adam 671 loss=0.160689 error=0.123842 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:40 downhill.base:232 Adam 672 loss=0.160565 error=0.123780 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:40 downhill.base:232 Adam 673 loss=0.160441 error=0.123717 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:40 downhill.base:232 Adam 674 loss=0.160317 error=0.123655 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:40 downhill.base:232 Adam 675 loss=0.160194 error=0.123592 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:41 downhill.base:232 Adam 676 loss=0.160071 error=0.123530 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:41 downhill.base:232 Adam 677 loss=0.159948 error=0.123469 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:41 downhill.base:232 Adam 678 loss=0.159826 error=0.123407 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:41 downhill.base:232 Adam 679 loss=0.159703 error=0.123346 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:41 downhill.base:232 Adam 680 loss=0.159582 error=0.123284 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:42 downhill.base:232 validation 68 loss=0.159460 error=0.123224 grad(U)=0.000001 grad(V)=0.000011 *\n",
      "I 2016-12-18 00:11:42 downhill.base:232 Adam 681 loss=0.159460 error=0.123224 grad(U)=0.000001 grad(V)=0.000011\n",
      "I 2016-12-18 00:11:42 downhill.base:232 Adam 682 loss=0.159339 error=0.123163 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:42 downhill.base:232 Adam 683 loss=0.159217 error=0.123102 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:42 downhill.base:232 Adam 684 loss=0.159097 error=0.123042 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:43 downhill.base:232 Adam 685 loss=0.158976 error=0.122982 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:43 downhill.base:232 Adam 686 loss=0.158856 error=0.122922 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:43 downhill.base:232 Adam 687 loss=0.158736 error=0.122862 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:43 downhill.base:232 Adam 688 loss=0.158616 error=0.122802 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:43 downhill.base:232 Adam 689 loss=0.158496 error=0.122743 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:44 downhill.base:232 Adam 690 loss=0.158377 error=0.122683 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:44 downhill.base:232 validation 69 loss=0.158258 error=0.122624 grad(U)=0.000001 grad(V)=0.000010 *\n",
      "I 2016-12-18 00:11:44 downhill.base:232 Adam 691 loss=0.158258 error=0.122624 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:44 downhill.base:232 Adam 692 loss=0.158140 error=0.122565 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:44 downhill.base:232 Adam 693 loss=0.158021 error=0.122507 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:45 downhill.base:232 Adam 694 loss=0.157903 error=0.122448 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:45 downhill.base:232 Adam 695 loss=0.157785 error=0.122390 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:45 downhill.base:232 Adam 696 loss=0.157668 error=0.122332 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:45 downhill.base:232 Adam 697 loss=0.157550 error=0.122274 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:45 downhill.base:232 Adam 698 loss=0.157433 error=0.122216 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:46 downhill.base:232 Adam 699 loss=0.157316 error=0.122159 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:46 downhill.base:232 Adam 700 loss=0.157200 error=0.122101 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:46 downhill.base:232 validation 70 loss=0.157084 error=0.122044 grad(U)=0.000001 grad(V)=0.000010 *\n",
      "I 2016-12-18 00:11:46 downhill.base:232 Adam 701 loss=0.157084 error=0.122044 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:46 downhill.base:232 Adam 702 loss=0.156967 error=0.121987 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:46 downhill.base:232 Adam 703 loss=0.156852 error=0.121930 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:47 downhill.base:232 Adam 704 loss=0.156736 error=0.121873 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:47 downhill.base:232 Adam 705 loss=0.156621 error=0.121817 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:47 downhill.base:232 Adam 706 loss=0.156506 error=0.121761 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:47 downhill.base:232 Adam 707 loss=0.156391 error=0.121705 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:47 downhill.base:232 Adam 708 loss=0.156277 error=0.121649 grad(U)=0.000001 grad(V)=0.000010\n",
      "I 2016-12-18 00:11:48 downhill.base:232 Adam 709 loss=0.156163 error=0.121593 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:48 downhill.base:232 Adam 710 loss=0.156049 error=0.121537 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:48 downhill.base:232 validation 71 loss=0.155935 error=0.121482 grad(U)=0.000001 grad(V)=0.000009 *\n",
      "I 2016-12-18 00:11:48 downhill.base:232 Adam 711 loss=0.155935 error=0.121482 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:48 downhill.base:232 Adam 712 loss=0.155821 error=0.121427 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:49 downhill.base:232 Adam 713 loss=0.155708 error=0.121372 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:49 downhill.base:232 Adam 714 loss=0.155595 error=0.121317 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:49 downhill.base:232 Adam 715 loss=0.155482 error=0.121262 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:49 downhill.base:232 Adam 716 loss=0.155370 error=0.121208 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:49 downhill.base:232 Adam 717 loss=0.155257 error=0.121153 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:50 downhill.base:232 Adam 718 loss=0.155146 error=0.121099 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:50 downhill.base:232 Adam 719 loss=0.155034 error=0.121045 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:50 downhill.base:232 Adam 720 loss=0.154922 error=0.120991 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:50 downhill.base:232 validation 72 loss=0.154811 error=0.120938 grad(U)=0.000001 grad(V)=0.000009 *\n",
      "I 2016-12-18 00:11:50 downhill.base:232 Adam 721 loss=0.154811 error=0.120938 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:51 downhill.base:232 Adam 722 loss=0.154700 error=0.120884 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:51 downhill.base:232 Adam 723 loss=0.154589 error=0.120831 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:51 downhill.base:232 Adam 724 loss=0.154479 error=0.120778 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:51 downhill.base:232 Adam 725 loss=0.154369 error=0.120725 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:51 downhill.base:232 Adam 726 loss=0.154259 error=0.120672 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:52 downhill.base:232 Adam 727 loss=0.154149 error=0.120620 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:52 downhill.base:232 Adam 728 loss=0.154040 error=0.120567 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:52 downhill.base:232 Adam 729 loss=0.153930 error=0.120515 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:52 downhill.base:232 Adam 730 loss=0.153821 error=0.120463 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:52 downhill.base:232 validation 73 loss=0.153713 error=0.120411 grad(U)=0.000001 grad(V)=0.000009 *\n",
      "I 2016-12-18 00:11:53 downhill.base:232 Adam 731 loss=0.153713 error=0.120411 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:53 downhill.base:232 Adam 732 loss=0.153604 error=0.120359 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:53 downhill.base:232 Adam 733 loss=0.153496 error=0.120307 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:53 downhill.base:232 Adam 734 loss=0.153388 error=0.120256 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:53 downhill.base:232 Adam 735 loss=0.153280 error=0.120205 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:54 downhill.base:232 Adam 736 loss=0.153173 error=0.120153 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:54 downhill.base:232 Adam 737 loss=0.153065 error=0.120102 grad(U)=0.000001 grad(V)=0.000009\n",
      "I 2016-12-18 00:11:54 downhill.base:232 Adam 738 loss=0.152958 error=0.120052 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:54 downhill.base:232 Adam 739 loss=0.152852 error=0.120001 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:54 downhill.base:232 Adam 740 loss=0.152745 error=0.119951 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:54 downhill.base:232 validation 74 loss=0.152639 error=0.119900 grad(U)=0.000001 grad(V)=0.000008 *\n",
      "I 2016-12-18 00:11:55 downhill.base:232 Adam 741 loss=0.152639 error=0.119900 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:55 downhill.base:232 Adam 742 loss=0.152533 error=0.119850 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:55 downhill.base:232 Adam 743 loss=0.152427 error=0.119800 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:55 downhill.base:232 Adam 744 loss=0.152321 error=0.119750 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:55 downhill.base:232 Adam 745 loss=0.152216 error=0.119701 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:56 downhill.base:232 Adam 746 loss=0.152110 error=0.119651 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:56 downhill.base:232 Adam 747 loss=0.152005 error=0.119602 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:56 downhill.base:232 Adam 748 loss=0.151901 error=0.119553 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:56 downhill.base:232 Adam 749 loss=0.151796 error=0.119504 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:56 downhill.base:232 Adam 750 loss=0.151692 error=0.119455 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:57 downhill.base:232 validation 75 loss=0.151588 error=0.119406 grad(U)=0.000001 grad(V)=0.000008 *\n",
      "I 2016-12-18 00:11:57 downhill.base:232 Adam 751 loss=0.151588 error=0.119406 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:57 downhill.base:232 Adam 752 loss=0.151484 error=0.119358 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:57 downhill.base:232 Adam 753 loss=0.151381 error=0.119309 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:57 downhill.base:232 Adam 754 loss=0.151277 error=0.119261 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:58 downhill.base:232 Adam 755 loss=0.151174 error=0.119213 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:58 downhill.base:232 Adam 756 loss=0.151071 error=0.119165 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:58 downhill.base:232 Adam 757 loss=0.150969 error=0.119117 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:58 downhill.base:232 Adam 758 loss=0.150866 error=0.119070 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:58 downhill.base:232 Adam 759 loss=0.150764 error=0.119022 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:59 downhill.base:232 Adam 760 loss=0.150662 error=0.118975 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:59 downhill.base:232 validation 76 loss=0.150561 error=0.118928 grad(U)=0.000001 grad(V)=0.000008 *\n",
      "I 2016-12-18 00:11:59 downhill.base:232 Adam 761 loss=0.150561 error=0.118928 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:59 downhill.base:232 Adam 762 loss=0.150459 error=0.118881 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:11:59 downhill.base:232 Adam 763 loss=0.150358 error=0.118834 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:12:00 downhill.base:232 Adam 764 loss=0.150257 error=0.118787 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:12:00 downhill.base:232 Adam 765 loss=0.150156 error=0.118741 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:12:00 downhill.base:232 Adam 766 loss=0.150056 error=0.118694 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:12:00 downhill.base:232 Adam 767 loss=0.149955 error=0.118648 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:12:00 downhill.base:232 Adam 768 loss=0.149855 error=0.118602 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:12:01 downhill.base:232 Adam 769 loss=0.149755 error=0.118556 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:12:01 downhill.base:232 Adam 770 loss=0.149656 error=0.118510 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:12:01 downhill.base:232 validation 77 loss=0.149556 error=0.118464 grad(U)=0.000001 grad(V)=0.000008 *\n",
      "I 2016-12-18 00:12:01 downhill.base:232 Adam 771 loss=0.149556 error=0.118464 grad(U)=0.000001 grad(V)=0.000008\n",
      "I 2016-12-18 00:12:01 downhill.base:232 Adam 772 loss=0.149457 error=0.118419 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:02 downhill.base:232 Adam 773 loss=0.149358 error=0.118374 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:02 downhill.base:232 Adam 774 loss=0.149259 error=0.118329 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:02 downhill.base:232 Adam 775 loss=0.149161 error=0.118284 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:02 downhill.base:232 Adam 776 loss=0.149062 error=0.118239 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:02 downhill.base:232 Adam 777 loss=0.148964 error=0.118194 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:03 downhill.base:232 Adam 778 loss=0.148866 error=0.118149 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:03 downhill.base:232 Adam 779 loss=0.148768 error=0.118105 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:03 downhill.base:232 Adam 780 loss=0.148671 error=0.118061 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:03 downhill.base:232 validation 78 loss=0.148574 error=0.118016 grad(U)=0.000001 grad(V)=0.000007 *\n",
      "I 2016-12-18 00:12:03 downhill.base:232 Adam 781 loss=0.148574 error=0.118016 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:04 downhill.base:232 Adam 782 loss=0.148477 error=0.117972 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:04 downhill.base:232 Adam 783 loss=0.148380 error=0.117929 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:04 downhill.base:232 Adam 784 loss=0.148283 error=0.117885 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:04 downhill.base:232 Adam 785 loss=0.148187 error=0.117841 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:04 downhill.base:232 Adam 786 loss=0.148091 error=0.117798 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:05 downhill.base:232 Adam 787 loss=0.147995 error=0.117755 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:05 downhill.base:232 Adam 788 loss=0.147899 error=0.117711 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:05 downhill.base:232 Adam 789 loss=0.147803 error=0.117668 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:05 downhill.base:232 Adam 790 loss=0.147708 error=0.117625 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:05 downhill.base:232 validation 79 loss=0.147613 error=0.117583 grad(U)=0.000001 grad(V)=0.000007 *\n",
      "I 2016-12-18 00:12:06 downhill.base:232 Adam 791 loss=0.147613 error=0.117583 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:06 downhill.base:232 Adam 792 loss=0.147518 error=0.117540 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:06 downhill.base:232 Adam 793 loss=0.147423 error=0.117498 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:06 downhill.base:232 Adam 794 loss=0.147329 error=0.117455 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:06 downhill.base:232 Adam 795 loss=0.147234 error=0.117413 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:07 downhill.base:232 Adam 796 loss=0.147140 error=0.117371 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:07 downhill.base:232 Adam 797 loss=0.147046 error=0.117329 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:07 downhill.base:232 Adam 798 loss=0.146953 error=0.117287 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:07 downhill.base:232 Adam 799 loss=0.146859 error=0.117246 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:07 downhill.base:232 Adam 800 loss=0.146766 error=0.117204 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:07 downhill.base:232 validation 80 loss=0.146673 error=0.117163 grad(U)=0.000001 grad(V)=0.000007 *\n",
      "I 2016-12-18 00:12:08 downhill.base:232 Adam 801 loss=0.146673 error=0.117163 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:08 downhill.base:232 Adam 802 loss=0.146580 error=0.117122 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:08 downhill.base:232 Adam 803 loss=0.146487 error=0.117081 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:08 downhill.base:232 Adam 804 loss=0.146395 error=0.117040 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:09 downhill.base:232 Adam 805 loss=0.146303 error=0.116999 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:09 downhill.base:232 Adam 806 loss=0.146210 error=0.116958 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:09 downhill.base:232 Adam 807 loss=0.146119 error=0.116918 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:09 downhill.base:232 Adam 808 loss=0.146027 error=0.116877 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:09 downhill.base:232 Adam 809 loss=0.145936 error=0.116837 grad(U)=0.000001 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:10 downhill.base:232 Adam 810 loss=0.145844 error=0.116797 grad(U)=0.000000 grad(V)=0.000007\n",
      "I 2016-12-18 00:12:10 downhill.base:232 validation 81 loss=0.145753 error=0.116757 grad(U)=0.000000 grad(V)=0.000006 *\n",
      "I 2016-12-18 00:12:10 downhill.base:232 Adam 811 loss=0.145753 error=0.116757 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:10 downhill.base:232 Adam 812 loss=0.145662 error=0.116717 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:11 downhill.base:232 Adam 813 loss=0.145572 error=0.116677 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:11 downhill.base:232 Adam 814 loss=0.145481 error=0.116638 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:11 downhill.base:232 Adam 815 loss=0.145391 error=0.116598 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:11 downhill.base:232 Adam 816 loss=0.145301 error=0.116559 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:11 downhill.base:232 Adam 817 loss=0.145211 error=0.116520 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:12 downhill.base:232 Adam 818 loss=0.145122 error=0.116481 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:12 downhill.base:232 Adam 819 loss=0.145032 error=0.116442 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:12 downhill.base:232 Adam 820 loss=0.144943 error=0.116403 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:12 downhill.base:232 validation 82 loss=0.144854 error=0.116364 grad(U)=0.000000 grad(V)=0.000006 *\n",
      "I 2016-12-18 00:12:13 downhill.base:232 Adam 821 loss=0.144854 error=0.116364 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:13 downhill.base:232 Adam 822 loss=0.144765 error=0.116326 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:13 downhill.base:232 Adam 823 loss=0.144677 error=0.116287 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:13 downhill.base:232 Adam 824 loss=0.144588 error=0.116249 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:13 downhill.base:232 Adam 825 loss=0.144500 error=0.116211 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:14 downhill.base:232 Adam 826 loss=0.144412 error=0.116173 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:14 downhill.base:232 Adam 827 loss=0.144324 error=0.116135 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:14 downhill.base:232 Adam 828 loss=0.144237 error=0.116097 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:14 downhill.base:232 Adam 829 loss=0.144149 error=0.116059 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:14 downhill.base:232 Adam 830 loss=0.144062 error=0.116022 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:14 downhill.base:232 validation 83 loss=0.143975 error=0.115984 grad(U)=0.000000 grad(V)=0.000006 *\n",
      "I 2016-12-18 00:12:15 downhill.base:232 Adam 831 loss=0.143975 error=0.115984 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:15 downhill.base:232 Adam 832 loss=0.143888 error=0.115947 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:15 downhill.base:232 Adam 833 loss=0.143801 error=0.115910 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:15 downhill.base:232 Adam 834 loss=0.143715 error=0.115872 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:15 downhill.base:232 Adam 835 loss=0.143628 error=0.115836 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:16 downhill.base:232 Adam 836 loss=0.143542 error=0.115799 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:16 downhill.base:232 Adam 837 loss=0.143456 error=0.115762 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:16 downhill.base:232 Adam 838 loss=0.143371 error=0.115726 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:16 downhill.base:232 Adam 839 loss=0.143285 error=0.115689 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:16 downhill.base:232 Adam 840 loss=0.143200 error=0.115653 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:17 downhill.base:232 validation 84 loss=0.143115 error=0.115616 grad(U)=0.000000 grad(V)=0.000006 *\n",
      "I 2016-12-18 00:12:17 downhill.base:232 Adam 841 loss=0.143115 error=0.115616 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:17 downhill.base:232 Adam 842 loss=0.143030 error=0.115580 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:17 downhill.base:232 Adam 843 loss=0.142945 error=0.115544 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:17 downhill.base:232 Adam 844 loss=0.142860 error=0.115509 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:18 downhill.base:232 Adam 845 loss=0.142776 error=0.115473 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:18 downhill.base:232 Adam 846 loss=0.142692 error=0.115437 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:18 downhill.base:232 Adam 847 loss=0.142608 error=0.115402 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:18 downhill.base:232 Adam 848 loss=0.142524 error=0.115366 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:18 downhill.base:232 Adam 849 loss=0.142440 error=0.115331 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:19 downhill.base:232 Adam 850 loss=0.142357 error=0.115296 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:19 downhill.base:232 validation 85 loss=0.142274 error=0.115261 grad(U)=0.000000 grad(V)=0.000006 *\n",
      "I 2016-12-18 00:12:19 downhill.base:232 Adam 851 loss=0.142274 error=0.115261 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:19 downhill.base:232 Adam 852 loss=0.142190 error=0.115226 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:19 downhill.base:232 Adam 853 loss=0.142108 error=0.115191 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:20 downhill.base:232 Adam 854 loss=0.142025 error=0.115157 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:20 downhill.base:232 Adam 855 loss=0.141943 error=0.115122 grad(U)=0.000000 grad(V)=0.000006\n",
      "I 2016-12-18 00:12:20 downhill.base:232 Adam 856 loss=0.141860 error=0.115088 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:20 downhill.base:232 Adam 857 loss=0.141778 error=0.115053 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:20 downhill.base:232 Adam 858 loss=0.141696 error=0.115019 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:21 downhill.base:232 Adam 859 loss=0.141614 error=0.114985 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:21 downhill.base:232 Adam 860 loss=0.141533 error=0.114951 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:21 downhill.base:232 validation 86 loss=0.141451 error=0.114917 grad(U)=0.000000 grad(V)=0.000005 *\n",
      "I 2016-12-18 00:12:21 downhill.base:232 Adam 861 loss=0.141451 error=0.114917 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:21 downhill.base:232 Adam 862 loss=0.141370 error=0.114883 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:22 downhill.base:232 Adam 863 loss=0.141289 error=0.114850 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:22 downhill.base:232 Adam 864 loss=0.141208 error=0.114816 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:22 downhill.base:232 Adam 865 loss=0.141128 error=0.114783 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:22 downhill.base:232 Adam 866 loss=0.141047 error=0.114750 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:22 downhill.base:232 Adam 867 loss=0.140967 error=0.114716 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:23 downhill.base:232 Adam 868 loss=0.140887 error=0.114683 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:23 downhill.base:232 Adam 869 loss=0.140807 error=0.114650 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:23 downhill.base:232 Adam 870 loss=0.140727 error=0.114618 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:23 downhill.base:232 validation 87 loss=0.140647 error=0.114585 grad(U)=0.000000 grad(V)=0.000005 *\n",
      "I 2016-12-18 00:12:23 downhill.base:232 Adam 871 loss=0.140647 error=0.114585 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:24 downhill.base:232 Adam 872 loss=0.140568 error=0.114552 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:24 downhill.base:232 Adam 873 loss=0.140488 error=0.114520 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:24 downhill.base:232 Adam 874 loss=0.140409 error=0.114487 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:24 downhill.base:232 Adam 875 loss=0.140330 error=0.114455 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:24 downhill.base:232 Adam 876 loss=0.140251 error=0.114423 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:25 downhill.base:232 Adam 877 loss=0.140173 error=0.114391 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:25 downhill.base:232 Adam 878 loss=0.140094 error=0.114359 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:25 downhill.base:232 Adam 879 loss=0.140016 error=0.114327 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:25 downhill.base:232 Adam 880 loss=0.139938 error=0.114295 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:25 downhill.base:232 validation 88 loss=0.139860 error=0.114263 grad(U)=0.000000 grad(V)=0.000005 *\n",
      "I 2016-12-18 00:12:26 downhill.base:232 Adam 881 loss=0.139860 error=0.114263 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:26 downhill.base:232 Adam 882 loss=0.139782 error=0.114232 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:26 downhill.base:232 Adam 883 loss=0.139704 error=0.114200 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:26 downhill.base:232 Adam 884 loss=0.139627 error=0.114169 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:26 downhill.base:232 Adam 885 loss=0.139550 error=0.114138 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:27 downhill.base:232 Adam 886 loss=0.139473 error=0.114107 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:27 downhill.base:232 Adam 887 loss=0.139396 error=0.114076 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:27 downhill.base:232 Adam 888 loss=0.139319 error=0.114045 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:27 downhill.base:232 Adam 889 loss=0.139243 error=0.114014 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:27 downhill.base:232 Adam 890 loss=0.139166 error=0.113983 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:28 downhill.base:232 validation 89 loss=0.139090 error=0.113952 grad(U)=0.000000 grad(V)=0.000005 *\n",
      "I 2016-12-18 00:12:28 downhill.base:232 Adam 891 loss=0.139090 error=0.113952 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:28 downhill.base:232 Adam 892 loss=0.139014 error=0.113922 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:28 downhill.base:232 Adam 893 loss=0.138938 error=0.113892 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:28 downhill.base:232 Adam 894 loss=0.138862 error=0.113861 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:29 downhill.base:232 Adam 895 loss=0.138787 error=0.113831 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:29 downhill.base:232 Adam 896 loss=0.138711 error=0.113801 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:29 downhill.base:232 Adam 897 loss=0.138636 error=0.113771 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:29 downhill.base:232 Adam 898 loss=0.138561 error=0.113741 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:29 downhill.base:232 Adam 899 loss=0.138486 error=0.113711 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:30 downhill.base:232 Adam 900 loss=0.138411 error=0.113682 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:30 downhill.base:232 validation 90 loss=0.138337 error=0.113652 grad(U)=0.000000 grad(V)=0.000005 *\n",
      "I 2016-12-18 00:12:30 downhill.base:232 Adam 901 loss=0.138337 error=0.113652 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:30 downhill.base:232 Adam 902 loss=0.138262 error=0.113623 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:30 downhill.base:232 Adam 903 loss=0.138188 error=0.113593 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:31 downhill.base:232 Adam 904 loss=0.138114 error=0.113564 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:31 downhill.base:232 Adam 905 loss=0.138040 error=0.113535 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:31 downhill.base:232 Adam 906 loss=0.137966 error=0.113506 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:31 downhill.base:232 Adam 907 loss=0.137892 error=0.113477 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:31 downhill.base:232 Adam 908 loss=0.137819 error=0.113448 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:32 downhill.base:232 Adam 909 loss=0.137746 error=0.113419 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:32 downhill.base:232 Adam 910 loss=0.137673 error=0.113390 grad(U)=0.000000 grad(V)=0.000005\n",
      "I 2016-12-18 00:12:32 downhill.base:232 validation 91 loss=0.137599 error=0.113362 grad(U)=0.000000 grad(V)=0.000004 *\n",
      "I 2016-12-18 00:12:32 downhill.base:232 Adam 911 loss=0.137599 error=0.113362 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:32 downhill.base:232 Adam 912 loss=0.137527 error=0.113333 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:33 downhill.base:232 Adam 913 loss=0.137454 error=0.113305 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:33 downhill.base:232 Adam 914 loss=0.137382 error=0.113277 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:33 downhill.base:232 Adam 915 loss=0.137309 error=0.113248 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:33 downhill.base:232 Adam 916 loss=0.137237 error=0.113220 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:33 downhill.base:232 Adam 917 loss=0.137165 error=0.113192 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:34 downhill.base:232 Adam 918 loss=0.137093 error=0.113164 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:34 downhill.base:232 Adam 919 loss=0.137021 error=0.113136 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:34 downhill.base:232 Adam 920 loss=0.136950 error=0.113109 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:34 downhill.base:232 validation 92 loss=0.136878 error=0.113081 grad(U)=0.000000 grad(V)=0.000004 *\n",
      "I 2016-12-18 00:12:34 downhill.base:232 Adam 921 loss=0.136878 error=0.113081 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:35 downhill.base:232 Adam 922 loss=0.136807 error=0.113054 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:35 downhill.base:232 Adam 923 loss=0.136736 error=0.113026 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:35 downhill.base:232 Adam 924 loss=0.136665 error=0.112999 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:35 downhill.base:232 Adam 925 loss=0.136594 error=0.112971 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:35 downhill.base:232 Adam 926 loss=0.136524 error=0.112944 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:36 downhill.base:232 Adam 927 loss=0.136453 error=0.112917 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:36 downhill.base:232 Adam 928 loss=0.136383 error=0.112890 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:36 downhill.base:232 Adam 929 loss=0.136313 error=0.112863 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:36 downhill.base:232 Adam 930 loss=0.136243 error=0.112837 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:36 downhill.base:232 validation 93 loss=0.136173 error=0.112810 grad(U)=0.000000 grad(V)=0.000004 *\n",
      "I 2016-12-18 00:12:37 downhill.base:232 Adam 931 loss=0.136173 error=0.112810 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:37 downhill.base:232 Adam 932 loss=0.136103 error=0.112783 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:37 downhill.base:232 Adam 933 loss=0.136034 error=0.112757 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:37 downhill.base:232 Adam 934 loss=0.135964 error=0.112730 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:37 downhill.base:232 Adam 935 loss=0.135895 error=0.112704 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:38 downhill.base:232 Adam 936 loss=0.135826 error=0.112678 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:38 downhill.base:232 Adam 937 loss=0.135757 error=0.112652 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:38 downhill.base:232 Adam 938 loss=0.135688 error=0.112626 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:38 downhill.base:232 Adam 939 loss=0.135620 error=0.112600 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:38 downhill.base:232 Adam 940 loss=0.135551 error=0.112574 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:39 downhill.base:232 validation 94 loss=0.135483 error=0.112548 grad(U)=0.000000 grad(V)=0.000004 *\n",
      "I 2016-12-18 00:12:39 downhill.base:232 Adam 941 loss=0.135483 error=0.112548 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:39 downhill.base:232 Adam 942 loss=0.135415 error=0.112522 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:39 downhill.base:232 Adam 943 loss=0.135347 error=0.112497 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:39 downhill.base:232 Adam 944 loss=0.135279 error=0.112471 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:40 downhill.base:232 Adam 945 loss=0.135211 error=0.112446 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:40 downhill.base:232 Adam 946 loss=0.135144 error=0.112421 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:40 downhill.base:232 Adam 947 loss=0.135076 error=0.112395 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:40 downhill.base:232 Adam 948 loss=0.135009 error=0.112370 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:40 downhill.base:232 Adam 949 loss=0.134942 error=0.112345 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:41 downhill.base:232 Adam 950 loss=0.134875 error=0.112320 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:41 downhill.base:232 validation 95 loss=0.134808 error=0.112295 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:41 downhill.base:232 Adam 951 loss=0.134808 error=0.112295 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:41 downhill.base:232 Adam 952 loss=0.134741 error=0.112270 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:41 downhill.base:232 Adam 953 loss=0.134675 error=0.112246 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:42 downhill.base:232 Adam 954 loss=0.134609 error=0.112221 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:42 downhill.base:232 Adam 955 loss=0.134542 error=0.112197 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:42 downhill.base:232 Adam 956 loss=0.134476 error=0.112172 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:42 downhill.base:232 Adam 957 loss=0.134410 error=0.112148 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:42 downhill.base:232 Adam 958 loss=0.134344 error=0.112123 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:43 downhill.base:232 Adam 959 loss=0.134279 error=0.112099 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:43 downhill.base:232 Adam 960 loss=0.134213 error=0.112075 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:43 downhill.base:232 validation 96 loss=0.134148 error=0.112051 grad(U)=0.000000 grad(V)=0.000004 *\n",
      "I 2016-12-18 00:12:43 downhill.base:232 Adam 961 loss=0.134148 error=0.112051 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:43 downhill.base:232 Adam 962 loss=0.134083 error=0.112027 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:44 downhill.base:232 Adam 963 loss=0.134018 error=0.112003 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:44 downhill.base:232 Adam 964 loss=0.133953 error=0.111979 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:44 downhill.base:232 Adam 965 loss=0.133888 error=0.111956 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:44 downhill.base:232 Adam 966 loss=0.133823 error=0.111932 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:44 downhill.base:232 Adam 967 loss=0.133759 error=0.111909 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:45 downhill.base:232 Adam 968 loss=0.133694 error=0.111885 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:45 downhill.base:232 Adam 969 loss=0.133630 error=0.111862 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:45 downhill.base:232 Adam 970 loss=0.133566 error=0.111838 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:45 downhill.base:232 validation 97 loss=0.133502 error=0.111815 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:45 downhill.base:232 Adam 971 loss=0.133502 error=0.111815 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:46 downhill.base:232 Adam 972 loss=0.133438 error=0.111792 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:46 downhill.base:232 Adam 973 loss=0.133374 error=0.111769 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:46 downhill.base:232 Adam 974 loss=0.133311 error=0.111746 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:46 downhill.base:232 Adam 975 loss=0.133247 error=0.111723 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:46 downhill.base:232 Adam 976 loss=0.133184 error=0.111700 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:47 downhill.base:232 Adam 977 loss=0.133121 error=0.111677 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:47 downhill.base:232 Adam 978 loss=0.133058 error=0.111655 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:47 downhill.base:232 Adam 979 loss=0.132995 error=0.111632 grad(U)=0.000000 grad(V)=0.000004\n",
      "I 2016-12-18 00:12:47 downhill.base:232 Adam 980 loss=0.132933 error=0.111610 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:47 downhill.base:232 validation 98 loss=0.132870 error=0.111587 grad(U)=0.000000 grad(V)=0.000003 *\n",
      "I 2016-12-18 00:12:47 downhill.base:232 Adam 981 loss=0.132870 error=0.111587 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:48 downhill.base:232 Adam 982 loss=0.132808 error=0.111565 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:48 downhill.base:232 Adam 983 loss=0.132745 error=0.111543 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:48 downhill.base:232 Adam 984 loss=0.132683 error=0.111520 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:48 downhill.base:232 Adam 985 loss=0.132621 error=0.111498 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:48 downhill.base:232 Adam 986 loss=0.132559 error=0.111476 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:49 downhill.base:232 Adam 987 loss=0.132498 error=0.111454 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:49 downhill.base:232 Adam 988 loss=0.132436 error=0.111432 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:49 downhill.base:232 Adam 989 loss=0.132375 error=0.111411 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:49 downhill.base:232 Adam 990 loss=0.132313 error=0.111389 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:49 downhill.base:232 validation 99 loss=0.132252 error=0.111367 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:50 downhill.base:232 Adam 991 loss=0.132252 error=0.111367 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:50 downhill.base:232 Adam 992 loss=0.132191 error=0.111346 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:50 downhill.base:232 Adam 993 loss=0.132130 error=0.111324 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:50 downhill.base:232 Adam 994 loss=0.132070 error=0.111303 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:51 downhill.base:232 Adam 995 loss=0.132009 error=0.111281 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:51 downhill.base:232 Adam 996 loss=0.131949 error=0.111260 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:51 downhill.base:232 Adam 997 loss=0.131888 error=0.111239 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:51 downhill.base:232 Adam 998 loss=0.131828 error=0.111218 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:52 downhill.base:232 Adam 999 loss=0.131768 error=0.111196 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:52 downhill.base:232 Adam 1000 loss=0.131708 error=0.111176 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:52 downhill.base:232 validation 100 loss=0.131648 error=0.111155 grad(U)=0.000000 grad(V)=0.000003 *\n",
      "I 2016-12-18 00:12:52 downhill.base:232 Adam 1001 loss=0.131648 error=0.111155 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:52 downhill.base:232 Adam 1002 loss=0.131588 error=0.111134 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:53 downhill.base:232 Adam 1003 loss=0.131529 error=0.111113 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:53 downhill.base:232 Adam 1004 loss=0.131470 error=0.111092 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:53 downhill.base:232 Adam 1005 loss=0.131410 error=0.111072 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:53 downhill.base:232 Adam 1006 loss=0.131351 error=0.111051 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:54 downhill.base:232 Adam 1007 loss=0.131292 error=0.111031 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:54 downhill.base:232 Adam 1008 loss=0.131233 error=0.111011 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:54 downhill.base:232 Adam 1009 loss=0.131175 error=0.110990 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:54 downhill.base:232 Adam 1010 loss=0.131116 error=0.110970 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:54 downhill.base:232 validation 101 loss=0.131058 error=0.110950 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:55 downhill.base:232 Adam 1011 loss=0.131058 error=0.110950 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:55 downhill.base:232 Adam 1012 loss=0.130999 error=0.110930 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:55 downhill.base:232 Adam 1013 loss=0.130941 error=0.110910 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:55 downhill.base:232 Adam 1014 loss=0.130883 error=0.110890 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:55 downhill.base:232 Adam 1015 loss=0.130825 error=0.110870 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:56 downhill.base:232 Adam 1016 loss=0.130767 error=0.110850 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:56 downhill.base:232 Adam 1017 loss=0.130709 error=0.110830 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:56 downhill.base:232 Adam 1018 loss=0.130652 error=0.110811 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:56 downhill.base:232 Adam 1019 loss=0.130594 error=0.110791 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:56 downhill.base:232 Adam 1020 loss=0.130537 error=0.110772 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:56 downhill.base:232 validation 102 loss=0.130480 error=0.110752 grad(U)=0.000000 grad(V)=0.000003 *\n",
      "I 2016-12-18 00:12:57 downhill.base:232 Adam 1021 loss=0.130480 error=0.110752 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:57 downhill.base:232 Adam 1022 loss=0.130423 error=0.110733 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:57 downhill.base:232 Adam 1023 loss=0.130366 error=0.110713 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:57 downhill.base:232 Adam 1024 loss=0.130309 error=0.110694 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:57 downhill.base:232 Adam 1025 loss=0.130252 error=0.110675 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:58 downhill.base:232 Adam 1026 loss=0.130195 error=0.110656 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:58 downhill.base:232 Adam 1027 loss=0.130139 error=0.110637 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:58 downhill.base:232 Adam 1028 loss=0.130083 error=0.110618 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:58 downhill.base:232 Adam 1029 loss=0.130026 error=0.110599 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:58 downhill.base:232 Adam 1030 loss=0.129970 error=0.110580 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:59 downhill.base:232 validation 103 loss=0.129915 error=0.110561 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:59 downhill.base:232 Adam 1031 loss=0.129915 error=0.110561 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:59 downhill.base:232 Adam 1032 loss=0.129859 error=0.110543 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:59 downhill.base:232 Adam 1033 loss=0.129803 error=0.110524 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:12:59 downhill.base:232 Adam 1034 loss=0.129747 error=0.110505 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:00 downhill.base:232 Adam 1035 loss=0.129692 error=0.110487 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:00 downhill.base:232 Adam 1036 loss=0.129637 error=0.110469 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:00 downhill.base:232 Adam 1037 loss=0.129581 error=0.110450 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:00 downhill.base:232 Adam 1038 loss=0.129526 error=0.110432 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:00 downhill.base:232 Adam 1039 loss=0.129471 error=0.110414 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:01 downhill.base:232 Adam 1040 loss=0.129417 error=0.110395 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:01 downhill.base:232 validation 104 loss=0.129362 error=0.110377 grad(U)=0.000000 grad(V)=0.000003 *\n",
      "I 2016-12-18 00:13:01 downhill.base:232 Adam 1041 loss=0.129362 error=0.110377 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:01 downhill.base:232 Adam 1042 loss=0.129307 error=0.110359 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:02 downhill.base:232 Adam 1043 loss=0.129253 error=0.110341 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:02 downhill.base:232 Adam 1044 loss=0.129198 error=0.110323 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:02 downhill.base:232 Adam 1045 loss=0.129144 error=0.110306 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:02 downhill.base:232 Adam 1046 loss=0.129090 error=0.110288 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:02 downhill.base:232 Adam 1047 loss=0.129036 error=0.110270 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:03 downhill.base:232 Adam 1048 loss=0.128982 error=0.110252 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:03 downhill.base:232 Adam 1049 loss=0.128929 error=0.110235 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:03 downhill.base:232 Adam 1050 loss=0.128875 error=0.110217 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:03 downhill.base:232 validation 105 loss=0.128821 error=0.110200 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:03 downhill.base:232 Adam 1051 loss=0.128821 error=0.110200 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:04 downhill.base:232 Adam 1052 loss=0.128768 error=0.110182 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:04 downhill.base:232 Adam 1053 loss=0.128715 error=0.110165 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:04 downhill.base:232 Adam 1054 loss=0.128662 error=0.110148 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:04 downhill.base:232 Adam 1055 loss=0.128609 error=0.110131 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:04 downhill.base:232 Adam 1056 loss=0.128556 error=0.110114 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:05 downhill.base:232 Adam 1057 loss=0.128503 error=0.110096 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:05 downhill.base:232 Adam 1058 loss=0.128450 error=0.110079 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:05 downhill.base:232 Adam 1059 loss=0.128398 error=0.110062 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:05 downhill.base:232 Adam 1060 loss=0.128345 error=0.110045 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:05 downhill.base:232 validation 106 loss=0.128293 error=0.110029 grad(U)=0.000000 grad(V)=0.000003 *\n",
      "I 2016-12-18 00:13:06 downhill.base:232 Adam 1061 loss=0.128293 error=0.110029 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:06 downhill.base:232 Adam 1062 loss=0.128241 error=0.110012 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:06 downhill.base:232 Adam 1063 loss=0.128189 error=0.109995 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:06 downhill.base:232 Adam 1064 loss=0.128137 error=0.109978 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:06 downhill.base:232 Adam 1065 loss=0.128085 error=0.109962 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:07 downhill.base:232 Adam 1066 loss=0.128034 error=0.109945 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:07 downhill.base:232 Adam 1067 loss=0.127982 error=0.109929 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:07 downhill.base:232 Adam 1068 loss=0.127930 error=0.109912 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:07 downhill.base:232 Adam 1069 loss=0.127879 error=0.109896 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:07 downhill.base:232 Adam 1070 loss=0.127828 error=0.109880 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:08 downhill.base:232 validation 107 loss=0.127777 error=0.109863 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:08 downhill.base:232 Adam 1071 loss=0.127777 error=0.109863 grad(U)=0.000000 grad(V)=0.000003\n",
      "I 2016-12-18 00:13:08 downhill.base:232 Adam 1072 loss=0.127726 error=0.109847 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:08 downhill.base:232 Adam 1073 loss=0.127675 error=0.109831 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:08 downhill.base:232 Adam 1074 loss=0.127624 error=0.109815 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:09 downhill.base:232 Adam 1075 loss=0.127573 error=0.109799 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:09 downhill.base:232 Adam 1076 loss=0.127523 error=0.109783 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:09 downhill.base:232 Adam 1077 loss=0.127472 error=0.109767 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:09 downhill.base:232 Adam 1078 loss=0.127422 error=0.109751 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:09 downhill.base:232 Adam 1079 loss=0.127372 error=0.109736 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:09 downhill.base:232 Adam 1080 loss=0.127321 error=0.109720 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:10 downhill.base:232 validation 108 loss=0.127271 error=0.109704 grad(U)=0.000000 grad(V)=0.000002 *\n",
      "I 2016-12-18 00:13:10 downhill.base:232 Adam 1081 loss=0.127271 error=0.109704 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:10 downhill.base:232 Adam 1082 loss=0.127221 error=0.109689 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:10 downhill.base:232 Adam 1083 loss=0.127172 error=0.109673 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:10 downhill.base:232 Adam 1084 loss=0.127122 error=0.109658 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:11 downhill.base:232 Adam 1085 loss=0.127072 error=0.109642 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:11 downhill.base:232 Adam 1086 loss=0.127023 error=0.109627 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:11 downhill.base:232 Adam 1087 loss=0.126973 error=0.109611 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:11 downhill.base:232 Adam 1088 loss=0.126924 error=0.109596 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:11 downhill.base:232 Adam 1089 loss=0.126875 error=0.109581 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:12 downhill.base:232 Adam 1090 loss=0.126826 error=0.109566 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:12 downhill.base:232 validation 109 loss=0.126777 error=0.109551 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:12 downhill.base:232 Adam 1091 loss=0.126777 error=0.109551 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:12 downhill.base:232 Adam 1092 loss=0.126728 error=0.109536 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:12 downhill.base:232 Adam 1093 loss=0.126679 error=0.109521 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:13 downhill.base:232 Adam 1094 loss=0.126631 error=0.109506 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:13 downhill.base:232 Adam 1095 loss=0.126582 error=0.109491 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:13 downhill.base:232 Adam 1096 loss=0.126534 error=0.109476 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:13 downhill.base:232 Adam 1097 loss=0.126486 error=0.109461 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:13 downhill.base:232 Adam 1098 loss=0.126437 error=0.109447 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:14 downhill.base:232 Adam 1099 loss=0.126389 error=0.109432 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:14 downhill.base:232 Adam 1100 loss=0.126341 error=0.109417 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:14 downhill.base:232 validation 110 loss=0.126294 error=0.109403 grad(U)=0.000000 grad(V)=0.000002 *\n",
      "I 2016-12-18 00:13:14 downhill.base:232 Adam 1101 loss=0.126294 error=0.109403 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:14 downhill.base:232 Adam 1102 loss=0.126246 error=0.109388 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:15 downhill.base:232 Adam 1103 loss=0.126198 error=0.109374 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:15 downhill.base:232 Adam 1104 loss=0.126151 error=0.109360 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:15 downhill.base:232 Adam 1105 loss=0.126103 error=0.109345 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:15 downhill.base:232 Adam 1106 loss=0.126056 error=0.109331 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:15 downhill.base:232 Adam 1107 loss=0.126009 error=0.109317 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:16 downhill.base:232 Adam 1108 loss=0.125962 error=0.109303 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:16 downhill.base:232 Adam 1109 loss=0.125915 error=0.109288 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:16 downhill.base:232 Adam 1110 loss=0.125868 error=0.109274 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:16 downhill.base:232 validation 111 loss=0.125821 error=0.109260 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:16 downhill.base:232 Adam 1111 loss=0.125821 error=0.109260 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:17 downhill.base:232 Adam 1112 loss=0.125774 error=0.109246 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:17 downhill.base:232 Adam 1113 loss=0.125727 error=0.109233 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:17 downhill.base:232 Adam 1114 loss=0.125681 error=0.109219 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:17 downhill.base:232 Adam 1115 loss=0.125635 error=0.109205 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:17 downhill.base:232 Adam 1116 loss=0.125588 error=0.109191 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:18 downhill.base:232 Adam 1117 loss=0.125542 error=0.109177 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:18 downhill.base:232 Adam 1118 loss=0.125496 error=0.109164 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:18 downhill.base:232 Adam 1119 loss=0.125450 error=0.109150 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:18 downhill.base:232 Adam 1120 loss=0.125404 error=0.109137 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:18 downhill.base:232 validation 112 loss=0.125359 error=0.109123 grad(U)=0.000000 grad(V)=0.000002 *\n",
      "I 2016-12-18 00:13:18 downhill.base:232 Adam 1121 loss=0.125359 error=0.109123 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:19 downhill.base:232 Adam 1122 loss=0.125313 error=0.109110 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:19 downhill.base:232 Adam 1123 loss=0.125267 error=0.109096 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:19 downhill.base:232 Adam 1124 loss=0.125222 error=0.109083 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:19 downhill.base:232 Adam 1125 loss=0.125176 error=0.109070 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:19 downhill.base:232 Adam 1126 loss=0.125131 error=0.109057 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:20 downhill.base:232 Adam 1127 loss=0.125086 error=0.109043 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:20 downhill.base:232 Adam 1128 loss=0.125041 error=0.109030 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:20 downhill.base:232 Adam 1129 loss=0.124996 error=0.109017 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:20 downhill.base:232 Adam 1130 loss=0.124951 error=0.109004 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:20 downhill.base:232 validation 113 loss=0.124906 error=0.108991 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:21 downhill.base:232 Adam 1131 loss=0.124906 error=0.108991 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:21 downhill.base:232 Adam 1132 loss=0.124862 error=0.108978 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:21 downhill.base:232 Adam 1133 loss=0.124817 error=0.108965 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:21 downhill.base:232 Adam 1134 loss=0.124773 error=0.108952 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:21 downhill.base:232 Adam 1135 loss=0.124728 error=0.108940 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:22 downhill.base:232 Adam 1136 loss=0.124684 error=0.108927 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:22 downhill.base:232 Adam 1137 loss=0.124640 error=0.108914 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:22 downhill.base:232 Adam 1138 loss=0.124596 error=0.108901 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:22 downhill.base:232 Adam 1139 loss=0.124552 error=0.108889 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:22 downhill.base:232 Adam 1140 loss=0.124508 error=0.108876 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:23 downhill.base:232 validation 114 loss=0.124464 error=0.108864 grad(U)=0.000000 grad(V)=0.000002 *\n",
      "I 2016-12-18 00:13:23 downhill.base:232 Adam 1141 loss=0.124464 error=0.108864 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:23 downhill.base:232 Adam 1142 loss=0.124421 error=0.108851 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:23 downhill.base:232 Adam 1143 loss=0.124377 error=0.108839 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:23 downhill.base:232 Adam 1144 loss=0.124334 error=0.108826 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:24 downhill.base:232 Adam 1145 loss=0.124290 error=0.108814 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:24 downhill.base:232 Adam 1146 loss=0.124247 error=0.108802 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:24 downhill.base:232 Adam 1147 loss=0.124204 error=0.108790 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:24 downhill.base:232 Adam 1148 loss=0.124160 error=0.108777 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:24 downhill.base:232 Adam 1149 loss=0.124118 error=0.108765 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:25 downhill.base:232 Adam 1150 loss=0.124075 error=0.108753 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:25 downhill.base:232 validation 115 loss=0.124032 error=0.108741 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:25 downhill.base:232 Adam 1151 loss=0.124032 error=0.108741 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:25 downhill.base:232 Adam 1152 loss=0.123989 error=0.108729 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:25 downhill.base:232 Adam 1153 loss=0.123946 error=0.108717 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:26 downhill.base:232 Adam 1154 loss=0.123904 error=0.108705 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:26 downhill.base:232 Adam 1155 loss=0.123861 error=0.108693 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:26 downhill.base:232 Adam 1156 loss=0.123819 error=0.108682 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:26 downhill.base:232 Adam 1157 loss=0.123777 error=0.108670 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:26 downhill.base:232 Adam 1158 loss=0.123735 error=0.108658 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:27 downhill.base:232 Adam 1159 loss=0.123692 error=0.108646 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:27 downhill.base:232 Adam 1160 loss=0.123650 error=0.108635 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:27 downhill.base:232 validation 116 loss=0.123608 error=0.108623 grad(U)=0.000000 grad(V)=0.000002 *\n",
      "I 2016-12-18 00:13:27 downhill.base:232 Adam 1161 loss=0.123608 error=0.108623 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:27 downhill.base:232 Adam 1162 loss=0.123567 error=0.108611 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:28 downhill.base:232 Adam 1163 loss=0.123525 error=0.108600 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:28 downhill.base:232 Adam 1164 loss=0.123483 error=0.108588 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:28 downhill.base:232 Adam 1165 loss=0.123442 error=0.108577 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:28 downhill.base:232 Adam 1166 loss=0.123400 error=0.108566 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:28 downhill.base:232 Adam 1167 loss=0.123359 error=0.108554 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:28 downhill.base:232 Adam 1168 loss=0.123318 error=0.108543 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:29 downhill.base:232 Adam 1169 loss=0.123277 error=0.108532 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:29 downhill.base:232 Adam 1170 loss=0.123236 error=0.108521 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:29 downhill.base:232 validation 117 loss=0.123195 error=0.108509 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:29 downhill.base:232 Adam 1171 loss=0.123195 error=0.108509 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:29 downhill.base:232 Adam 1172 loss=0.123154 error=0.108498 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:30 downhill.base:232 Adam 1173 loss=0.123113 error=0.108487 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:30 downhill.base:232 Adam 1174 loss=0.123072 error=0.108476 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:30 downhill.base:232 Adam 1175 loss=0.123032 error=0.108465 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:30 downhill.base:232 Adam 1176 loss=0.122991 error=0.108454 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:30 downhill.base:232 Adam 1177 loss=0.122951 error=0.108443 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:31 downhill.base:232 Adam 1178 loss=0.122910 error=0.108432 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:31 downhill.base:232 Adam 1179 loss=0.122870 error=0.108422 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:31 downhill.base:232 Adam 1180 loss=0.122830 error=0.108411 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:31 downhill.base:232 validation 118 loss=0.122790 error=0.108400 grad(U)=0.000000 grad(V)=0.000002 *\n",
      "I 2016-12-18 00:13:31 downhill.base:232 Adam 1181 loss=0.122790 error=0.108400 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:32 downhill.base:232 Adam 1182 loss=0.122750 error=0.108389 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:32 downhill.base:232 Adam 1183 loss=0.122710 error=0.108379 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:32 downhill.base:232 Adam 1184 loss=0.122670 error=0.108368 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:32 downhill.base:232 Adam 1185 loss=0.122630 error=0.108357 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:32 downhill.base:232 Adam 1186 loss=0.122591 error=0.108347 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:33 downhill.base:232 Adam 1187 loss=0.122551 error=0.108336 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:33 downhill.base:232 Adam 1188 loss=0.122512 error=0.108326 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:33 downhill.base:232 Adam 1189 loss=0.122473 error=0.108315 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:33 downhill.base:232 Adam 1190 loss=0.122433 error=0.108305 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:33 downhill.base:232 validation 119 loss=0.122394 error=0.108295 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:34 downhill.base:232 Adam 1191 loss=0.122394 error=0.108295 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:34 downhill.base:232 Adam 1192 loss=0.122355 error=0.108284 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:34 downhill.base:232 Adam 1193 loss=0.122316 error=0.108274 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:34 downhill.base:232 Adam 1194 loss=0.122277 error=0.108264 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:34 downhill.base:232 Adam 1195 loss=0.122238 error=0.108254 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:35 downhill.base:232 Adam 1196 loss=0.122199 error=0.108244 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:35 downhill.base:232 Adam 1197 loss=0.122161 error=0.108234 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:35 downhill.base:232 Adam 1198 loss=0.122122 error=0.108223 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:35 downhill.base:232 Adam 1199 loss=0.122084 error=0.108213 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:35 downhill.base:232 Adam 1200 loss=0.122045 error=0.108203 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:36 downhill.base:232 validation 120 loss=0.122007 error=0.108194 grad(U)=0.000000 grad(V)=0.000002 *\n",
      "I 2016-12-18 00:13:36 downhill.base:232 Adam 1201 loss=0.122007 error=0.108194 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:36 downhill.base:232 Adam 1202 loss=0.121968 error=0.108184 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:36 downhill.base:232 Adam 1203 loss=0.121930 error=0.108174 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:36 downhill.base:232 Adam 1204 loss=0.121892 error=0.108164 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:37 downhill.base:232 Adam 1205 loss=0.121854 error=0.108154 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:37 downhill.base:232 Adam 1206 loss=0.121816 error=0.108144 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:37 downhill.base:232 Adam 1207 loss=0.121779 error=0.108135 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:37 downhill.base:232 Adam 1208 loss=0.121741 error=0.108125 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:37 downhill.base:232 Adam 1209 loss=0.121703 error=0.108115 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:38 downhill.base:232 Adam 1210 loss=0.121666 error=0.108106 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:38 downhill.base:232 validation 121 loss=0.121628 error=0.108096 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:38 downhill.base:232 Adam 1211 loss=0.121628 error=0.108096 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:38 downhill.base:232 Adam 1212 loss=0.121591 error=0.108086 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:38 downhill.base:232 Adam 1213 loss=0.121553 error=0.108077 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:39 downhill.base:232 Adam 1214 loss=0.121516 error=0.108067 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:39 downhill.base:232 Adam 1215 loss=0.121479 error=0.108058 grad(U)=0.000000 grad(V)=0.000002\n",
      "I 2016-12-18 00:13:39 downhill.base:232 Adam 1216 loss=0.121442 error=0.108049 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:39 downhill.base:232 Adam 1217 loss=0.121405 error=0.108039 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:39 downhill.base:232 Adam 1218 loss=0.121368 error=0.108030 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:40 downhill.base:232 Adam 1219 loss=0.121331 error=0.108021 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:40 downhill.base:232 Adam 1220 loss=0.121294 error=0.108011 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:40 downhill.base:232 validation 122 loss=0.121257 error=0.108002 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:13:40 downhill.base:232 Adam 1221 loss=0.121257 error=0.108002 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:40 downhill.base:232 Adam 1222 loss=0.121221 error=0.107993 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:40 downhill.base:232 Adam 1223 loss=0.121184 error=0.107984 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:41 downhill.base:232 Adam 1224 loss=0.121148 error=0.107975 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:41 downhill.base:232 Adam 1225 loss=0.121111 error=0.107966 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:41 downhill.base:232 Adam 1226 loss=0.121075 error=0.107957 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:41 downhill.base:232 Adam 1227 loss=0.121039 error=0.107948 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:41 downhill.base:232 Adam 1228 loss=0.121003 error=0.107939 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:42 downhill.base:232 Adam 1229 loss=0.120967 error=0.107930 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:42 downhill.base:232 Adam 1230 loss=0.120931 error=0.107921 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:42 downhill.base:232 validation 123 loss=0.120895 error=0.107912 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:42 downhill.base:232 Adam 1231 loss=0.120895 error=0.107912 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:42 downhill.base:232 Adam 1232 loss=0.120859 error=0.107903 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:43 downhill.base:232 Adam 1233 loss=0.120823 error=0.107894 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:43 downhill.base:232 Adam 1234 loss=0.120788 error=0.107886 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:43 downhill.base:232 Adam 1235 loss=0.120752 error=0.107877 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:43 downhill.base:232 Adam 1236 loss=0.120717 error=0.107868 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:43 downhill.base:232 Adam 1237 loss=0.120681 error=0.107859 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:44 downhill.base:232 Adam 1238 loss=0.120646 error=0.107851 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:44 downhill.base:232 Adam 1239 loss=0.120611 error=0.107842 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:44 downhill.base:232 Adam 1240 loss=0.120576 error=0.107834 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:44 downhill.base:232 validation 124 loss=0.120540 error=0.107825 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:13:44 downhill.base:232 Adam 1241 loss=0.120540 error=0.107825 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:45 downhill.base:232 Adam 1242 loss=0.120505 error=0.107817 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:45 downhill.base:232 Adam 1243 loss=0.120470 error=0.107808 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:45 downhill.base:232 Adam 1244 loss=0.120435 error=0.107800 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:45 downhill.base:232 Adam 1245 loss=0.120401 error=0.107792 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:45 downhill.base:232 Adam 1246 loss=0.120366 error=0.107783 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:46 downhill.base:232 Adam 1247 loss=0.120331 error=0.107775 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:46 downhill.base:232 Adam 1248 loss=0.120297 error=0.107767 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:46 downhill.base:232 Adam 1249 loss=0.120262 error=0.107758 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:46 downhill.base:232 Adam 1250 loss=0.120228 error=0.107750 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:46 downhill.base:232 validation 125 loss=0.120194 error=0.107742 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:47 downhill.base:232 Adam 1251 loss=0.120194 error=0.107742 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:47 downhill.base:232 Adam 1252 loss=0.120160 error=0.107734 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:47 downhill.base:232 Adam 1253 loss=0.120125 error=0.107726 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:47 downhill.base:232 Adam 1254 loss=0.120091 error=0.107718 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:47 downhill.base:232 Adam 1255 loss=0.120057 error=0.107710 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:48 downhill.base:232 Adam 1256 loss=0.120023 error=0.107702 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:48 downhill.base:232 Adam 1257 loss=0.119989 error=0.107694 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:48 downhill.base:232 Adam 1258 loss=0.119956 error=0.107686 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:48 downhill.base:232 Adam 1259 loss=0.119922 error=0.107678 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:48 downhill.base:232 Adam 1260 loss=0.119888 error=0.107670 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:49 downhill.base:232 validation 126 loss=0.119855 error=0.107662 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:13:49 downhill.base:232 Adam 1261 loss=0.119855 error=0.107662 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:49 downhill.base:232 Adam 1262 loss=0.119821 error=0.107654 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:49 downhill.base:232 Adam 1263 loss=0.119788 error=0.107646 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:49 downhill.base:232 Adam 1264 loss=0.119754 error=0.107639 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:50 downhill.base:232 Adam 1265 loss=0.119721 error=0.107631 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:50 downhill.base:232 Adam 1266 loss=0.119688 error=0.107623 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:50 downhill.base:232 Adam 1267 loss=0.119655 error=0.107616 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:50 downhill.base:232 Adam 1268 loss=0.119622 error=0.107608 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:50 downhill.base:232 Adam 1269 loss=0.119589 error=0.107600 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:50 downhill.base:232 Adam 1270 loss=0.119556 error=0.107593 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:51 downhill.base:232 validation 127 loss=0.119523 error=0.107585 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:51 downhill.base:232 Adam 1271 loss=0.119523 error=0.107585 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:51 downhill.base:232 Adam 1272 loss=0.119491 error=0.107578 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:51 downhill.base:232 Adam 1273 loss=0.119458 error=0.107570 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:51 downhill.base:232 Adam 1274 loss=0.119425 error=0.107563 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:52 downhill.base:232 Adam 1275 loss=0.119393 error=0.107555 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:52 downhill.base:232 Adam 1276 loss=0.119360 error=0.107548 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:52 downhill.base:232 Adam 1277 loss=0.119328 error=0.107541 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:52 downhill.base:232 Adam 1278 loss=0.119296 error=0.107533 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:52 downhill.base:232 Adam 1279 loss=0.119263 error=0.107526 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:53 downhill.base:232 Adam 1280 loss=0.119231 error=0.107519 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:53 downhill.base:232 validation 128 loss=0.119199 error=0.107511 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:13:53 downhill.base:232 Adam 1281 loss=0.119199 error=0.107511 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:53 downhill.base:232 Adam 1282 loss=0.119167 error=0.107504 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:53 downhill.base:232 Adam 1283 loss=0.119135 error=0.107497 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:54 downhill.base:232 Adam 1284 loss=0.119103 error=0.107490 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:54 downhill.base:232 Adam 1285 loss=0.119072 error=0.107483 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:54 downhill.base:232 Adam 1286 loss=0.119040 error=0.107475 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:54 downhill.base:232 Adam 1287 loss=0.119008 error=0.107468 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:54 downhill.base:232 Adam 1288 loss=0.118977 error=0.107461 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:55 downhill.base:232 Adam 1289 loss=0.118945 error=0.107454 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:55 downhill.base:232 Adam 1290 loss=0.118913 error=0.107447 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:55 downhill.base:232 validation 129 loss=0.118882 error=0.107440 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:55 downhill.base:232 Adam 1291 loss=0.118882 error=0.107440 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:55 downhill.base:232 Adam 1292 loss=0.118851 error=0.107433 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:56 downhill.base:232 Adam 1293 loss=0.118820 error=0.107427 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:56 downhill.base:232 Adam 1294 loss=0.118788 error=0.107420 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:56 downhill.base:232 Adam 1295 loss=0.118757 error=0.107413 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:56 downhill.base:232 Adam 1296 loss=0.118726 error=0.107406 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:56 downhill.base:232 Adam 1297 loss=0.118696 error=0.107399 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:57 downhill.base:232 Adam 1298 loss=0.118665 error=0.107392 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:57 downhill.base:232 Adam 1299 loss=0.118634 error=0.107386 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:57 downhill.base:232 Adam 1300 loss=0.118603 error=0.107379 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:57 downhill.base:232 validation 130 loss=0.118573 error=0.107372 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:13:57 downhill.base:232 Adam 1301 loss=0.118573 error=0.107372 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:58 downhill.base:232 Adam 1302 loss=0.118542 error=0.107366 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:58 downhill.base:232 Adam 1303 loss=0.118511 error=0.107359 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:58 downhill.base:232 Adam 1304 loss=0.118481 error=0.107352 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:58 downhill.base:232 Adam 1305 loss=0.118451 error=0.107346 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:58 downhill.base:232 Adam 1306 loss=0.118420 error=0.107339 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:59 downhill.base:232 Adam 1307 loss=0.118390 error=0.107333 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:59 downhill.base:232 Adam 1308 loss=0.118360 error=0.107326 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:59 downhill.base:232 Adam 1309 loss=0.118330 error=0.107320 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:59 downhill.base:232 Adam 1310 loss=0.118300 error=0.107313 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:13:59 downhill.base:232 validation 131 loss=0.118270 error=0.107307 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:00 downhill.base:232 Adam 1311 loss=0.118270 error=0.107307 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:00 downhill.base:232 Adam 1312 loss=0.118240 error=0.107300 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:00 downhill.base:232 Adam 1313 loss=0.118210 error=0.107294 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:00 downhill.base:232 Adam 1314 loss=0.118181 error=0.107288 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:00 downhill.base:232 Adam 1315 loss=0.118151 error=0.107281 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:01 downhill.base:232 Adam 1316 loss=0.118121 error=0.107275 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:01 downhill.base:232 Adam 1317 loss=0.118092 error=0.107269 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:01 downhill.base:232 Adam 1318 loss=0.118062 error=0.107263 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:01 downhill.base:232 Adam 1319 loss=0.118033 error=0.107256 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:01 downhill.base:232 Adam 1320 loss=0.118004 error=0.107250 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:01 downhill.base:232 validation 132 loss=0.117974 error=0.107244 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:14:02 downhill.base:232 Adam 1321 loss=0.117974 error=0.107244 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:02 downhill.base:232 Adam 1322 loss=0.117945 error=0.107238 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:02 downhill.base:232 Adam 1323 loss=0.117916 error=0.107232 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:02 downhill.base:232 Adam 1324 loss=0.117887 error=0.107226 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:02 downhill.base:232 Adam 1325 loss=0.117858 error=0.107220 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:03 downhill.base:232 Adam 1326 loss=0.117829 error=0.107214 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:03 downhill.base:232 Adam 1327 loss=0.117800 error=0.107208 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:03 downhill.base:232 Adam 1328 loss=0.117771 error=0.107202 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:03 downhill.base:232 Adam 1329 loss=0.117742 error=0.107196 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:03 downhill.base:232 Adam 1330 loss=0.117714 error=0.107190 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:04 downhill.base:232 validation 133 loss=0.117685 error=0.107184 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:04 downhill.base:232 Adam 1331 loss=0.117685 error=0.107184 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:04 downhill.base:232 Adam 1332 loss=0.117657 error=0.107178 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:04 downhill.base:232 Adam 1333 loss=0.117628 error=0.107172 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:04 downhill.base:232 Adam 1334 loss=0.117600 error=0.107166 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:05 downhill.base:232 Adam 1335 loss=0.117571 error=0.107160 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:05 downhill.base:232 Adam 1336 loss=0.117543 error=0.107154 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:05 downhill.base:232 Adam 1337 loss=0.117515 error=0.107149 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:05 downhill.base:232 Adam 1338 loss=0.117487 error=0.107143 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:05 downhill.base:232 Adam 1339 loss=0.117459 error=0.107137 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:06 downhill.base:232 Adam 1340 loss=0.117430 error=0.107131 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:06 downhill.base:232 validation 134 loss=0.117402 error=0.107126 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:06 downhill.base:232 Adam 1341 loss=0.117402 error=0.107126 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:06 downhill.base:232 Adam 1342 loss=0.117375 error=0.107120 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:06 downhill.base:232 Adam 1343 loss=0.117347 error=0.107114 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:07 downhill.base:232 Adam 1344 loss=0.117319 error=0.107109 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:07 downhill.base:232 Adam 1345 loss=0.117291 error=0.107103 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:07 downhill.base:232 Adam 1346 loss=0.117264 error=0.107098 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:07 downhill.base:232 Adam 1347 loss=0.117236 error=0.107092 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:07 downhill.base:232 Adam 1348 loss=0.117209 error=0.107087 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:08 downhill.base:232 Adam 1349 loss=0.117181 error=0.107081 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:08 downhill.base:232 Adam 1350 loss=0.117154 error=0.107076 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:08 downhill.base:232 validation 135 loss=0.117126 error=0.107070 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:14:08 downhill.base:232 Adam 1351 loss=0.117126 error=0.107070 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:08 downhill.base:232 Adam 1352 loss=0.117099 error=0.107065 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:09 downhill.base:232 Adam 1353 loss=0.117072 error=0.107059 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:09 downhill.base:232 Adam 1354 loss=0.117045 error=0.107054 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:09 downhill.base:232 Adam 1355 loss=0.117018 error=0.107048 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:09 downhill.base:232 Adam 1356 loss=0.116990 error=0.107043 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:09 downhill.base:232 Adam 1357 loss=0.116964 error=0.107038 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:10 downhill.base:232 Adam 1358 loss=0.116937 error=0.107033 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:10 downhill.base:232 Adam 1359 loss=0.116910 error=0.107027 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:10 downhill.base:232 Adam 1360 loss=0.116883 error=0.107022 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:10 downhill.base:232 validation 136 loss=0.116856 error=0.107017 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:10 downhill.base:232 Adam 1361 loss=0.116856 error=0.107017 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:11 downhill.base:232 Adam 1362 loss=0.116830 error=0.107011 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:11 downhill.base:232 Adam 1363 loss=0.116803 error=0.107006 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:11 downhill.base:232 Adam 1364 loss=0.116776 error=0.107001 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:11 downhill.base:232 Adam 1365 loss=0.116750 error=0.106996 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:11 downhill.base:232 Adam 1366 loss=0.116723 error=0.106991 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:12 downhill.base:232 Adam 1367 loss=0.116697 error=0.106986 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:12 downhill.base:232 Adam 1368 loss=0.116671 error=0.106981 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:12 downhill.base:232 Adam 1369 loss=0.116644 error=0.106976 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:12 downhill.base:232 Adam 1370 loss=0.116618 error=0.106971 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:12 downhill.base:232 validation 137 loss=0.116592 error=0.106965 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:12 downhill.base:232 Adam 1371 loss=0.116592 error=0.106965 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:13 downhill.base:232 Adam 1372 loss=0.116566 error=0.106960 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:13 downhill.base:232 Adam 1373 loss=0.116540 error=0.106955 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:13 downhill.base:232 Adam 1374 loss=0.116514 error=0.106950 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:13 downhill.base:232 Adam 1375 loss=0.116488 error=0.106946 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:13 downhill.base:232 Adam 1376 loss=0.116462 error=0.106941 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:14 downhill.base:232 Adam 1377 loss=0.116436 error=0.106936 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:14 downhill.base:232 Adam 1378 loss=0.116411 error=0.106931 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:14 downhill.base:232 Adam 1379 loss=0.116385 error=0.106926 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:14 downhill.base:232 Adam 1380 loss=0.116359 error=0.106921 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:14 downhill.base:232 validation 138 loss=0.116334 error=0.106916 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:14:15 downhill.base:232 Adam 1381 loss=0.116334 error=0.106916 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:15 downhill.base:232 Adam 1382 loss=0.116308 error=0.106911 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:15 downhill.base:232 Adam 1383 loss=0.116283 error=0.106907 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:15 downhill.base:232 Adam 1384 loss=0.116258 error=0.106902 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:15 downhill.base:232 Adam 1385 loss=0.116232 error=0.106897 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:16 downhill.base:232 Adam 1386 loss=0.116207 error=0.106892 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:16 downhill.base:232 Adam 1387 loss=0.116182 error=0.106888 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:16 downhill.base:232 Adam 1388 loss=0.116157 error=0.106883 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:16 downhill.base:232 Adam 1389 loss=0.116131 error=0.106878 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:16 downhill.base:232 Adam 1390 loss=0.116106 error=0.106874 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:17 downhill.base:232 validation 139 loss=0.116081 error=0.106869 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:17 downhill.base:232 Adam 1391 loss=0.116081 error=0.106869 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:17 downhill.base:232 Adam 1392 loss=0.116057 error=0.106864 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:17 downhill.base:232 Adam 1393 loss=0.116032 error=0.106860 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:17 downhill.base:232 Adam 1394 loss=0.116007 error=0.106855 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:18 downhill.base:232 Adam 1395 loss=0.115982 error=0.106851 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:18 downhill.base:232 Adam 1396 loss=0.115957 error=0.106846 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:18 downhill.base:232 Adam 1397 loss=0.115933 error=0.106842 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:18 downhill.base:232 Adam 1398 loss=0.115908 error=0.106837 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:18 downhill.base:232 Adam 1399 loss=0.115884 error=0.106833 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:19 downhill.base:232 Adam 1400 loss=0.115859 error=0.106828 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:19 downhill.base:232 validation 140 loss=0.115835 error=0.106824 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:19 downhill.base:232 Adam 1401 loss=0.115835 error=0.106824 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:19 downhill.base:232 Adam 1402 loss=0.115811 error=0.106819 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:19 downhill.base:232 Adam 1403 loss=0.115786 error=0.106815 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:20 downhill.base:232 Adam 1404 loss=0.115762 error=0.106811 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:20 downhill.base:232 Adam 1405 loss=0.115738 error=0.106806 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:20 downhill.base:232 Adam 1406 loss=0.115714 error=0.106802 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:20 downhill.base:232 Adam 1407 loss=0.115690 error=0.106798 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:20 downhill.base:232 Adam 1408 loss=0.115666 error=0.106793 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:21 downhill.base:232 Adam 1409 loss=0.115642 error=0.106789 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:21 downhill.base:232 Adam 1410 loss=0.115618 error=0.106785 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:21 downhill.base:232 validation 141 loss=0.115594 error=0.106780 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:14:21 downhill.base:232 Adam 1411 loss=0.115594 error=0.106780 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:21 downhill.base:232 Adam 1412 loss=0.115570 error=0.106776 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:22 downhill.base:232 Adam 1413 loss=0.115547 error=0.106772 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:22 downhill.base:232 Adam 1414 loss=0.115523 error=0.106768 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:22 downhill.base:232 Adam 1415 loss=0.115499 error=0.106764 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:22 downhill.base:232 Adam 1416 loss=0.115476 error=0.106759 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:22 downhill.base:232 Adam 1417 loss=0.115452 error=0.106755 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:23 downhill.base:232 Adam 1418 loss=0.115429 error=0.106751 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:23 downhill.base:232 Adam 1419 loss=0.115405 error=0.106747 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:23 downhill.base:232 Adam 1420 loss=0.115382 error=0.106743 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:23 downhill.base:232 validation 142 loss=0.115359 error=0.106739 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:23 downhill.base:232 Adam 1421 loss=0.115359 error=0.106739 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:23 downhill.base:232 Adam 1422 loss=0.115336 error=0.106735 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:24 downhill.base:232 Adam 1423 loss=0.115312 error=0.106731 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:24 downhill.base:232 Adam 1424 loss=0.115289 error=0.106727 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:24 downhill.base:232 Adam 1425 loss=0.115266 error=0.106723 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:24 downhill.base:232 Adam 1426 loss=0.115243 error=0.106719 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:24 downhill.base:232 Adam 1427 loss=0.115220 error=0.106715 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:25 downhill.base:232 Adam 1428 loss=0.115197 error=0.106711 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:25 downhill.base:232 Adam 1429 loss=0.115174 error=0.106707 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:25 downhill.base:232 Adam 1430 loss=0.115151 error=0.106703 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:25 downhill.base:232 validation 143 loss=0.115129 error=0.106699 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:25 downhill.base:232 Adam 1431 loss=0.115129 error=0.106699 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:26 downhill.base:232 Adam 1432 loss=0.115106 error=0.106695 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:26 downhill.base:232 Adam 1433 loss=0.115083 error=0.106691 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:26 downhill.base:232 Adam 1434 loss=0.115060 error=0.106687 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:26 downhill.base:232 Adam 1435 loss=0.115038 error=0.106684 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:26 downhill.base:232 Adam 1436 loss=0.115015 error=0.106680 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:27 downhill.base:232 Adam 1437 loss=0.114993 error=0.106676 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:27 downhill.base:232 Adam 1438 loss=0.114970 error=0.106672 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:27 downhill.base:232 Adam 1439 loss=0.114948 error=0.106668 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:27 downhill.base:232 Adam 1440 loss=0.114926 error=0.106665 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:27 downhill.base:232 validation 144 loss=0.114903 error=0.106661 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:14:28 downhill.base:232 Adam 1441 loss=0.114903 error=0.106661 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:28 downhill.base:232 Adam 1442 loss=0.114881 error=0.106657 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:28 downhill.base:232 Adam 1443 loss=0.114859 error=0.106653 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:28 downhill.base:232 Adam 1444 loss=0.114837 error=0.106650 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:28 downhill.base:232 Adam 1445 loss=0.114815 error=0.106646 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:29 downhill.base:232 Adam 1446 loss=0.114793 error=0.106642 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:29 downhill.base:232 Adam 1447 loss=0.114771 error=0.106639 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:29 downhill.base:232 Adam 1448 loss=0.114749 error=0.106635 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:29 downhill.base:232 Adam 1449 loss=0.114727 error=0.106632 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:29 downhill.base:232 Adam 1450 loss=0.114705 error=0.106628 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:30 downhill.base:232 validation 145 loss=0.114683 error=0.106624 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:30 downhill.base:232 Adam 1451 loss=0.114683 error=0.106624 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:30 downhill.base:232 Adam 1452 loss=0.114662 error=0.106621 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:30 downhill.base:232 Adam 1453 loss=0.114640 error=0.106617 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:30 downhill.base:232 Adam 1454 loss=0.114618 error=0.106614 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:31 downhill.base:232 Adam 1455 loss=0.114597 error=0.106610 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:31 downhill.base:232 Adam 1456 loss=0.114575 error=0.106607 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:31 downhill.base:232 Adam 1457 loss=0.114554 error=0.106603 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:31 downhill.base:232 Adam 1458 loss=0.114532 error=0.106600 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:31 downhill.base:232 Adam 1459 loss=0.114511 error=0.106596 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:32 downhill.base:232 Adam 1460 loss=0.114490 error=0.106593 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:32 downhill.base:232 validation 146 loss=0.114468 error=0.106590 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:32 downhill.base:232 Adam 1461 loss=0.114468 error=0.106590 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:32 downhill.base:232 Adam 1462 loss=0.114447 error=0.106586 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:32 downhill.base:232 Adam 1463 loss=0.114426 error=0.106583 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:33 downhill.base:232 Adam 1464 loss=0.114405 error=0.106579 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:33 downhill.base:232 Adam 1465 loss=0.114384 error=0.106576 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:33 downhill.base:232 Adam 1466 loss=0.114363 error=0.106573 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:33 downhill.base:232 Adam 1467 loss=0.114342 error=0.106569 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:33 downhill.base:232 Adam 1468 loss=0.114321 error=0.106566 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:34 downhill.base:232 Adam 1469 loss=0.114300 error=0.106563 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:34 downhill.base:232 Adam 1470 loss=0.114279 error=0.106559 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:34 downhill.base:232 validation 147 loss=0.114259 error=0.106556 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:14:34 downhill.base:232 Adam 1471 loss=0.114259 error=0.106556 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:34 downhill.base:232 Adam 1472 loss=0.114238 error=0.106553 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:35 downhill.base:232 Adam 1473 loss=0.114217 error=0.106550 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:35 downhill.base:232 Adam 1474 loss=0.114197 error=0.106547 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:35 downhill.base:232 Adam 1475 loss=0.114176 error=0.106543 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:35 downhill.base:232 Adam 1476 loss=0.114156 error=0.106540 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:35 downhill.base:232 Adam 1477 loss=0.114135 error=0.106537 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:36 downhill.base:232 Adam 1478 loss=0.114115 error=0.106534 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:36 downhill.base:232 Adam 1479 loss=0.114094 error=0.106531 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:36 downhill.base:232 Adam 1480 loss=0.114074 error=0.106527 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:36 downhill.base:232 validation 148 loss=0.114054 error=0.106524 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:36 downhill.base:232 Adam 1481 loss=0.114054 error=0.106524 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:36 downhill.base:232 Adam 1482 loss=0.114033 error=0.106521 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:37 downhill.base:232 Adam 1483 loss=0.114013 error=0.106518 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:37 downhill.base:232 Adam 1484 loss=0.113993 error=0.106515 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:37 downhill.base:232 Adam 1485 loss=0.113973 error=0.106512 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:37 downhill.base:232 Adam 1486 loss=0.113953 error=0.106509 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:37 downhill.base:232 Adam 1487 loss=0.113933 error=0.106506 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:38 downhill.base:232 Adam 1488 loss=0.113913 error=0.106503 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:38 downhill.base:232 Adam 1489 loss=0.113893 error=0.106500 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:38 downhill.base:232 Adam 1490 loss=0.113873 error=0.106497 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:38 downhill.base:232 validation 149 loss=0.113853 error=0.106494 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:38 downhill.base:232 Adam 1491 loss=0.113853 error=0.106494 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:39 downhill.base:232 Adam 1492 loss=0.113833 error=0.106491 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:39 downhill.base:232 Adam 1493 loss=0.113813 error=0.106488 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:39 downhill.base:232 Adam 1494 loss=0.113794 error=0.106485 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:39 downhill.base:232 Adam 1495 loss=0.113774 error=0.106482 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:39 downhill.base:232 Adam 1496 loss=0.113754 error=0.106479 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:40 downhill.base:232 Adam 1497 loss=0.113735 error=0.106476 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:40 downhill.base:232 Adam 1498 loss=0.113715 error=0.106473 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:40 downhill.base:232 Adam 1499 loss=0.113696 error=0.106470 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:40 downhill.base:232 Adam 1500 loss=0.113676 error=0.106467 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:40 downhill.base:232 validation 150 loss=0.113657 error=0.106465 grad(U)=0.000000 grad(V)=0.000001 *\n",
      "I 2016-12-18 00:14:41 downhill.base:232 Adam 1501 loss=0.113657 error=0.106465 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:41 downhill.base:232 Adam 1502 loss=0.113638 error=0.106462 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:41 downhill.base:232 Adam 1503 loss=0.113619 error=0.106459 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:41 downhill.base:232 Adam 1504 loss=0.113599 error=0.106456 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:41 downhill.base:232 Adam 1505 loss=0.113580 error=0.106453 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:42 downhill.base:232 Adam 1506 loss=0.113561 error=0.106450 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:42 downhill.base:232 Adam 1507 loss=0.113542 error=0.106448 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:42 downhill.base:232 Adam 1508 loss=0.113522 error=0.106445 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:42 downhill.base:232 Adam 1509 loss=0.113503 error=0.106442 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:42 downhill.base:232 Adam 1510 loss=0.113484 error=0.106439 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:43 downhill.base:232 validation 151 loss=0.113465 error=0.106437 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:43 downhill.base:232 Adam 1511 loss=0.113465 error=0.106437 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:43 downhill.base:232 Adam 1512 loss=0.113446 error=0.106434 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:43 downhill.base:232 Adam 1513 loss=0.113427 error=0.106431 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:43 downhill.base:232 Adam 1514 loss=0.113409 error=0.106429 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:44 downhill.base:232 Adam 1515 loss=0.113390 error=0.106426 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:44 downhill.base:232 Adam 1516 loss=0.113371 error=0.106423 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:44 downhill.base:232 Adam 1517 loss=0.113352 error=0.106421 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:44 downhill.base:232 Adam 1518 loss=0.113334 error=0.106418 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:44 downhill.base:232 Adam 1519 loss=0.113315 error=0.106415 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:45 downhill.base:232 Adam 1520 loss=0.113296 error=0.106413 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:45 downhill.base:232 validation 152 loss=0.113278 error=0.106410 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:45 downhill.base:232 Adam 1521 loss=0.113278 error=0.106410 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:45 downhill.base:232 Adam 1522 loss=0.113259 error=0.106407 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:45 downhill.base:232 Adam 1523 loss=0.113241 error=0.106405 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:46 downhill.base:232 Adam 1524 loss=0.113223 error=0.106402 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:46 downhill.base:232 Adam 1525 loss=0.113204 error=0.106400 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:46 downhill.base:232 Adam 1526 loss=0.113186 error=0.106397 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:46 downhill.base:232 Adam 1527 loss=0.113168 error=0.106395 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:46 downhill.base:232 Adam 1528 loss=0.113149 error=0.106392 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:47 downhill.base:232 Adam 1529 loss=0.113131 error=0.106389 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:47 downhill.base:232 Adam 1530 loss=0.113113 error=0.106387 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:47 downhill.base:232 validation 153 loss=0.113095 error=0.106384 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:47 downhill.base:232 Adam 1531 loss=0.113095 error=0.106384 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:47 downhill.base:232 Adam 1532 loss=0.113077 error=0.106382 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:47 downhill.base:232 Adam 1533 loss=0.113059 error=0.106379 grad(U)=0.000000 grad(V)=0.000001\n",
      "I 2016-12-18 00:14:48 downhill.base:232 Adam 1534 loss=0.113041 error=0.106377 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:48 downhill.base:232 Adam 1535 loss=0.113023 error=0.106375 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:48 downhill.base:232 Adam 1536 loss=0.113005 error=0.106372 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:48 downhill.base:232 Adam 1537 loss=0.112987 error=0.106370 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:48 downhill.base:232 Adam 1538 loss=0.112969 error=0.106367 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:49 downhill.base:232 Adam 1539 loss=0.112952 error=0.106365 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:49 downhill.base:232 Adam 1540 loss=0.112934 error=0.106362 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:49 downhill.base:232 validation 154 loss=0.112916 error=0.106360 grad(U)=0.000000 grad(V)=0.000000 *\n",
      "I 2016-12-18 00:14:49 downhill.base:232 Adam 1541 loss=0.112916 error=0.106360 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:49 downhill.base:232 Adam 1542 loss=0.112899 error=0.106358 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:50 downhill.base:232 Adam 1543 loss=0.112881 error=0.106355 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:50 downhill.base:232 Adam 1544 loss=0.112863 error=0.106353 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:50 downhill.base:232 Adam 1545 loss=0.112846 error=0.106351 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:50 downhill.base:232 Adam 1546 loss=0.112828 error=0.106348 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:50 downhill.base:232 Adam 1547 loss=0.112811 error=0.106346 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:51 downhill.base:232 Adam 1548 loss=0.112793 error=0.106344 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:51 downhill.base:232 Adam 1549 loss=0.112776 error=0.106341 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:51 downhill.base:232 Adam 1550 loss=0.112758 error=0.106339 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:51 downhill.base:232 validation 155 loss=0.112741 error=0.106337 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:52 downhill.base:232 Adam 1551 loss=0.112741 error=0.106337 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:52 downhill.base:232 Adam 1552 loss=0.112724 error=0.106334 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:52 downhill.base:232 Adam 1553 loss=0.112707 error=0.106332 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:52 downhill.base:232 Adam 1554 loss=0.112689 error=0.106330 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:53 downhill.base:232 Adam 1555 loss=0.112672 error=0.106328 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:53 downhill.base:232 Adam 1556 loss=0.112655 error=0.106325 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:53 downhill.base:232 Adam 1557 loss=0.112638 error=0.106323 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:53 downhill.base:232 Adam 1558 loss=0.112621 error=0.106321 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:53 downhill.base:232 Adam 1559 loss=0.112604 error=0.106319 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:54 downhill.base:232 Adam 1560 loss=0.112587 error=0.106317 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:54 downhill.base:232 validation 156 loss=0.112570 error=0.106314 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:54 downhill.base:232 Adam 1561 loss=0.112570 error=0.106314 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:54 downhill.base:232 Adam 1562 loss=0.112553 error=0.106312 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:54 downhill.base:232 Adam 1563 loss=0.112537 error=0.106310 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:55 downhill.base:232 Adam 1564 loss=0.112520 error=0.106308 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:55 downhill.base:232 Adam 1565 loss=0.112503 error=0.106306 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:55 downhill.base:232 Adam 1566 loss=0.112486 error=0.106303 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:55 downhill.base:232 Adam 1567 loss=0.112470 error=0.106301 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:55 downhill.base:232 Adam 1568 loss=0.112453 error=0.106299 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:56 downhill.base:232 Adam 1569 loss=0.112437 error=0.106297 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:56 downhill.base:232 Adam 1570 loss=0.112420 error=0.106295 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:56 downhill.base:232 validation 157 loss=0.112404 error=0.106293 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:56 downhill.base:232 Adam 1571 loss=0.112404 error=0.106293 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:56 downhill.base:232 Adam 1572 loss=0.112387 error=0.106291 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:56 downhill.base:232 Adam 1573 loss=0.112371 error=0.106289 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:57 downhill.base:232 Adam 1574 loss=0.112354 error=0.106287 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:57 downhill.base:232 Adam 1575 loss=0.112338 error=0.106285 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:57 downhill.base:232 Adam 1576 loss=0.112322 error=0.106283 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:57 downhill.base:232 Adam 1577 loss=0.112305 error=0.106281 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:57 downhill.base:232 Adam 1578 loss=0.112289 error=0.106279 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:58 downhill.base:232 Adam 1579 loss=0.112273 error=0.106277 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:58 downhill.base:232 Adam 1580 loss=0.112257 error=0.106275 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:58 downhill.base:232 validation 158 loss=0.112241 error=0.106273 grad(U)=0.000000 grad(V)=0.000000 *\n",
      "I 2016-12-18 00:14:58 downhill.base:232 Adam 1581 loss=0.112241 error=0.106273 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:58 downhill.base:232 Adam 1582 loss=0.112225 error=0.106271 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:59 downhill.base:232 Adam 1583 loss=0.112209 error=0.106269 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:59 downhill.base:232 Adam 1584 loss=0.112192 error=0.106267 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:59 downhill.base:232 Adam 1585 loss=0.112176 error=0.106265 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:59 downhill.base:232 Adam 1586 loss=0.112161 error=0.106263 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:14:59 downhill.base:232 Adam 1587 loss=0.112145 error=0.106261 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:00 downhill.base:232 Adam 1588 loss=0.112129 error=0.106259 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:00 downhill.base:232 Adam 1589 loss=0.112113 error=0.106257 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:00 downhill.base:232 Adam 1590 loss=0.112097 error=0.106255 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:00 downhill.base:232 validation 159 loss=0.112082 error=0.106253 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:00 downhill.base:232 Adam 1591 loss=0.112082 error=0.106253 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:01 downhill.base:232 Adam 1592 loss=0.112066 error=0.106251 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:01 downhill.base:232 Adam 1593 loss=0.112050 error=0.106249 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:01 downhill.base:232 Adam 1594 loss=0.112035 error=0.106247 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:01 downhill.base:232 Adam 1595 loss=0.112019 error=0.106246 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:01 downhill.base:232 Adam 1596 loss=0.112003 error=0.106244 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:02 downhill.base:232 Adam 1597 loss=0.111988 error=0.106242 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:02 downhill.base:232 Adam 1598 loss=0.111972 error=0.106240 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:02 downhill.base:232 Adam 1599 loss=0.111957 error=0.106238 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:02 downhill.base:232 Adam 1600 loss=0.111941 error=0.106236 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:02 downhill.base:232 validation 160 loss=0.111926 error=0.106235 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:03 downhill.base:232 Adam 1601 loss=0.111926 error=0.106235 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:03 downhill.base:232 Adam 1602 loss=0.111911 error=0.106233 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:03 downhill.base:232 Adam 1603 loss=0.111895 error=0.106231 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:03 downhill.base:232 Adam 1604 loss=0.111880 error=0.106229 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:03 downhill.base:232 Adam 1605 loss=0.111865 error=0.106227 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:04 downhill.base:232 Adam 1606 loss=0.111850 error=0.106226 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:04 downhill.base:232 Adam 1607 loss=0.111834 error=0.106224 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:04 downhill.base:232 Adam 1608 loss=0.111819 error=0.106222 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:04 downhill.base:232 Adam 1609 loss=0.111804 error=0.106220 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:04 downhill.base:232 Adam 1610 loss=0.111789 error=0.106219 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:05 downhill.base:232 validation 161 loss=0.111774 error=0.106217 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:05 downhill.base:232 Adam 1611 loss=0.111774 error=0.106217 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:05 downhill.base:232 Adam 1612 loss=0.111759 error=0.106215 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:05 downhill.base:232 Adam 1613 loss=0.111744 error=0.106213 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:05 downhill.base:232 Adam 1614 loss=0.111729 error=0.106212 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:06 downhill.base:232 Adam 1615 loss=0.111714 error=0.106210 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:06 downhill.base:232 Adam 1616 loss=0.111699 error=0.106208 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:06 downhill.base:232 Adam 1617 loss=0.111684 error=0.106207 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:06 downhill.base:232 Adam 1618 loss=0.111669 error=0.106205 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:07 downhill.base:232 Adam 1619 loss=0.111654 error=0.106203 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:07 downhill.base:232 Adam 1620 loss=0.111639 error=0.106202 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:07 downhill.base:232 validation 162 loss=0.111625 error=0.106200 grad(U)=0.000000 grad(V)=0.000000 *\n",
      "I 2016-12-18 00:15:07 downhill.base:232 Adam 1621 loss=0.111625 error=0.106200 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:07 downhill.base:232 Adam 1622 loss=0.111610 error=0.106198 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:08 downhill.base:232 Adam 1623 loss=0.111595 error=0.106197 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:08 downhill.base:232 Adam 1624 loss=0.111581 error=0.106195 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:08 downhill.base:232 Adam 1625 loss=0.111566 error=0.106193 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:08 downhill.base:232 Adam 1626 loss=0.111551 error=0.106192 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:09 downhill.base:232 Adam 1627 loss=0.111537 error=0.106190 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:09 downhill.base:232 Adam 1628 loss=0.111522 error=0.106189 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:09 downhill.base:232 Adam 1629 loss=0.111508 error=0.106187 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:09 downhill.base:232 Adam 1630 loss=0.111493 error=0.106185 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:09 downhill.base:232 validation 163 loss=0.111479 error=0.106184 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:09 downhill.base:232 Adam 1631 loss=0.111479 error=0.106184 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:10 downhill.base:232 Adam 1632 loss=0.111465 error=0.106182 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:10 downhill.base:232 Adam 1633 loss=0.111450 error=0.106181 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:10 downhill.base:232 Adam 1634 loss=0.111436 error=0.106179 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:10 downhill.base:232 Adam 1635 loss=0.111422 error=0.106178 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:10 downhill.base:232 Adam 1636 loss=0.111407 error=0.106176 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:11 downhill.base:232 Adam 1637 loss=0.111393 error=0.106174 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:11 downhill.base:232 Adam 1638 loss=0.111379 error=0.106173 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:11 downhill.base:232 Adam 1639 loss=0.111365 error=0.106171 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:11 downhill.base:232 Adam 1640 loss=0.111351 error=0.106170 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:11 downhill.base:232 validation 164 loss=0.111337 error=0.106168 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:12 downhill.base:232 Adam 1641 loss=0.111337 error=0.106168 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:12 downhill.base:232 Adam 1642 loss=0.111323 error=0.106167 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:12 downhill.base:232 Adam 1643 loss=0.111309 error=0.106165 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:12 downhill.base:232 Adam 1644 loss=0.111295 error=0.106164 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:12 downhill.base:232 Adam 1645 loss=0.111281 error=0.106163 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:13 downhill.base:232 Adam 1646 loss=0.111267 error=0.106161 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:13 downhill.base:232 Adam 1647 loss=0.111253 error=0.106160 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:13 downhill.base:232 Adam 1648 loss=0.111239 error=0.106158 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:13 downhill.base:232 Adam 1649 loss=0.111225 error=0.106157 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:13 downhill.base:232 Adam 1650 loss=0.111211 error=0.106155 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:14 downhill.base:232 validation 165 loss=0.111198 error=0.106154 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:14 downhill.base:232 Adam 1651 loss=0.111198 error=0.106154 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:14 downhill.base:232 Adam 1652 loss=0.111184 error=0.106152 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:14 downhill.base:232 Adam 1653 loss=0.111170 error=0.106151 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:15 downhill.base:232 Adam 1654 loss=0.111157 error=0.106150 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:15 downhill.base:232 Adam 1655 loss=0.111143 error=0.106148 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:15 downhill.base:232 Adam 1656 loss=0.111129 error=0.106147 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:15 downhill.base:232 Adam 1657 loss=0.111116 error=0.106145 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:16 downhill.base:232 Adam 1658 loss=0.111102 error=0.106144 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:16 downhill.base:232 Adam 1659 loss=0.111089 error=0.106143 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:16 downhill.base:232 Adam 1660 loss=0.111075 error=0.106141 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:16 downhill.base:232 validation 166 loss=0.111062 error=0.106140 grad(U)=0.000000 grad(V)=0.000000 *\n",
      "I 2016-12-18 00:15:16 downhill.base:232 Adam 1661 loss=0.111062 error=0.106140 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:17 downhill.base:232 Adam 1662 loss=0.111048 error=0.106139 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:17 downhill.base:232 Adam 1663 loss=0.111035 error=0.106137 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:17 downhill.base:232 Adam 1664 loss=0.111022 error=0.106136 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:17 downhill.base:232 Adam 1665 loss=0.111008 error=0.106134 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:17 downhill.base:232 Adam 1666 loss=0.110995 error=0.106133 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:18 downhill.base:232 Adam 1667 loss=0.110982 error=0.106132 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:18 downhill.base:232 Adam 1668 loss=0.110969 error=0.106131 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:18 downhill.base:232 Adam 1669 loss=0.110955 error=0.106129 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:18 downhill.base:232 Adam 1670 loss=0.110942 error=0.106128 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:18 downhill.base:232 validation 167 loss=0.110929 error=0.106127 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:19 downhill.base:232 Adam 1671 loss=0.110929 error=0.106127 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:19 downhill.base:232 Adam 1672 loss=0.110916 error=0.106125 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:19 downhill.base:232 Adam 1673 loss=0.110903 error=0.106124 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:19 downhill.base:232 Adam 1674 loss=0.110890 error=0.106123 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:19 downhill.base:232 Adam 1675 loss=0.110877 error=0.106121 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:20 downhill.base:232 Adam 1676 loss=0.110864 error=0.106120 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:20 downhill.base:232 Adam 1677 loss=0.110851 error=0.106119 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:20 downhill.base:232 Adam 1678 loss=0.110838 error=0.106118 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:20 downhill.base:232 Adam 1679 loss=0.110825 error=0.106116 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:20 downhill.base:232 Adam 1680 loss=0.110812 error=0.106115 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:21 downhill.base:232 validation 168 loss=0.110799 error=0.106114 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:21 downhill.base:232 Adam 1681 loss=0.110799 error=0.106114 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:21 downhill.base:232 Adam 1682 loss=0.110786 error=0.106113 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:21 downhill.base:232 Adam 1683 loss=0.110774 error=0.106111 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:21 downhill.base:232 Adam 1684 loss=0.110761 error=0.106110 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:22 downhill.base:232 Adam 1685 loss=0.110748 error=0.106109 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:22 downhill.base:232 Adam 1686 loss=0.110735 error=0.106108 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:22 downhill.base:232 Adam 1687 loss=0.110723 error=0.106107 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:22 downhill.base:232 Adam 1688 loss=0.110710 error=0.106105 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:22 downhill.base:232 Adam 1689 loss=0.110698 error=0.106104 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:23 downhill.base:232 Adam 1690 loss=0.110685 error=0.106103 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:23 downhill.base:232 validation 169 loss=0.110673 error=0.106102 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:23 downhill.base:232 Adam 1691 loss=0.110673 error=0.106102 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:23 downhill.base:232 Adam 1692 loss=0.110660 error=0.106101 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:23 downhill.base:232 Adam 1693 loss=0.110648 error=0.106099 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:24 downhill.base:232 Adam 1694 loss=0.110635 error=0.106098 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:24 downhill.base:232 Adam 1695 loss=0.110623 error=0.106097 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:24 downhill.base:232 Adam 1696 loss=0.110611 error=0.106096 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:24 downhill.base:232 Adam 1697 loss=0.110598 error=0.106095 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:24 downhill.base:232 Adam 1698 loss=0.110586 error=0.106094 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:25 downhill.base:232 Adam 1699 loss=0.110574 error=0.106093 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:25 downhill.base:232 Adam 1700 loss=0.110561 error=0.106091 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:25 downhill.base:232 validation 170 loss=0.110549 error=0.106090 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:25 downhill.base:232 Adam 1701 loss=0.110549 error=0.106090 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:25 downhill.base:232 Adam 1702 loss=0.110537 error=0.106089 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:26 downhill.base:232 Adam 1703 loss=0.110525 error=0.106088 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:26 downhill.base:232 Adam 1704 loss=0.110513 error=0.106087 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:26 downhill.base:232 Adam 1705 loss=0.110501 error=0.106086 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:26 downhill.base:232 Adam 1706 loss=0.110489 error=0.106085 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:26 downhill.base:232 Adam 1707 loss=0.110477 error=0.106084 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:27 downhill.base:232 Adam 1708 loss=0.110465 error=0.106083 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:27 downhill.base:232 Adam 1709 loss=0.110453 error=0.106081 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:27 downhill.base:232 Adam 1710 loss=0.110441 error=0.106080 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:27 downhill.base:232 validation 171 loss=0.110429 error=0.106079 grad(U)=0.000000 grad(V)=0.000000 *\n",
      "I 2016-12-18 00:15:27 downhill.base:232 Adam 1711 loss=0.110429 error=0.106079 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:28 downhill.base:232 Adam 1712 loss=0.110417 error=0.106078 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:28 downhill.base:232 Adam 1713 loss=0.110405 error=0.106077 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:28 downhill.base:232 Adam 1714 loss=0.110393 error=0.106076 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:28 downhill.base:232 Adam 1715 loss=0.110381 error=0.106075 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:28 downhill.base:232 Adam 1716 loss=0.110370 error=0.106074 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:29 downhill.base:232 Adam 1717 loss=0.110358 error=0.106073 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:29 downhill.base:232 Adam 1718 loss=0.110346 error=0.106072 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:29 downhill.base:232 Adam 1719 loss=0.110335 error=0.106071 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:29 downhill.base:232 Adam 1720 loss=0.110323 error=0.106070 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:29 downhill.base:232 validation 172 loss=0.110311 error=0.106069 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:29 downhill.base:232 Adam 1721 loss=0.110311 error=0.106069 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:30 downhill.base:232 Adam 1722 loss=0.110300 error=0.106068 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:30 downhill.base:232 Adam 1723 loss=0.110288 error=0.106067 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:30 downhill.base:232 Adam 1724 loss=0.110277 error=0.106066 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:30 downhill.base:232 Adam 1725 loss=0.110265 error=0.106065 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:30 downhill.base:232 Adam 1726 loss=0.110254 error=0.106064 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:31 downhill.base:232 Adam 1727 loss=0.110242 error=0.106063 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:31 downhill.base:232 Adam 1728 loss=0.110231 error=0.106062 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:31 downhill.base:232 Adam 1729 loss=0.110220 error=0.106061 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:31 downhill.base:232 Adam 1730 loss=0.110208 error=0.106060 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:31 downhill.base:232 validation 173 loss=0.110197 error=0.106059 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:32 downhill.base:232 Adam 1731 loss=0.110197 error=0.106059 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:32 downhill.base:232 Adam 1732 loss=0.110186 error=0.106058 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:32 downhill.base:232 Adam 1733 loss=0.110174 error=0.106057 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:32 downhill.base:232 Adam 1734 loss=0.110163 error=0.106056 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:32 downhill.base:232 Adam 1735 loss=0.110152 error=0.106055 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:33 downhill.base:232 Adam 1736 loss=0.110141 error=0.106054 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:33 downhill.base:232 Adam 1737 loss=0.110129 error=0.106053 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:33 downhill.base:232 Adam 1738 loss=0.110118 error=0.106052 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:33 downhill.base:232 Adam 1739 loss=0.110107 error=0.106052 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:33 downhill.base:232 Adam 1740 loss=0.110096 error=0.106051 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:34 downhill.base:232 validation 174 loss=0.110085 error=0.106050 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:34 downhill.base:232 Adam 1741 loss=0.110085 error=0.106050 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:34 downhill.base:232 Adam 1742 loss=0.110074 error=0.106049 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:34 downhill.base:232 Adam 1743 loss=0.110063 error=0.106048 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:34 downhill.base:232 Adam 1744 loss=0.110052 error=0.106047 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:35 downhill.base:232 Adam 1745 loss=0.110041 error=0.106046 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:35 downhill.base:232 Adam 1746 loss=0.110030 error=0.106045 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:35 downhill.base:232 Adam 1747 loss=0.110019 error=0.106044 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:35 downhill.base:232 Adam 1748 loss=0.110008 error=0.106043 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:35 downhill.base:232 Adam 1749 loss=0.109997 error=0.106043 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:36 downhill.base:232 Adam 1750 loss=0.109987 error=0.106042 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:36 downhill.base:232 validation 175 loss=0.109976 error=0.106041 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:36 downhill.base:232 Adam 1751 loss=0.109976 error=0.106041 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:36 downhill.base:232 Adam 1752 loss=0.109965 error=0.106040 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:36 downhill.base:232 Adam 1753 loss=0.109955 error=0.106039 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:37 downhill.base:232 Adam 1754 loss=0.109944 error=0.106038 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:37 downhill.base:232 Adam 1755 loss=0.109933 error=0.106037 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:37 downhill.base:232 Adam 1756 loss=0.109923 error=0.106036 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:37 downhill.base:232 Adam 1757 loss=0.109912 error=0.106036 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:37 downhill.base:232 Adam 1758 loss=0.109901 error=0.106035 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:38 downhill.base:232 Adam 1759 loss=0.109891 error=0.106034 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:38 downhill.base:232 Adam 1760 loss=0.109880 error=0.106033 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:38 downhill.base:232 validation 176 loss=0.109870 error=0.106032 grad(U)=0.000000 grad(V)=0.000000 *\n",
      "I 2016-12-18 00:15:38 downhill.base:232 Adam 1761 loss=0.109870 error=0.106032 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:38 downhill.base:232 Adam 1762 loss=0.109859 error=0.106031 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:39 downhill.base:232 Adam 1763 loss=0.109849 error=0.106031 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:39 downhill.base:232 Adam 1764 loss=0.109839 error=0.106030 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:39 downhill.base:232 Adam 1765 loss=0.109828 error=0.106029 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:39 downhill.base:232 Adam 1766 loss=0.109818 error=0.106028 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:39 downhill.base:232 Adam 1767 loss=0.109808 error=0.106027 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:40 downhill.base:232 Adam 1768 loss=0.109797 error=0.106027 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:40 downhill.base:232 Adam 1769 loss=0.109787 error=0.106026 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:40 downhill.base:232 Adam 1770 loss=0.109777 error=0.106025 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:40 downhill.base:232 validation 177 loss=0.109766 error=0.106024 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:40 downhill.base:232 Adam 1771 loss=0.109766 error=0.106024 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:41 downhill.base:232 Adam 1772 loss=0.109756 error=0.106023 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:41 downhill.base:232 Adam 1773 loss=0.109746 error=0.106023 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:41 downhill.base:232 Adam 1774 loss=0.109736 error=0.106022 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:41 downhill.base:232 Adam 1775 loss=0.109725 error=0.106021 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:41 downhill.base:232 Adam 1776 loss=0.109715 error=0.106020 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:42 downhill.base:232 Adam 1777 loss=0.109705 error=0.106020 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:42 downhill.base:232 Adam 1778 loss=0.109695 error=0.106019 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:42 downhill.base:232 Adam 1779 loss=0.109685 error=0.106018 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:42 downhill.base:232 Adam 1780 loss=0.109675 error=0.106017 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:42 downhill.base:232 validation 178 loss=0.109665 error=0.106017 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:43 downhill.base:232 Adam 1781 loss=0.109665 error=0.106017 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:43 downhill.base:232 Adam 1782 loss=0.109655 error=0.106016 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:43 downhill.base:232 Adam 1783 loss=0.109645 error=0.106015 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:43 downhill.base:232 Adam 1784 loss=0.109635 error=0.106014 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:43 downhill.base:232 Adam 1785 loss=0.109625 error=0.106014 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:44 downhill.base:232 Adam 1786 loss=0.109615 error=0.106013 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:44 downhill.base:232 Adam 1787 loss=0.109606 error=0.106012 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:44 downhill.base:232 Adam 1788 loss=0.109596 error=0.106011 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:44 downhill.base:232 Adam 1789 loss=0.109586 error=0.106011 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:44 downhill.base:232 Adam 1790 loss=0.109576 error=0.106010 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:45 downhill.base:232 validation 179 loss=0.109566 error=0.106009 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:45 downhill.base:232 Adam 1791 loss=0.109566 error=0.106009 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:45 downhill.base:232 Adam 1792 loss=0.109557 error=0.106009 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:45 downhill.base:232 Adam 1793 loss=0.109547 error=0.106008 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:45 downhill.base:232 Adam 1794 loss=0.109537 error=0.106007 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:46 downhill.base:232 Adam 1795 loss=0.109528 error=0.106006 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:46 downhill.base:232 Adam 1796 loss=0.109518 error=0.106006 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:46 downhill.base:232 Adam 1797 loss=0.109508 error=0.106005 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:46 downhill.base:232 Adam 1798 loss=0.109499 error=0.106004 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:46 downhill.base:232 Adam 1799 loss=0.109489 error=0.106004 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:47 downhill.base:232 Adam 1800 loss=0.109479 error=0.106003 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:47 downhill.base:232 validation 180 loss=0.109470 error=0.106002 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:47 downhill.base:232 Adam 1801 loss=0.109470 error=0.106002 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:47 downhill.base:232 Adam 1802 loss=0.109460 error=0.106002 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:47 downhill.base:232 Adam 1803 loss=0.109451 error=0.106001 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:48 downhill.base:232 Adam 1804 loss=0.109441 error=0.106000 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:48 downhill.base:232 Adam 1805 loss=0.109432 error=0.106000 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:48 downhill.base:232 Adam 1806 loss=0.109422 error=0.105999 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:48 downhill.base:232 Adam 1807 loss=0.109413 error=0.105998 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:48 downhill.base:232 Adam 1808 loss=0.109404 error=0.105998 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:49 downhill.base:232 Adam 1809 loss=0.109394 error=0.105997 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:49 downhill.base:232 Adam 1810 loss=0.109385 error=0.105997 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:49 downhill.base:232 validation 181 loss=0.109375 error=0.105996 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:49 downhill.base:232 Adam 1811 loss=0.109375 error=0.105996 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:50 downhill.base:232 Adam 1812 loss=0.109366 error=0.105995 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:50 downhill.base:232 Adam 1813 loss=0.109357 error=0.105995 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:50 downhill.base:232 Adam 1814 loss=0.109348 error=0.105994 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:50 downhill.base:232 Adam 1815 loss=0.109338 error=0.105993 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:50 downhill.base:232 Adam 1816 loss=0.109329 error=0.105993 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:51 downhill.base:232 Adam 1817 loss=0.109320 error=0.105992 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:51 downhill.base:232 Adam 1818 loss=0.109311 error=0.105992 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:51 downhill.base:232 Adam 1819 loss=0.109301 error=0.105991 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:51 downhill.base:232 Adam 1820 loss=0.109292 error=0.105990 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:52 downhill.base:232 validation 182 loss=0.109283 error=0.105990 grad(U)=0.000000 grad(V)=0.000000 *\n",
      "I 2016-12-18 00:15:52 downhill.base:232 Adam 1821 loss=0.109283 error=0.105990 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:52 downhill.base:232 Adam 1822 loss=0.109274 error=0.105989 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:52 downhill.base:232 Adam 1823 loss=0.109265 error=0.105988 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:52 downhill.base:232 Adam 1824 loss=0.109256 error=0.105988 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:53 downhill.base:232 Adam 1825 loss=0.109247 error=0.105987 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:53 downhill.base:232 Adam 1826 loss=0.109238 error=0.105987 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:53 downhill.base:232 Adam 1827 loss=0.109229 error=0.105986 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:53 downhill.base:232 Adam 1828 loss=0.109220 error=0.105986 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:53 downhill.base:232 Adam 1829 loss=0.109211 error=0.105985 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:54 downhill.base:232 Adam 1830 loss=0.109202 error=0.105984 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:54 downhill.base:232 validation 183 loss=0.109193 error=0.105984 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:54 downhill.base:232 Adam 1831 loss=0.109193 error=0.105984 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:54 downhill.base:232 Adam 1832 loss=0.109184 error=0.105983 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:54 downhill.base:232 Adam 1833 loss=0.109175 error=0.105983 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:55 downhill.base:232 Adam 1834 loss=0.109166 error=0.105982 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:55 downhill.base:232 Adam 1835 loss=0.109157 error=0.105982 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:55 downhill.base:232 Adam 1836 loss=0.109148 error=0.105981 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:55 downhill.base:232 Adam 1837 loss=0.109139 error=0.105981 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:55 downhill.base:232 Adam 1838 loss=0.109131 error=0.105980 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:56 downhill.base:232 Adam 1839 loss=0.109122 error=0.105979 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:56 downhill.base:232 Adam 1840 loss=0.109113 error=0.105979 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:56 downhill.base:232 validation 184 loss=0.109104 error=0.105978 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:56 downhill.base:232 Adam 1841 loss=0.109104 error=0.105978 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:56 downhill.base:232 Adam 1842 loss=0.109096 error=0.105978 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:57 downhill.base:232 Adam 1843 loss=0.109087 error=0.105977 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:57 downhill.base:232 Adam 1844 loss=0.109078 error=0.105977 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:57 downhill.base:232 Adam 1845 loss=0.109070 error=0.105976 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:57 downhill.base:232 Adam 1846 loss=0.109061 error=0.105976 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:57 downhill.base:232 Adam 1847 loss=0.109052 error=0.105975 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:58 downhill.base:232 Adam 1848 loss=0.109044 error=0.105975 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:58 downhill.base:232 Adam 1849 loss=0.109035 error=0.105974 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:58 downhill.base:232 Adam 1850 loss=0.109027 error=0.105974 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:58 downhill.base:232 validation 185 loss=0.109018 error=0.105973 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:58 downhill.base:232 Adam 1851 loss=0.109018 error=0.105973 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:59 downhill.base:232 Adam 1852 loss=0.109010 error=0.105973 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:59 downhill.base:232 Adam 1853 loss=0.109001 error=0.105972 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:59 downhill.base:232 Adam 1854 loss=0.108993 error=0.105972 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:59 downhill.base:232 Adam 1855 loss=0.108984 error=0.105971 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:15:59 downhill.base:232 Adam 1856 loss=0.108976 error=0.105971 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:00 downhill.base:232 Adam 1857 loss=0.108967 error=0.105970 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:00 downhill.base:232 Adam 1858 loss=0.108959 error=0.105970 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:00 downhill.base:232 Adam 1859 loss=0.108951 error=0.105969 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:00 downhill.base:232 Adam 1860 loss=0.108942 error=0.105969 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:00 downhill.base:232 validation 186 loss=0.108934 error=0.105968 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:01 downhill.base:232 Adam 1861 loss=0.108934 error=0.105968 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:01 downhill.base:232 Adam 1862 loss=0.108926 error=0.105968 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:01 downhill.base:232 Adam 1863 loss=0.108917 error=0.105967 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:01 downhill.base:232 Adam 1864 loss=0.108909 error=0.105967 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:01 downhill.base:232 Adam 1865 loss=0.108901 error=0.105966 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:02 downhill.base:232 Adam 1866 loss=0.108893 error=0.105966 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:02 downhill.base:232 Adam 1867 loss=0.108884 error=0.105965 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:02 downhill.base:232 Adam 1868 loss=0.108876 error=0.105965 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:02 downhill.base:232 Adam 1869 loss=0.108868 error=0.105964 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:02 downhill.base:232 Adam 1870 loss=0.108860 error=0.105964 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:03 downhill.base:232 validation 187 loss=0.108852 error=0.105964 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:03 downhill.base:232 Adam 1871 loss=0.108852 error=0.105964 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:03 downhill.base:232 Adam 1872 loss=0.108844 error=0.105963 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:03 downhill.base:232 Adam 1873 loss=0.108836 error=0.105963 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:03 downhill.base:232 Adam 1874 loss=0.108827 error=0.105962 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:04 downhill.base:232 Adam 1875 loss=0.108819 error=0.105962 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:04 downhill.base:232 Adam 1876 loss=0.108811 error=0.105961 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:04 downhill.base:232 Adam 1877 loss=0.108803 error=0.105961 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:04 downhill.base:232 Adam 1878 loss=0.108795 error=0.105960 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:04 downhill.base:232 Adam 1879 loss=0.108787 error=0.105960 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:05 downhill.base:232 Adam 1880 loss=0.108779 error=0.105960 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:05 downhill.base:232 validation 188 loss=0.108772 error=0.105959 grad(U)=0.000000 grad(V)=0.000000\n",
      "I 2016-12-18 00:16:05 downhill.base:414 patience elapsed!\n",
      "1.06273722934\n"
     ]
    }
   ],
   "source": [
    "import fancyimpute as fi\n",
    "import our_helpers as ohe\n",
    "\n",
    "X_filled_nnm_2 = fi.MatrixFactorization(rank=2,l1_penalty=0.065, l2_penalty=0.065).complete(train)\n",
    "error = ohe.compute_error2(test, X_filled_nnm_2, nz_test)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.03 achvenebda fancyiomputis matrix factorization metodi, kaggleze = 1.05776"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
